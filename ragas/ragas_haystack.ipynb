{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-06T23:15:29.153544Z",
     "start_time": "2024-11-06T23:15:20.986356Z"
    }
   },
   "source": "%pip uninstall -y haystack-ai  ragas-haystack",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: haystack-ai 2.6.1Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Uninstalling haystack-ai-2.6.1:\n",
      "  Successfully uninstalled haystack-ai-2.6.1\n",
      "Found existing installation: ragas-haystack 1.0.1\n",
      "Uninstalling ragas-haystack-1.0.1:\n",
      "  Successfully uninstalled ragas-haystack-1.0.1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:03:08.034288Z",
     "start_time": "2024-11-06T23:03:00.409255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A valid OpenAI API key must be provided as an environment variable \"OPENAI_API_KEY\" to run this example.\n",
    "\n",
    "from haystack import Pipeline\n",
    "\n",
    "from haystack_integrations.components.evaluators.ragas import RagasEvaluator, RagasMetric\n",
    "\n",
    "QUESTIONS = [\n",
    "    \"Which is the most popular global sport?\",\n",
    "    \"Who created the Python language?\",\n",
    "]\n",
    "CONTEXTS = [\n",
    "    [\n",
    "        \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n",
    "    ],\n",
    "]\n",
    "RESPONSES = [\n",
    "    \"Football is the most popular sport with around 4 billion followers worldwide\",\n",
    "    \"Python language was created by Guido van Rossum.\",\n",
    "]\n",
    "\n",
    "GROUND_TRUTHS = [\n",
    "    \"Football is the most popular sport\",\n",
    "    \"Python language was created by Guido van Rossum.\",\n",
    "]\n",
    "\n",
    "pipeline = Pipeline()\n",
    "evaluator_context = RagasEvaluator(\n",
    "    metric=RagasMetric.CONTEXT_PRECISION,\n",
    ")\n",
    "evaluator_aspect = RagasEvaluator(\n",
    "    metric=RagasMetric.ASPECT_CRITIQUE,\n",
    "    metric_params={\"name\": \"custom\", \"definition\": \"Is this answer problematic for children?\", \"strictness\": 3},\n",
    ")\n",
    "pipeline.add_component(\"evaluator_context\", evaluator_context)\n",
    "pipeline.add_component(\"evaluator_aspect\", evaluator_aspect)\n",
    "\n",
    "# Each metric expects a specific set of parameters as input. Refer to the\n",
    "# Ragas class' documentation for more details.\n",
    "results = pipeline.run(\n",
    "    {\n",
    "        \"evaluator_context\": {\"questions\": QUESTIONS, \"contexts\": CONTEXTS, \"ground_truths\": GROUND_TRUTHS},\n",
    "        \"evaluator_aspect\": {\"questions\": QUESTIONS, \"contexts\": CONTEXTS, \"responses\": RESPONSES},\n",
    "    }\n",
    ")\n",
    "\n",
    "for component in [\"evaluator_context\", \"evaluator_aspect\"]:\n",
    "    for output in results[component][\"results\"]:\n",
    "        print(output)\n"
   ],
   "id": "bf5e610552041fa9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\metrics\\__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "C:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\metrics\\__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOpenAIError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 42\u001B[0m\n\u001B[0;32m     38\u001B[0m pipeline\u001B[38;5;241m.\u001B[39madd_component(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluator_aspect\u001B[39m\u001B[38;5;124m\"\u001B[39m, evaluator_aspect)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Each metric expects a specific set of parameters as input. Refer to the\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# Ragas class' documentation for more details.\u001B[39;00m\n\u001B[1;32m---> 42\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m     44\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mevaluator_context\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mQUESTIONS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontexts\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mCONTEXTS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mground_truths\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mGROUND_TRUTHS\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mevaluator_aspect\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mQUESTIONS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontexts\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mCONTEXTS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponses\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mRESPONSES\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m component \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluator_context\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluator_aspect\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m results[component][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\haystack\\core\\pipeline\\pipeline.py:229\u001B[0m, in \u001B[0;36mPipeline.run\u001B[1;34m(self, data, include_outputs_from)\u001B[0m\n\u001B[0;32m    226\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaximum run count \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_runs_per_component\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m reached for component \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PipelineMaxComponentRuns(msg)\n\u001B[1;32m--> 229\u001B[0m res: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_component\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomponents_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m include_outputs_from:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;66;03m# Deepcopy the outputs to prevent downstream nodes from modifying them\u001B[39;00m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;66;03m# We don't care about loops - Always store the last output.\u001B[39;00m\n\u001B[0;32m    234\u001B[0m     extra_outputs[name] \u001B[38;5;241m=\u001B[39m deepcopy(res)\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\haystack\\core\\pipeline\\pipeline.py:67\u001B[0m, in \u001B[0;36mPipeline._run_component\u001B[1;34m(self, name, inputs)\u001B[0m\n\u001B[0;32m     65\u001B[0m span\u001B[38;5;241m.\u001B[39mset_content_tag(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaystack.component.input\u001B[39m\u001B[38;5;124m\"\u001B[39m, inputs)\n\u001B[0;32m     66\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning component \u001B[39m\u001B[38;5;132;01m{component_name}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, component_name\u001B[38;5;241m=\u001B[39mname)\n\u001B[1;32m---> 67\u001B[0m res: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[43minstance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mnodes[name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvisits\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;66;03m# After a Component that has variadic inputs is run, we need to reset the variadic inputs that were consumed\u001B[39;00m\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\haystack_integrations\\components\\evaluators\\ragas\\evaluator.py:125\u001B[0m, in \u001B[0;36mRagasEvaluator.run\u001B[1;34m(self, **inputs)\u001B[0m\n\u001B[0;32m    122\u001B[0m converted_inputs: List[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdescriptor\u001B[38;5;241m.\u001B[39minput_converter(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs))  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m    124\u001B[0m dataset \u001B[38;5;241m=\u001B[39m Dataset\u001B[38;5;241m.\u001B[39mfrom_list(converted_inputs)\n\u001B[1;32m--> 125\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend_metric\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m OutputConverters\u001B[38;5;241m.\u001B[39mvalidate_outputs(results)\n\u001B[0;32m    128\u001B[0m converted_results \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    129\u001B[0m     [result\u001B[38;5;241m.\u001B[39mto_dict()] \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdescriptor\u001B[38;5;241m.\u001B[39moutput_converter(results, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetric, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetric_params)\n\u001B[0;32m    130\u001B[0m ]\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\haystack_integrations\\components\\evaluators\\ragas\\evaluator.py:102\u001B[0m, in \u001B[0;36mRagasEvaluator._invoke_evaluate\u001B[1;34m(dataset, metric)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_invoke_evaluate\u001B[39m(dataset: Dataset, metric: Metric) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Result:\n\u001B[1;32m--> 102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\_analytics.py:129\u001B[0m, in \u001B[0;36mtrack_was_completed.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m    128\u001B[0m     track(IsCompleteEvent(event_type\u001B[38;5;241m=\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, is_completed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[1;32m--> 129\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m     track(IsCompleteEvent(event_type\u001B[38;5;241m=\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, is_completed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\evaluation.py:187\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map)\u001B[0m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(metric, MetricWithLLM) \u001B[38;5;129;01mand\u001B[39;00m metric\u001B[38;5;241m.\u001B[39mllm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m llm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 187\u001B[0m         llm \u001B[38;5;241m=\u001B[39m \u001B[43mllm_factory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m     metric\u001B[38;5;241m.\u001B[39mllm \u001B[38;5;241m=\u001B[39m llm\n\u001B[0;32m    189\u001B[0m     llm_changed\u001B[38;5;241m.\u001B[39mappend(i)\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\llms\\base.py:307\u001B[0m, in \u001B[0;36mllm_factory\u001B[1;34m(model, run_config, default_headers, base_url)\u001B[0m\n\u001B[0;32m    304\u001B[0m     default_headers \u001B[38;5;241m=\u001B[39m helicone_config\u001B[38;5;241m.\u001B[39mdefault_headers()\n\u001B[0;32m    305\u001B[0m     base_url \u001B[38;5;241m=\u001B[39m helicone_config\u001B[38;5;241m.\u001B[39mbase_url\n\u001B[1;32m--> 307\u001B[0m openai_model \u001B[38;5;241m=\u001B[39m \u001B[43mChatOpenAI\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    308\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_url\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_url\u001B[49m\n\u001B[0;32m    309\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LangchainLLMWrapper(openai_model, run_config)\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\"\"\"\u001B[39;00m\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:551\u001B[0m, in \u001B[0;36mBaseChatOpenAI.validate_environment\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    549\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhttp_client \u001B[38;5;241m=\u001B[39m httpx\u001B[38;5;241m.\u001B[39mClient(proxy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopenai_proxy)\n\u001B[0;32m    550\u001B[0m     sync_specific \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp_client\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhttp_client}\n\u001B[1;32m--> 551\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_client \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mclient_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msync_specific\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_client\u001B[38;5;241m.\u001B[39mchat\u001B[38;5;241m.\u001B[39mcompletions\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masync_client:\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\openai\\_client.py:105\u001B[0m, in \u001B[0;36mOpenAI.__init__\u001B[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001B[0m\n\u001B[0;32m    103\u001B[0m     api_key \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m api_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m OpenAIError(\n\u001B[0;32m    106\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    107\u001B[0m     )\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m api_key\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m organization \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mOpenAIError\u001B[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
