{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:12:38.279181Z",
     "start_time": "2024-11-06T23:12:21.487570Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install --upgrade datasets ragas langchain-openai unstructured \"unstructured[md]\" libmagic  python-magic python-magic-bin  ",
   "id": "db6f9b5b08b62856",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: datasets in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: ragas in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (0.1.16)\n",
      "Collecting ragas\n",
      "  Using cached ragas-0.2.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: langchain-openai in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (0.2.6)\n",
      "Requirement already satisfied: unstructured in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (0.16.4)\n",
      "Requirement already satisfied: libmagic in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: filelock in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tiktoken in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (0.8.0)\n",
      "Requirement already satisfied: langchain in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (0.3.7)\n",
      "Requirement already satisfied: langchain-core in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (0.3.15)\n",
      "Requirement already satisfied: langchain-community in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (0.3.5)\n",
      "Requirement already satisfied: nest-asyncio in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (2.9.2)\n",
      "Requirement already satisfied: openai>1 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (1.54.1)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: chardet in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (5.3.0)\n",
      "Requirement already satisfied: nltk in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (2024.10.22)\n",
      "Requirement already satisfied: langdetect in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (3.10.1)\n",
      "Requirement already satisfied: backoff in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (4.11.0)\n",
      "Requirement already satisfied: unstructured-client in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (0.27.0)\n",
      "Requirement already satisfied: wrapt in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: psutil in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (6.0.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (0.0.1)\n",
      "Requirement already satisfied: html5lib in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langchain-core->ragas) (0.1.140)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langchain-core->ragas) (9.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from openai>1->ragas) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from openai>1->ragas) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from openai>1->ragas) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic>=2->ragas) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from tiktoken->ragas) (2024.9.11)\n",
      "Requirement already satisfied: colorama in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from dataclasses-json->unstructured) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from html5lib->unstructured) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langchain->ragas) (2.0.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langchain->ragas) (0.3.2)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langchain-community->ragas) (2.6.1)\n",
      "Requirement already satisfied: click in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: olefile in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured-client->unstructured) (43.0.3)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured-client->unstructured) (0.2.0)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured-client->unstructured) (5.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core->ragas) (3.10.11)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\sw\\anaconda3\\envs\\torch\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Using cached ragas-0.2.3-py3-none-any.whl (141 kB)\n",
      "Installing collected packages: ragas\n",
      "  Attempting uninstall: ragas\n",
      "    Found existing installation: ragas 0.1.16\n",
      "    Uninstalling ragas-0.1.16:\n",
      "      Successfully uninstalled ragas-0.1.16\n",
      "Successfully installed ragas-0.2.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragas-haystack 1.0.1 requires ragas<=0.1.16,>=0.1.11, but you have ragas 0.2.3 which is incompatible.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:16:49.099822Z",
     "start_time": "2024-11-06T23:16:45.742268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ],
   "id": "14038c45f5b4252e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:16:52.888472Z",
     "start_time": "2024-11-06T23:16:52.880752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OPENAI_DEPLOYMENT = 'gpt-35-turbo'\n",
    "OPENAI_MODEL_NAME = 'gpt-35-turbo'\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-03-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ciopenai.openai.azure.com/\"\n"
   ],
   "id": "b308ffc6b3e33428",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-06T23:17:11.350784Z",
     "start_time": "2024-11-06T23:16:54.359321Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from ragas import EvaluationDataset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAI, AzureOpenAIEmbeddings\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:17:13.139487Z",
     "start_time": "2024-11-06T23:17:11.379157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"explodinggradients/amnesty_qa\",\"english_v3\", trust_remote_code=True)\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset[\"eval\"])"
   ],
   "id": "98f3adde2cfe0c0b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:30:04.192561Z",
     "start_time": "2024-11-06T23:29:55.670876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\")\n",
    "embeddings_llm = AzureOpenAIEmbeddings(deployment=\"text-embedding-ada-002\")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_llm)\n",
    "\n",
    "# Run the LLM\n",
    "llm.invoke(\"Tell me a joke\")"
   ],
   "id": "56a010b9b6822cba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a joke for you:\\n\\nWhy don't skeletons fight each other?\\n\\nThey don't have the guts!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 11, 'total_tokens': 33, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}, id='run-c20dee85-5eab-45e0-8932-a9b2f2541f8b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 22, 'total_tokens': 33, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:33:23.169805Z",
     "start_time": "2024-11-06T23:30:11.655858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm), \n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics, llm=llm, embeddings=embeddings_llm)"
   ],
   "id": "f98fef7e5eafcd62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2d5fdd6b5bc4bd4ad239461d1698b16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:33:23.278566Z",
     "start_time": "2024-11-06T23:33:23.241233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = results.to_pandas()\n",
    "df"
   ],
   "id": "5444888547a237f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What are the global implications of the USA Su...   \n",
       "1   Which companies are the main contributors to G...   \n",
       "2   Which private companies in the Americas are th...   \n",
       "3   What action did Amnesty International urge its...   \n",
       "4   What are the recommendations made by Amnesty I...   \n",
       "5   Who are the target audience of the two books c...   \n",
       "6   Which right guarantees access to comprehensive...   \n",
       "7   Who has the right to be fully informed about h...   \n",
       "8   When can individuals be found guilty under Art...   \n",
       "9   When does the prosecution consider statements ...   \n",
       "10  What factors have contributed to the decline o...   \n",
       "11  What conditions designate wetlands as Ramsar s...   \n",
       "12                      Where was COP15 held in 2022?   \n",
       "13  What is the purpose of the agreement known as ...   \n",
       "14  Who failed to explicitly recognize Indigenous ...   \n",
       "15  What are the consequences of criminalizing abo...   \n",
       "16  What responsibilities should social media comp...   \n",
       "17  What role do social media companies play in pr...   \n",
       "18  What labor abuses were documented by Amnesty I...   \n",
       "19  When did the government of Qatar start repeali...   \n",
       "\n",
       "                                   retrieved_contexts  \\\n",
       "0   [- In 2022, the USA Supreme Court handed down ...   \n",
       "1   [In recent years, there has been increasing pr...   \n",
       "2   [The issue of greenhouse gas emissions has bec...   \n",
       "3   [In the case of the Ogoni 9, Amnesty Internati...   \n",
       "4   [In recent years, Amnesty International has fo...   \n",
       "5   [In addition to children, parents, teachers, a...   \n",
       "6   [The right to truth is a fundamental human rig...   \n",
       "7   [In many cases, the identities of perpetrators...   \n",
       "8   [Article 207.3 of the Russian Criminal Code pe...   \n",
       "9   [- As long as their statements are contrary to...   \n",
       "10  [The economic challenges facing Nicaragua have...   \n",
       "11  [Wetlands designated as Ramsar sites must meet...   \n",
       "12  [The city of Kunming, located in the Yunnan pr...   \n",
       "13  [The 30x30 agreement aims to protect 30% of th...   \n",
       "14  [The lack of explicit recognition of Indigenou...   \n",
       "15  [- Abortion criminalization contributes to sti...   \n",
       "16  [Social media companies play a significant rol...   \n",
       "17  [Companies, including social media companies, ...   \n",
       "18  [The kafala system in Qatar, which ties a migr...   \n",
       "19  [Qatar's efforts to improve the rights and wor...   \n",
       "\n",
       "                                             response  \\\n",
       "0   The global implications of the USA Supreme Cou...   \n",
       "1   According to the Carbon Majors database, the m...   \n",
       "2   According to the Carbon Majors database, the l...   \n",
       "3   Amnesty International urged its supporters to ...   \n",
       "4   Amnesty International made several recommendat...   \n",
       "5   The target audience of the two books created b...   \n",
       "6   The right that guarantees access to comprehens...   \n",
       "7   Everyone has the right to be fully informed ab...   \n",
       "8   Under Article 207.3 of the Russian Criminal Co...   \n",
       "9   Under Article 207.3 of the Russian Criminal Co...   \n",
       "10  There are several factors that have contribute...   \n",
       "11  Wetlands are designated as Ramsar sites based ...   \n",
       "12          COP15 was held in Kunming, China in 2022.   \n",
       "13  The purpose of the agreement known as 30x30 is...   \n",
       "14  At COP15, the United Nations Climate Change Co...   \n",
       "15  Criminalizing abortion can have severe consequ...   \n",
       "16  Social media companies have a responsibility t...   \n",
       "17  Social media companies play a crucial role in ...   \n",
       "18  Amnesty International has documented several l...   \n",
       "19  The government of Qatar started repealing rest...   \n",
       "\n",
       "                                            reference  context_recall  \\\n",
       "0   The global implications of the USA Supreme Cou...             1.0   \n",
       "1   According to the Carbon Majors database, the m...             1.0   \n",
       "2   The largest private companies in the Americas ...             1.0   \n",
       "3   Amnesty International urged its supporters to ...             1.0   \n",
       "4   The recommendations made by Amnesty Internatio...             1.0   \n",
       "5   The target audience of the two books created b...             1.0   \n",
       "6   The right that guarantees access to comprehens...             1.0   \n",
       "7   The victims of gross human rights violations a...             1.0   \n",
       "8   Individuals can be found guilty under Article ...             1.0   \n",
       "9   The prosecution considers statements contrary ...             1.0   \n",
       "10  The factors that have contributed to the decli...             1.0   \n",
       "11  The conditions that designate wetlands as Rams...             1.0   \n",
       "12        COP15 was held in Montreal, Canada in 2022.             1.0   \n",
       "13  The purpose of the agreement known as 30x30 is...             1.0   \n",
       "14  The States failed to explicitly recognize Indi...             1.0   \n",
       "15  The consequences of criminalizing abortion for...             1.0   \n",
       "16  Social media companies should have the respons...             1.0   \n",
       "17  Social media companies play a role in protecti...             1.0   \n",
       "18  Amnesty International documented labor abuses ...             1.0   \n",
       "19  The government of Qatar started repealing rest...             1.0   \n",
       "\n",
       "    factual_correctness  faithfulness  semantic_similarity  \n",
       "0                  0.38      0.480000             0.959244  \n",
       "1                  0.11      0.120000             0.941763  \n",
       "2                  0.26      0.000000             0.959363  \n",
       "3                  0.25      0.600000             0.926831  \n",
       "4                  0.07      0.033333             0.919207  \n",
       "5                  0.80      0.500000             0.987077  \n",
       "6                  0.91      1.000000             0.993913  \n",
       "7                  0.45      0.340909             0.942685  \n",
       "8                  0.00      0.000000             0.911983  \n",
       "9                  0.00      0.000000             0.951724  \n",
       "10                 0.49      0.588235             0.964409  \n",
       "11                 0.17      0.269231             0.946286  \n",
       "12                 0.40      1.000000             0.926384  \n",
       "13                 0.55      0.785714             0.966234  \n",
       "14                 0.00      0.000000             0.913105  \n",
       "15                 0.30      0.750000             0.965486  \n",
       "16                 0.08      0.240000             0.932237  \n",
       "17                 0.09      0.307692             0.956551  \n",
       "18                 0.53      0.432432             0.969198  \n",
       "19                 0.00      0.000000             0.978606  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the global implications of the USA Su...</td>\n",
       "      <td>[- In 2022, the USA Supreme Court handed down ...</td>\n",
       "      <td>The global implications of the USA Supreme Cou...</td>\n",
       "      <td>The global implications of the USA Supreme Cou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.959244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which companies are the main contributors to G...</td>\n",
       "      <td>[In recent years, there has been increasing pr...</td>\n",
       "      <td>According to the Carbon Majors database, the m...</td>\n",
       "      <td>According to the Carbon Majors database, the m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.941763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which private companies in the Americas are th...</td>\n",
       "      <td>[The issue of greenhouse gas emissions has bec...</td>\n",
       "      <td>According to the Carbon Majors database, the l...</td>\n",
       "      <td>The largest private companies in the Americas ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What action did Amnesty International urge its...</td>\n",
       "      <td>[In the case of the Ogoni 9, Amnesty Internati...</td>\n",
       "      <td>Amnesty International urged its supporters to ...</td>\n",
       "      <td>Amnesty International urged its supporters to ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.926831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the recommendations made by Amnesty I...</td>\n",
       "      <td>[In recent years, Amnesty International has fo...</td>\n",
       "      <td>Amnesty International made several recommendat...</td>\n",
       "      <td>The recommendations made by Amnesty Internatio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.919207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who are the target audience of the two books c...</td>\n",
       "      <td>[In addition to children, parents, teachers, a...</td>\n",
       "      <td>The target audience of the two books created b...</td>\n",
       "      <td>The target audience of the two books created b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.987077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which right guarantees access to comprehensive...</td>\n",
       "      <td>[The right to truth is a fundamental human rig...</td>\n",
       "      <td>The right that guarantees access to comprehens...</td>\n",
       "      <td>The right that guarantees access to comprehens...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who has the right to be fully informed about h...</td>\n",
       "      <td>[In many cases, the identities of perpetrators...</td>\n",
       "      <td>Everyone has the right to be fully informed ab...</td>\n",
       "      <td>The victims of gross human rights violations a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.942685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When can individuals be found guilty under Art...</td>\n",
       "      <td>[Article 207.3 of the Russian Criminal Code pe...</td>\n",
       "      <td>Under Article 207.3 of the Russian Criminal Co...</td>\n",
       "      <td>Individuals can be found guilty under Article ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>When does the prosecution consider statements ...</td>\n",
       "      <td>[- As long as their statements are contrary to...</td>\n",
       "      <td>Under Article 207.3 of the Russian Criminal Co...</td>\n",
       "      <td>The prosecution considers statements contrary ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.951724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What factors have contributed to the decline o...</td>\n",
       "      <td>[The economic challenges facing Nicaragua have...</td>\n",
       "      <td>There are several factors that have contribute...</td>\n",
       "      <td>The factors that have contributed to the decli...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.964409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What conditions designate wetlands as Ramsar s...</td>\n",
       "      <td>[Wetlands designated as Ramsar sites must meet...</td>\n",
       "      <td>Wetlands are designated as Ramsar sites based ...</td>\n",
       "      <td>The conditions that designate wetlands as Rams...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.946286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Where was COP15 held in 2022?</td>\n",
       "      <td>[The city of Kunming, located in the Yunnan pr...</td>\n",
       "      <td>COP15 was held in Kunming, China in 2022.</td>\n",
       "      <td>COP15 was held in Montreal, Canada in 2022.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of the agreement known as ...</td>\n",
       "      <td>[The 30x30 agreement aims to protect 30% of th...</td>\n",
       "      <td>The purpose of the agreement known as 30x30 is...</td>\n",
       "      <td>The purpose of the agreement known as 30x30 is...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.966234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Who failed to explicitly recognize Indigenous ...</td>\n",
       "      <td>[The lack of explicit recognition of Indigenou...</td>\n",
       "      <td>At COP15, the United Nations Climate Change Co...</td>\n",
       "      <td>The States failed to explicitly recognize Indi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What are the consequences of criminalizing abo...</td>\n",
       "      <td>[- Abortion criminalization contributes to sti...</td>\n",
       "      <td>Criminalizing abortion can have severe consequ...</td>\n",
       "      <td>The consequences of criminalizing abortion for...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.965486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What responsibilities should social media comp...</td>\n",
       "      <td>[Social media companies play a significant rol...</td>\n",
       "      <td>Social media companies have a responsibility t...</td>\n",
       "      <td>Social media companies should have the respons...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.932237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What role do social media companies play in pr...</td>\n",
       "      <td>[Companies, including social media companies, ...</td>\n",
       "      <td>Social media companies play a crucial role in ...</td>\n",
       "      <td>Social media companies play a role in protecti...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.956551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What labor abuses were documented by Amnesty I...</td>\n",
       "      <td>[The kafala system in Qatar, which ties a migr...</td>\n",
       "      <td>Amnesty International has documented several l...</td>\n",
       "      <td>Amnesty International documented labor abuses ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.969198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>When did the government of Qatar start repeali...</td>\n",
       "      <td>[Qatar's efforts to improve the rights and wor...</td>\n",
       "      <td>The government of Qatar started repealing rest...</td>\n",
       "      <td>The government of Qatar started repealing rest...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test set generator",
   "id": "be005e101a8a5662"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:07:29.038107Z",
     "start_time": "2024-11-06T23:07:19.893323Z"
    }
   },
   "cell_type": "code",
   "source": "!git clone https://huggingface.co/datasets/explodinggradients/Sample_Docs_Markdown",
   "id": "cfd67a3a742eeb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Sample_Docs_Markdown'...\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:24:53.473740Z",
     "start_time": "2024-11-06T23:24:28.246020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "path = \"Sample_Docs_Markdown/\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()"
   ],
   "id": "effca4976f1f8188",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:26:53.370761Z",
     "start_time": "2024-11-06T23:26:53.351078Z"
    }
   },
   "cell_type": "code",
   "source": "docs",
   "id": "485ae990d4a32e0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Sample_Docs_Markdown\\\\1701.06538v1.md'}, page_content='Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts Layer\\n\\nNoam Shazeer1, Azalia Mirhoseini∗†1, Krzysztof Maziarz∗2, Andy Davis1, Quoc Le1, Geoffrey Hinton1and Jeff Dean1 1Google Brain, {noam,azalia,andydavis,qvl,geoffhinton,jeff}@google.com 2Jagiellonian University, Cracow, krzysztof.maziarz@student.uj.edu.pl\\n\\nAbstract\\n\\nThe capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.\\n\\n1 Introduction And Related Work 1.1 Conditional Computation\\n\\nExploiting scale in both training data and model size has been central to the success of deep learning. When datasets are sufficiently large, increasing the capacity (number of parameters) of neural networks can give much better prediction accuracy. This has been shown in domains such as text (Sutskever et al., 2014; Bahdanau et al., 2014; Jozefowicz et al., 2016; Wu et al., 2016), images (Krizhevsky et al., 2012; Le et al., 2012), and audio (Hinton et al., 2012; Amodei et al., 2015). For typical deep learning models, where the entire model is activated for every example, this leads to a roughly quadratic blow-up in training costs, as both the model size and the number of training examples increase. Unfortunately, the advances in computing power and distributed computation fall short of meeting such demand.\\n\\nVarious forms of conditional computation have been proposed as a way to increase model capacity without a proportional increase in computational costs (Davis & Arel, 2013; Bengio et al., 2013; Eigen et al., 2013; Ludovic Denoyer, 2014; Cho & Bengio, 2014; Bengio et al., 2015; Almahairi et al., 2015). In these schemes, large parts of a network are active or inactive on a per-example basis. The gating decisions may be binary or sparse and continuous, stochastic or deterministic. Various forms of reinforcement learning and back-propagation are proposed for trarining the gating decisions.\\n\\nWhile these ideas are promising in theory, no work to date has yet demonstrated massive improvements in model capacity, training time, or model quality. We blame this on a combination of the following challenges:\\n\\nModern computing devices, especially GPUs, are much faster at arithmetic than at branching. Most of the works above recognize this and propose turning on/off large chunks of the network with each gating decision.\\n\\nLarge batch sizes are critical for performance, as they amortize the costs of parameter transfers and updates. Conditional computation reduces the batch sizes for the conditionally active chunks of the network.\\n\\nNetwork bandwidth can be a bottleneck. A cluster of GPUs may have computational power thousands of times greater than the aggregate inter-device network bandwidth. To be computationally efficient, the relative computational versus network demands of an algorithm must exceed this ratio. Embedding layers, which can be seen as a form of conditional computation, are handicapped by this very problem. Since the embeddings generally need to be sent across the network, the number of (example, parameter) interactions is limited by network bandwidth instead of computational capacity.\\n\\nDepending on the scheme, loss terms may be necessary to achieve the desired level of sparsity per-chunk and/or per example. Bengio et al. (2015) use three such terms. These issues can affect both model quality and load-balancing.\\n\\nModel capacity is most critical for very large data sets. The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images. It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions, let alone billions of parameters. In this work, we for the first time address all of the above challenges and finally realize the promise of conditional computation. We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state-of-the-art results on public language modeling and translation data sets.\\n\\n1.2 OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER Our approach to conditional computation is to introduce a new type of general purpose neural network component: a Sparsely-Gated Mixture-of-Experts Layer (MoE). The MoE consists of a number of experts, each a simple feed-forward neural network, and a trainable gating network which selects a sparse combination of the experts to process each input (see Figure 1). All parts of the network are trained jointly by back-propagation.\\n\\nWhile the introduced technique is generic, in this paper we focus on language modeling and machine translation tasks, which are known to benefit from very large models. In particular, we apply a MoE convolutionally between stacked LSTM layers (Hochreiter & Schmidhuber, 1997), as in Figure 1.\\n\\nThe MoE is called once for each position in the text, selecting a potentially different combination of experts at each position. The different experts tend to become highly specialized based on syntax and semantics (see Appendix E Table 9). On both language modeling and machine translation benchmarks, we improve on best published results at a fraction of the computational cost.\\n\\n1.3 Related Work On Mixtures Of Experts\\n\\nSince its introduction more than two decades ago (Jacobs et al., 1991; Jordan & Jacobs, 1994), the mixture-of-experts approach has been the subject of much research. Different types of expert architectures hae been proposed such as SVMs (Collobert et al., 2002), Gaussian Processes (Tresp, 2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Dirichlet Processes (Shahbaba & Neal, 2009), and deep networks. Other work has focused on different expert configurations such as a hierarchical structure (Yao et al., 2009), infinite numbers of experts (Rasmussen & Ghahramani, 2002), and adding experts sequentially (Aljundi et al., 2016). Garmash & Monz (2016) suggest an ensemble model in the format of mixture of experts for machine translation. The gating network is trained on a pre-trained ensemble NMT model. The works above concern top-level mixtures of experts. The mixture of experts is the whole model. Eigen et al. (2013) introduce the idea of using multiple MoEs with their own gating networks as parts of a deep model. It is intuitive that the latter approach is more powerful, since complex problems may contain many sub-problems each requiring different experts. They also allude in their conclusion to the potential to introduce sparsity, turning MoEs into a vehicle for computational computation.\\n\\nOur work builds on this use of MoEs as a general purpose neural network component. While Eigen et al. (2013) uses two stacked MoEs allowing for two sets of gating decisions, our convolutional application of the MoE allows for different gating decisions at each position in the text. We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity.\\n\\n2 The Structure Of The Mixture-Of-Experts Layer\\n\\nThe Mixture-of-Experts (MoE) layer consists of a set of n \"expert networks\" E1, · · · , En, and a \"gating network\" G whose output is a sparse n-dimensional vector. Figure 1 shows an overview of the MoE module. The experts are themselves neural networks, each with their own parameters.\\n\\nAlthough in principle we only require that the experts accept the same sized inputs and produce the same-sized outputs, in our initial investigations in this paper, we restrict ourselves to the case where the models are feed-forward networks with identical architectures, but with separate parameters.\\n\\nLet us denote by G(x) and Ei(x) the output of the gating network and the output of the i-th expert network for a given input x. The output y of the MoE module can be written as follows:\\n\\n$$y=\\\\sum_{i=1}^{n}G(x){i}E{i}(x)$$ $$(\\\\mathbf{l})$$ G(x)iEi(x) (1) We save computation based on the sparsity of the output of G(x). Wherever G(x)i = 0, we need not compute Ei(x). In our experiments, we have up to thousands of experts, but only need to evaluate a handful of them for every example. If the number of experts is very large, we can reduce the branching factor by using a two-level hierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted combination of \"experts\", each of which is itself a secondary mixture-of-experts with its own gating network. In the following we focus on ordinary MoEs. We provide more details on hierarchical MoEs in Appendix B. Our implementation is related to other models of conditional computation. A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in (Cho & Bengio, 2014). A MoE whose experts have one hidden layer is similar to the block-wise dropout described in (Bengio et al., 2015), where the dropped-out layer is sandwiched between fully-activated layers.\\n\\n2.1 Gating Network\\n\\nSoftmax Gating: A simple choice of non-sparse gating function (Jordan & Jacobs, 1994) is to multiply the input by a trainable weight matrix Wg and then apply the Sof tmax function.\\n\\n$$G_{\\\\sigma}(x)=S o f t m a x(x\\\\cdot W_{g})$$ $\\\\eqref{eq:walpha}$. Gσ(x) = Sof tmax(x · Wg) (2) Noisy Top-K Gating: We add two components to the Softmax gating network: sparsity and noise. Before taking the softmax function, we add tunable Gaussian noise, then keep only the top k values, setting the rest to −∞ (which causes the corresponding gate values to equal 0). The sparsity serves to save computation, as described above. While this form of sparsity creates some theoretically scary discontinuities in the output of gating function, we have not yet observed this to be a problem in practice. The noise term helps with load balancing, as will be discussed in Appendix A. The amount of noise per component is controlled by a second trainable weight matrix Wnoise.\\n\\n$$G(x)=S o f t m a x(K e e p T o p K(H(x),k))$$ $$(4)$$ $$(S)$$ G(x) = Sof tmax(KeepT opK(H(x), k)) (3) $$H(x){i}=(x\\\\cdot W{g}){i}+S t a n d a r d N o r m a l()\\\\cdot S o f t p l u s((x\\\\cdot W{n o i s e}){i})$$ $KeepTopK(v,k){i}=\\\\begin{cases}v_{i}&\\\\text{if}v_{i}\\\\text{is in the top}k\\\\text{elements of}v.\\\\ -\\\\infty&\\\\text{otherwise.}\\\\end{cases}$ Training the Gating Network We train the gating network by simple back-propagation, along with the rest of the model. If we choose k > 1, the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network. This type of occasionally-sensitive behavior is described in (Bengio et al., 2013) with respect to noisy rectifiers. Gradients also backpropagate through the gating network to its inputs. Our method differs here from (Bengio et al., 2015) who use boolean gates and a REINFORCE-style approach to train the gating network.\\n\\n3 Addressing Performance Challenges 3.1 The Shrinking Batch Problem\\n\\nOn modern CPUs and GPUs, large batch sizes are necessary for computational efficiency, so as to amortize the overhead of parameter loads and updates. If the gating network chooses k out of n experts for each example, then for a batch of b examples, each expert receives a much smaller batch of approximately kb n b examples. This causes a naive MoE implementation to become very inefficient as the number of experts increases. The solution to this shrinking batch problem is to make the original batch size as large as possible. However, batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes. We propose the following techniques for increasing the batch size: Mixing Data Parallelism and Model Parallelism: In a conventional distributed training setting, multiple copies of the model on different devices asynchronously process distinct batches of data, and parameters are synchronized through a set of parameter servers. In our technique, these different batches run synchronously so that they can be combined for the MoE layer. We distribute the standard layers of the model and the gating network according to conventional data-parallel schemes, but keep only one shared copy of each expert. Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data-parallel input batches. The same set of devices function as data-parallel replicas (for the standard layers and the gating networks) and as model-parallel shards (each hosting a subset of the experts). If the model is distributed over d devices, and each device processes a batch of size b, each expert receives a batch of approximately kbd nexamples. Thus, we achieve a factor of d improvement in expert batch size.\\n\\nIn the case of a hierarchical MoE (Section B), the primary gating network employs data parallelism, and the secondary MoEs employ model parallelism. Each secondary MoE resides on one device.\\n\\nThis technique allows us to increase the number of experts (and hence the number of parameters) by proportionally increasing the number of devices in the training cluster. The total batch size increases, keeping the batch size per expert constant. The memory and bandwidth requirements per device also remain constant, as do the step times, as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model. It is our goal to train a trillionparameter model on a trillion-word corpus. We have not scaled our systems this far as of the writing of this paper, but it should be possible by adding more hardware.\\n\\nTaking Advantage of Convolutionality: In our language models, we apply the same MoE to each time step of the previous layer. If we wait for the previous layer to finish, we can apply the MoE to all the time steps together as one big batch. Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps.\\n\\nIncreasing Batch Size for a Recurrent MoE: We suspect that even more powerful models may involve applying a MoE recurrently. For example, the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly, such models break the convolutional trick from the last paragraph, since the input to the MoE at one timestep depends on the output of the MoE at the previous timestep. Gruslys et al. (2016) describe a technique for drastically reducing the number of stored activations in an unrolled RNN, at the cost of recomputing forward activations. This would allow for a large increase in batch size.\\n\\n3.2 Network Bandwidth\\n\\nAnother major performance concern in distributed computing is network bandwidth. Since the experts are stationary (see above) and the number of gating parameters is small, most of the communication involves sending the inputs and outputs of the experts across the network. To maintain computational efficiency, the ratio of an expert\\'s computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device. For GPUs, this may be thousands to one. In our experiments, we use experts with one hidden layer containing thousands of RELU-activated units. Since the weight matrices in the expert have sizes input_size×hidden_size and hidden_size × output_size, the ratio of computation to input and output is equal to the size of the hidden layer. Conveniently, we can increase computational efficiency simply by using a larger hidden layer, or more hidden layers.\\n\\n4 Balancing Expert Utilization\\n\\nWe have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts. This imbalance is self-reinforcing, as the favored experts are trained more rapidly and thus are selected even more by the gating network. Eigen et al. (2013) describe the same phenomenon, and use a hard constraint at the beginning of training to avoid this local minimum. Bengio et al. (2015) include a soft constraint on the batch-wise average of each gate.1 We take a soft constraint approach. We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert. We define an additional loss Limportance, which is added to the overall loss function for the model. This loss is equal to the square of the coefficient of variation of the set of importance values, multiplied by a hand-tuned scaling factor wimportance. This additional loss encourages all experts to have equal importance.\\n\\n$$I m p o r t a n c e(X)=\\\\sum_{x\\\\in X}G(x)$$ $$L_{importance}(X)=w_{importance}\\\\cdot CV(Importance(X))^{2}\\\\tag{7}$$ $$(6)$$\\n\\nWhile this loss function can ensure equal importance, experts may still receive very different numbers of examples. For example, one expert may receive a few examples with large weights, and another may receive many examples with small weights. This can cause memory and performance problems on distributed hardware. To solve this problem, we introduce a second loss function, Lload , which ensures balanced loads. Appendix A contains the definition of this function, along with experimental results.\\n\\n5 Experiments 5.1 1 Billion Word Language Modeling Benchmark\\n\\nDataset: This dataset, introduced by (Chelba et al., 2013) consists of shuffled unique sentences from news articles, totaling approximately 829 million words, with a vocabulary of 793,471 words.\\n\\nPrevious State-of-the-Art: The best previously published results (Jozefowicz et al., 2016) use models consisting of one or more stacked Long Short-Term Memory (LSTM) layers (Hochreiter & Schmidhuber, 1997; Gers et al., 2000). The number of parameters in the LSTM layers of these models vary from 2 million to 151 million. Quality increases greatly with parameter count, as do computational costs. Results for these models form the top line of Figure 2-right. MoE Models: Our models consist of two stacked LSTM layers with a MoE layer between them (see Figure 1). We vary the sizes of the layers and the number of experts. For full details on model architecture, training regimen, additional baselines and results, see Appendix C.\\n\\nLow Computation, Varied Capacity: To investigate the effects of adding capacity, we trained\\n\\na series of MoE models all with roughly equal computational costs: about 8 million multiply-andadds per training example per timestep in the forwards pass, excluding the softmax layer. We call this metric (ops/timestep). We trained models with flat MoEs containing 4, 32, and 256 experts, and models with hierarchical MoEs containing 256, 1024, and 4096 experts. Each expert had about 1 million parameters. For all the MoE layers, 4 experts were active per input.\\n\\nThe results of these models are shown in Figure 2-left. The model with 4 always-active experts performed (unsurprisingly) similarly to the computationally-matched baseline models, while the largest of the models (4096 experts) achieved an impressive 24% lower perplexity on the test set. Figure 2: Model comparison on 1-Billion-Word Language-Modeling Benchmark. On the left, we plot test perplexity as a function of model capacity for models with similar computational budgets of approximately 8-million-ops-per-timestep. On the right, we plot test perplexity as a function of computational budget. The top line represents the LSTM models from (Jozefowicz et al., 2016).\\n\\nThe bottom line represents 4-billion parameter MoE models with different computational budgets.\\n\\nVaried Computation, High Capacity: In addition to the largest model from the previous section, we trained two more MoE models with similarly high capacity (4 billion parameters), but higher computation budgets. These models had larger LSTMs, and fewer but larger and experts. Details\\n\\nTest Test #Parameters ops/timestep Training TFLOPS Perplexity Perplexity excluding embedding Time /GPU 10 epochs 100 epochs and softmax layers 10 epochs Best Published Results 34.7 30.6 151 million 151 million 59 hours, 32 k40s 1.09 Low-Budget MoE Model 34.1 4303 million 8.9 million 15 hours, 16 k40s 0.74 Medium-Budget MoE Model 31.3 4313 million 33.8 million 17 hours, 32 k40s 1.22 High-Budget MoE Model 28.0 4371 million 142.7 million 47 hours, 32 k40s 1.56\\n\\nTable 1: Summary of high-capacity MoE-augmented models with varying computational budgets, vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C. can be found in Appendix C.2. Results of these three models form the bottom line of Figure 2-right.\\n\\nTable 1 compares the results of these models to the best previously-published result on this dataset .\\n\\nEven the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation.\\n\\nComputational Efficiency: We trained our models using TensorFlow (Abadi et al., 2016) on clusters containing 16-32 Tesla K40 GPUs. For each of our models, we determine computational efficiency in TFLOPS/GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster. The operation counts used here are higher than the ones we report in our ops/timestep numbers in that we include the backwards pass, we include the importance-sampling-based training of the softmax layer, and we count a multiply-and-add as two separate operations. For all of our MoE models, the floating point operations involved in the experts represent between 37% and 46% of the total.\\n\\nFor our baseline models wtih no MoE, observed computational efficiency ranged from 1.07-1.29 TFLOPS/GPU. For our low-computation MoE models, computation efficiency ranged from 0.740.90 TFLOPS/GPU, except for the 4-expert model which did not make full use of the available parallelism. Our highest-computation MoE model was more efficient at 1.56 TFLOPS/GPU, likely due to the larger matrices. These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS/GPU claimed by NVIDIA. Detailed results are in Appendix C, Table 7.\\n\\n5.2 100 BILLION WORD GOOGLE NEWS CORPUS\\n\\nOn the 1-billion-word corpus, adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion, as can be seen in Figure 2-left. We hypothesized that for a larger training set, even higher capacities would produce significant quality improvements. We constructed a similar training set consisting of shuffled unique sentences from Google\\'s internal news corpus, totalling roughly 100 billion words. Similarly to the previous section, we tested a series of models with similar computational costs of about 8 million ops/timestep. In addition to a baseline LSTM model, we trained models augmented with MoE layers containing 32, 256, 1024, 4096, 16384, 65536, and 131072 experts. This corresponds to up to 137 billion parameters in the MoE layer. Details on architecture, training, and results are given in Appendix D.\\n\\nResults: Figure 3 shows test perplexity as a function of capacity after training on 10 billion words (top line) and 100 billion words (bottom line). When training over the full 100 billion words, test perplexity improves significantly up to 65536 experts (68 billion parameters), dropping 39% lower than the computationally matched baseline, but degrades at 131072 experts, possibly a result of too much sparsity. The widening gap between the two lines demonstrates (unsurprisingly) that increased model capacity helps more on larger training sets. Even at 65536 experts (99.994% layer sparsity), computational efficiency for the model stays at a respectable 0.72 TFLOPS/GPU.\\n\\n5.3 Machine Translation (Single Language Pair)\\n\\nModel Architecture: Our model was a modified version of the GNMT model described in (Wu et al., 2016). To reduce computation, we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively. We inserted MoE layers in both the encoder (between layers 2 and 3) and the decoder (between layers 1 and 2). Each MoE layer contained up to 2048 experts each with about two million parameters, adding a total of about 8 billion parameters to the models. Further details on model architecture, testing procedure and results can be found in Appendix E.\\n\\nDatasets: We benchmarked our method on the WMT\\'14 En→Fr and En→De corpora, whose training sets have 36M sentence pairs and 5M sentence pairs, respectively. The experimental protocols were also similar to those in (Wu et al., 2016): newstest2014 was used as the test set to compare against previous work (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016), while the combination of newstest2012 and newstest2013 was used as the development set. We also tested the same model on a Google\\'s Production English to French data.\\n\\nTable 2: Results on WMT\\'14 En→ Fr newstest2014 (bold values represent best results). Model Test Test ops/timenstep Total Training Perplexity BLEU #Parameters Time MoE with 2048 Experts 2.69 40.35 85M 8.7B 3 days/64 k40s MoE with 2048 Experts (longer training) 2.63 40.56 85M 8.7B 6 days/64 k40s GNMT (Wu et al., 2016) 2.79 39.22 214M 278M 6 days/96 k80s GNMT+RL (Wu et al., 2016) 2.96 39.92 214M 278M 6 days/96 k80s PBMT (Durrani et al., 2014) 37.0 LSTM (6-layer) (Luong et al., 2015b) 31.5 LSTM (6-layer+PosUnk) (Luong et al., 2015b) 33.1 DeepAtt (Zhou et al., 2016) 37.7 DeepAtt+PosUnk (Zhou et al., 2016) 39.2\\n\\nTable 3: Results on WMT\\'14 En → De newstest2014 (bold values represent best results). Model Test Test ops/timestep Total Training Perplexity BLEU #Parameters Time MoE with 2048 Experts 4.64 26.03 85M 8.7B 1 day/64 k40s GNMT (Wu et al., 2016) 5.25 24.91 214M 278M 1 day/96 k80s GNMT +RL (Wu et al., 2016) 8.08 24.66 214M 278M 1 day/96 k80s PBMT (Durrani et al., 2014) 20.7 DeepAtt (Zhou et al., 2016) 20.6\\n\\nTable 4: Results on the Google Production En→ Fr dataset (bold values represent best results). Model Eval Eval Test Test ops/timestep Total Training Perplexity BLEU Perplexity BLEU #Parameters Time MoE with 2048 Experts 2.60 37.27 2.69 36.57 85M 8.7B 1 day/64 k40s GNMT (Wu et al., 2016) 2.78 35.80 2.87 35.56 214M 278M 6 days/96 k80s\\n\\nResults: Tables 2, 3, and 4 show the results of our largest models, compared with published results. Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT\\'14 En→Fr and En→De benchmarks. As our models did not use RL refinement, these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in (Wu et al., 2016). The perplexity scores are also better.2 On the Google Production dataset, our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time.\\n\\n5.4 Multilingual Machine Translation\\n\\nDataset: (Johnson et al., 2016) train a single GNMT (Wu et al., 2016) model on a very large combined dataset of twelve language pairs. Results are somewhat worse than those for 12 separately trained single-pair GNMT models. This is not surprising, given that the twelve models have 12 times the capacity and twelve times the aggregate training of the one model. We repeat this experiment with a single MoE-augmented model. See Appendix E for details on model architecture.\\n\\nWe train our model on the same dataset as (Johnson et al., 2016) and process the same number of training examples (about 3 billion sentence pairs). Our training time was shorter due to the lower computational budget of our model.\\n\\nResults: Results for the single-pair GNMT models, the multilingual GNMT model and the multilingual MoE model are given in Table 5. The MoE model achieves 19% lower perplexity on the dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus.\\n\\nTable 5: Multilingual Machine Translation (bold values represent best results). GNMT-Mono GNMT-Multi MoE-Multi MoE-Multi vs. GNMT-Multi Parameters 278M / model 278M 8.7B ops/timestep 212M 212M 102M training time, hardware various 21 days, 96 k20s 12 days, 64 k40s Perplexity (dev) 4.14 3.35 -19% French → English Test BLEU 36.47 34.40 37.46 +3.06 German → English Test BLEU 31.77 31.17 34.80 +3.63 Japanese → English Test BLEU 23.41 21.62 25.91 +4.29 Korean → English Test BLEU 25.42 22.87 28.71 +5.84 Portuguese → English Test BLEU 44.40 42.53 46.13 +3.60 Spanish → English Test BLEU 38.00 36.04 39.39 +3.35 English → French Test BLEU 35.37 34.00 36.59 +2.59 English → German Test BLEU 26.43 23.15 24.53 +1.38 English → Japanese Test BLEU 23.66 21.10 22.78 +1.68 English → Korean Test BLEU 19.75 18.41 16.62 -1.79 English → Portuguese Test BLEU 38.40 37.35 37.90 +0.55 English → Spanish Test BLEU 34.50 34.25 36.21 +1.96\\n\\n6 Conclusion\\n\\nThis work is the first to demonstrate major wins from conditional computation in deep networks.\\n\\nWe carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions. While we focused on text, conditional computation may help in other domains as well, provided sufficiently large training sets. We look forward to seeing many novel implementations and applications of conditional computation in the years to come.\\n\\nAcknowledgments\\n\\nWe would like to thank all of the members of the Google Brain and Google Translate teams who helped us with this project, in particular Zhifeng Chen, Yonghui Wu, and Melvin Johnson. Thanks also to our anonymous ICLR reviewers for the helpful suggestions on making this paper better.\\n\\n2Reported perplexities relative to the tokenization used by both our models and GNMT.\\n\\nReferences\\n\\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gregory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR, abs/1603.04467, 2016. URL http://arxiv.org/abs/1603.04467.\\n\\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. CoRR, abs/1611.06194, 2016. URL http://arxiv.org/abs/1611.\\n\\n06194.\\n\\nA. Almahairi, N. Ballas, T. Cooijmans, Y. Zheng, H. Larochelle, and A. Courville. Dynamic Capacity Networks. ArXiv e-prints, November 2015.\\n\\nDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Y. Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, and Zhenyao Zhu. Deep speech 2: End-to-end speech recognition in english and mandarin. arXiv preprint arXiv:1512.02595, 2015. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\\n\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.\\n\\nYoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling.\\n\\narXiv preprint arXiv:1312.3005, 2013.\\n\\nK. Cho and Y. Bengio. Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning. ArXiv e-prints, June 2014. Ronan Collobert, Samy Bengio, and Yoshua Bengio. A parallel mixture of SVMs for very large scale problems. Neural Computing, 2002.\\n\\nAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks. arXiv preprint arXiv:1312.4461, 2013. Marc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian processes. In ICML, 2015.\\n\\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization, 2010.\\n\\nNadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh\\'s phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, 2014.\\n\\nDavid Eigen, Marc\\'Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013. Ekaterina Garmash and Christof Monz. Ensemble learning for multi-source neural machine translation. In staff.science.uva.nl/c.monz, 2016.\\n\\nFelix A. Gers, Jürgen A. Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with lstm. Neural Computation, 2000.\\n\\nAudrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient backpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/ abs/1606.03401. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition, 2015.\\n\\nGeoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 2012. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 1997.\\n\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\\n\\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computing, 1991.\\n\\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\\'s multilingual neural machine translation system: Enabling zero-shot translation.\\n\\nCoRR, abs/1611.04558, 2016. URL http://arxiv.org/abs/1611.04558.\\n\\nMichael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\\n\\nNeural Computing, 1994. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n\\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\nReinhard Kneser and Hermann. Ney. Improved backingoff for m-gram language modeling., 1995.\\n\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.\\n\\nQuoc V. Le, Marc\\'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeffrey Dean, and Andrew Y. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012. Patrick Gallinari Ludovic Denoyer. Deep sequential neural network. arXiv preprint arXiv:1410.0510, 2014.\\n\\nMinh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attentionbased neural machine translation. EMNLP, 2015a.\\n\\nMinh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Addressing the rare word problem in neural machine translation. ACL, 2015b.\\n\\nCarl Edward Rasmussen and Zoubin Ghahramani. Infinite mixtures of Gaussian process experts.\\n\\nNIPS, 2002.\\n\\nHasim Sak, Andrew W Senior, and Françoise Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In INTERSPEECH, pp. 338–342, 2014.\\n\\nMike Schuster and Kaisuke Nakajima. Japanese and Korean voice search. ICASSP, 2012.\\n\\nBabak Shahbaba and Radford Neal. Nonlinear models using dirichlet process mixtures. JMLR, 2009.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.\\n\\nIn NIPS, 2014.\\n\\nLucas Theis and Matthias Bethge. Generative image modeling using spatial LSTMs. In NIPS, 2015. Volker Tresp. Mixtures of Gaussian Processes. In NIPS, 2001.\\n\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\\'s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\nBangpeng Yao, Dirk Walther, Diane Beck, and Li Fei-fei. Hierarchical mixture of classification experts uncovers interactions between brain regions. In NIPS. 2009.\\n\\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\\n\\narXiv preprint arXiv:1409.2329, 2014.\\n\\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016.\\n\\nAppendices A Load-Balancing Loss\\n\\nAs discussed in section 4, for load-balancing purposes, we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples. Unfortunately, the number of examples received by an expert is a discrete quantity, so it can not be used in backpropagation. Instead, we define a smooth estimator Load(X) of the number of examples assigned to each expert for a batch X of inputs. The smoothness allows us to back-propagate gradients through the estimator. This is the purpose of the noise term in the gating function. We define P(x, i) as the probability that G(x)iis nonzero, given a new random choice of noise on element i, but keeping the already-sampled choices of noise on the other elements. To compute P(x, i), we note that the G(x)iis nonzero if and only if H(x)iis greater than the k th-greatest element of H(x) excluding itself. The probability works out to be:\\n\\n$$P(x,i)=P r\\\\Big((x\\\\cdot W_{g}){i}+S t a n d x N o r a l()\\\\cdot S o f t p u s((x\\\\cdot W{n o i s e})_{i})$$ $$>k t h_e x c l u d i n(H(x),k,i)\\\\Big)$$ $$({\\\\boldsymbol{8}})$$\\n\\n$$(9)$$ $$(10)$$ (8) Where kth_excluding(v, k, i) means the kth highest component of v, excluding component i. Simplifying, we get:\\n\\n$$P(x,i)=\\\\Phi{\\\\Big(}{\\\\frac{(x\\\\cdot W_{g}){i}-k t h_c x c l u d i n g(H(x),k,i)}{S o f t p l u s((x\\\\cdot W{n o i s e})_{i})}}{\\\\Big)}$$\\n\\nWhere Φ is the CDF of the standard normal distribution.\\n\\n$$L o a d(X){i}=\\\\sum{x\\\\in X}P(x,i)$$\\n\\n$$(11)$$ P(x, i) (10) We can now define the load loss to be the square of the coefficient of variation of the load vector, multiplied by a hand-tuned scaling factor wload.\\n\\n$$L_{l o a d}(X)=w_{l o a d}\\\\cdot C V(L o a d(X))^{2}$$ Lload(X) = wload · CV (Load(X))2(11) Initial Load Imbalance: To avoid out-of-memory errors, we need to initialize the network in a state of approximately equal expert load (since the soft constraints need some time to work). To accomplish this, we initialize the matrices Wg and Wnoise to all zeros, which yields no signal and some noise.\\n\\nExperiments: We trained a set of models with identical architecture (the MoE-256 model described in Appendix C), using different values of wimportance and wload. We trained each model for 10 epochs, then measured perplexity on the test set. We also measured the coefficients of variation in Importance and Load, as well as ratio of the load on the most overloaded expert to the average load. This last value is significant for load balancing purposes on distributed hardware. All of these metrics were averaged over several training batches.\\n\\nwimportance wload Test Perplexity CV (Importance(X)) CV (Load(X)) max(Load(X)) mean(Load(X)) 0.0 0.0 39.8 3.04 3.01 17.80 0.2 0.0 35.6 0.06 0.17 1.47 0.0 0.2 35.7 0.22 0.04 1.15 0.1 0.1 35.6 0.06 0.05 1.14 0.01 0.01 35.7 0.48 0.11 1.37 1.0 1.0 35.7 0.03 0.02 1.07\\n\\nTable 6: Experiments with different combinations of losses.\\n\\n13 Results: Results are reported in Table 6. All the combinations containing at least one the two losses led to very similar model quality, where having no loss was much worse. Models with higher values of wload had lower loads on the most overloaded expert.\\n\\nB Hierachical Mixture Of Experts\\n\\nIf the number of experts is very large, we can reduce the branching factor by using a two-level hierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted combination of \"experts\", each of which is itself a secondary mixture-of-experts with its own gating network.3If the hierarchical MoE consists of a groups of b experts each, we denote the primary gating network by Gprimary, the secondary gating networks by (G1, G2..Ga), and the expert networks by (E0,0, E0,1..Ea,b). The output of the MoE is given by:\\n\\n$$y_{H}=\\\\sum_{i=1}^{a}\\\\sum_{j=1}^{b}G_{p r i m a r y}(x){i}\\\\cdot G{i}(x){j}\\\\cdot E{i,j}(x)$$ $$(12)$$ $$(13)$$ $$(14)$$\\n\\nOur metrics of expert utilization change to the following:\\n\\n$$I m p o r t a n c e_{H}(X){i,j}=\\\\sum{x\\\\in X}G_{p r i m a r y}(x){i}\\\\cdot G{i}(x){j}$$ $$L o a d{H}(X){i,j}={\\\\frac{L o a d{p r i m a r y}(X){i}\\\\cdot L o a d{i}(X^{(i)})_{j}}{|X^{(i)}|}}$$\\n\\nLoadprimary and Loadi deonte the Load functions for the primary gating network and i th secondary gating network respectively. X(i) denotes the subset of X for which Gprimary(x)i > 0.\\n\\nIt would seem simpler to let LoadH(X)i,j = Loadi(Xi)j , but this would not have a gradient with respect to the primary gating network, so we use the formulation above.\\n\\nC 1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS C.1 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS Model Architecture: Our model consists of five layers: a word embedding layer, a recurrent Long Short-Term Memory (LSTM) layer (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), a MoE layer, a second LSTM layer, and a softmax layer. The dimensionality of the embedding layer, the number of units in each LSTM layer, and the input and output dimensionality of the MoE layer are all equal to 512. For every layer other than the softmax, we apply drouput (Zaremba et al., 2014) to the layer output, dropping each activation with probability DropP rob, otherwise dividing by (1 − DropP rob). After dropout, the output of the previous layer is added to the layer output.\\n\\nThis residual connection encourages gradient flow (He et al., 2015).\\n\\nMoE Layer Architecture: Each expert in the MoE layer is a feed forward network with one ReLU-activated hidden layer of size 1024 and an output layer of size 512. Thus, each expert contains [512 ∗ 1024] + [1024 ∗ 512] = 1M parameters. The output of the MoE layer is passed through a sigmoid function before dropout. We varied the number of experts between models, using ordinary MoE layers with 4, 32 and 256 experts and hierarchical MoE layers with 256, 1024 and 4096 experts.\\n\\nWe call the resulting models MoE-4, MoE-32, MoE-256, MoE-256-h, MoE-1024-h and MoE-4096h. For the hierarchical MoE layers, the first level branching factor was 16, corresponding to the number of GPUs in our cluster. We use Noisy-Top-K Gating (see Section 2.1) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers. Thus, each example is processed by exactly 4 experts for a total of 4M ops/timestep. The two LSTM layers contribute 2M ops/timestep each for the desired total of 8M.\\n\\n3 We have not found the need for deeper hierarchies.\\n\\nComputationally-Matched Baselines: The MoE-4 model does not employ sparsity, since all 4 experts are always used. In addition, we trained four more computationally-matched baseline models with no sparsity:\\n\\nMoE-1-Wide: The MoE layer consists of a single \"expert\" containing one ReLU-activated hidden layer of size 4096.\\n\\nMoE-1-Deep: The MoE layer consists of a single \"expert\" containing four ReLU-activated hidden layers, each with size 1024.\\n\\n4xLSTM-512: We replace the MoE layer with two additional 512-unit LSTM layers.\\n\\nLSTM-2048-512: The model contains one 2048-unit LSTM layer (and no MoE). The output of the LSTM is projected down to 512 dimensions (Sak et al., 2014). The next timestep of the LSTM receives the projected output. This is identical to one of the models published in (Jozefowicz et al., 2016). We re-ran it to account for differences in training regimen, and obtained results very similar to the published ones. Training: The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3. Each batch consisted of a set of sentences totaling roughly 300,000 words. In the interest of time, we limited training to 10 epochs, (27,000 steps). Training took 12-16 hours for all models, except for MoE-4, which took 18 hours (since all the expert computation was performed on only 4 of 16 GPUs). We used the Adam optimizer (Kingma & Ba, 2015). The base learning rate was increased linearly for the first 1000 training steps, and decreased after that so as to be proportional to the inverse square root of the step number. The Softmax output layer was trained efficiently using importance sampling similarly to the models in (Jozefowicz et al., 2016). For each model, we performed a hyper-parmeter search to find the best dropout probability, in increments of 0.1.\\n\\nTo ensure balanced expert utilization we set wimportance = 0.1 and wload = 0.1, as described in Section 4 and Appendix A.\\n\\nResults: We evaluate our model using perplexity on the holdout dataset, used by (Chelba et al., 2013; Jozefowicz et al., 2016). We follow the standard procedure and sum over all the words including the end of sentence symbol. Results are reported in Table 7. For each model, we report the test perplexity, the computational budget, the parameter counts, the value of DropP rob, and the computational efficiency.\\n\\nModel Test Test ops/timestep #Params excluding Total Drop- TFLOPS Perplexity Perplexity (millions) embed. & softmax #Params P rob per GPU 10 epochs (final) (millions) (billions) (observed) Kneser-Ney 5-gram* 67.6 0.00001 1.8 LSTM-512-512* 54.1 2.4 2.4 0.8 0.1 LSTM-1024-512* 48.2 4.7 4.7 0.8 0.1 LSTM-2048-512* 45.0 43.7 9.4 9.4 0.8 0.1 0.61 LSTM-2048-512 44.7 9.4 9.4 0.8 0.1 1.21 4xLSTM-512 46.0 8.4 8.4 0.8 0.1 1.07 MoE-1-Wide 46.1 8.4 8.4 0.8 0.1 1.29 MoE-1-Deep 45.7 8.4 8.4 0.8 0.1 1.29 MoE-4 45.0 8.4 8.4 0.8 0.1 0.52 MoE-32 39.7 8.4 37.8 0.9 0.1 0.87 MoE-256 35.7 8.6 272.9 1.1 0.1 0.81 MoE-256-h 36.0 8.4 272.9 1.1 0.1 0.89 MoE-1024-h 34.6 8.5 1079.0 1.9 0.2 0.90 MoE-4096-h 34.1 8.9 4303.4 5.1 0.2 0.74 2xLSTM-8192-1024* 34.7 30.6 151.0 151.0 1.8 0.25 1.09 MoE-34M 31.3 33.8 4313.9 6.0 0.3 1.22 MoE-143M 28.0 142.7 4371.1 6.0 0.4 1.56\\n\\nC.2 More Expensive Models\\n\\nWe ran two additional models (MoE-34M and MoE-143M) to investigate the effects of adding more computation in the presence of a large MoE layer. These models have computation budgets of 34M and 143M ops/timestep. Similar to the models above, these models use a MoE layer between two LSTM layers. The dimensionality of the embedding layer, and the input and output dimensionality of the MoE layer are set to 1024 instead of 512. For MoE-34M, the LSTM layers have 1024 units.\\n\\nFor MoE-143M, the LSTM layers have 4096 units and an output projection of size 1024 (Sak et al., 2014). MoE-34M uses a hierarchical MoE layer with 1024 experts, each with a hidden layer of size 2048. MoE-143M uses a hierarchical MoE layer with 256 experts, each with a hidden layer of size 8192. Both models have 4B parameters in the MoE layers. We searched for the best DropP rob for each model, and trained each model for 10 epochs.\\n\\nThe two models achieved test perplexity of 31.3 and 28.0 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table 7. The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by 18%.\\n\\nD 100 BILLION WORD GOOGLE NEWS CORPUS - EXPERIMENTAL DETAILS Model Architecture: The models are similar in structure to the 8-million-operations-per-timestep models described in the previous section. We vary the number of experts between models, using an ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384, 65536 and 131072 experts. For the hierarchical MoE layers, the first level branching factors are 32, 32, 64, 128, 256 and 256, respectively.\\n\\nTraining: Models are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models, which are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the parameters. For all models, training batch sizes are approximately 2.5 million words. Models are trained once-through over about 100 billion words. We implement several memory optimizations in order to fit up to 1 billion parameters per GPU. First, we do not store the activations of the hidden layers of the experts, but instead recompute them on the backwards pass. Secondly, we modify the optimizer on the expert parameters to require less auxiliary storage: The Adam optimizer (Kingma & Ba, 2015) keeps first and second moment estimates of the perparameter gradients. This triples the required memory. To avoid keeping a first-moment estimator, we set β1 = 0. To reduce the size of the second moment estimator, we replace it with a factored approximation. For a matrix of parameters, instead of maintaining a full matrix of second-moment estimators, we maintain vectors of row-wise and column-wise averages of that matrix. At each step, the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one. This technique could similarly be applied to Adagrad (Duchi et al., 2010).\\n\\nModel Test Test ops/timestep #Params excluding Total TFLOPS Perplexity Perplexity (millions) embed. & softmax #Params per GPU .1 epochs 1 epoch (millions) (billions) (observed) Kneser-Ney 5-gram 67.1 45.3 0.00001 76.0 4xLSTM-512 54.5 47.0 8.4 8.4 0.1 1.23 MoE-32 48.5 40.4 8.4 37.8 0.1 0.83 MoE-256-h 42.8 35.3 8.4 272.9 0.4 1.11 MoE-1024-h 40.3 32.7 8.5 1079.0 1.2 1.14 MoE-4096-h 38.9 30.9 8.6 4303.4 4.4 1.07 MoE-16384-h 38.2 29.7 8.8 17201.0 17.3 0.96 MoE-65536-h 38.2 28.9 9.2 68791.0 68.9 0.72 MoE-131072-h 39.8 29.2 9.7 137577.6 137.7 0.30\\n\\nResults: We evaluate our model using perplexity on a holdout dataset. Results are reported in Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE model than for the baseline model. It is notable that the measured computational efficiency of the largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely a result of the fact that, for purposes of comparison to the other models, we did not increase the training batch size proportionally to the number of GPUs. For comparison, we include results for a computationally matched baseline model consisting of 4 LSTMs, and for an unpruned 5-gram model with Kneser-Ney smoothing (Kneser & Ney, 1995).4\\n\\nE Machine Translation - Experimental Details\\n\\nModel Architecture for Single Language Pair MoE Models: Our model is a modified version of the GNMT model described in (Wu et al., 2016). To reduce computation, we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively. We insert MoE layers in both the encoder (between layers 2 and 3) and the decoder (between layers 1 and 2). We use an attention mechanism between the encoder and decoder, with the first decoder LSTM receiving output from and providing input for the attention 5. All of the layers in our model have input and output dimensionality of 512. Our LSTM layers have 2048 hidden units, with a 512-dimensional output projection. We add residual connections around all LSTM and MoE layers to encourage gradient flow (He et al., 2015). Similar to GNMT, to effectively deal with rare words, we used subword units (also known as \"wordpieces\") (Schuster & Nakajima, 2012) for inputs and outputs in our system.\\n\\nWe use a shared source and target vocabulary of 32K wordpieces. We also used the same beam search technique as proposed in (Wu et al., 2016).\\n\\nWe train models with different numbers of experts in the MoE layers. In addition to a baseline model with no MoE layers, we train models with flat MoE layers containing 32 experts, and models with hierarchical MoE layers containing 512 and 2048 experts. The flat MoE layers use k = 4 and the hierarchical MoE models use k = 2 at each level of the gating network. Thus, each input is processed by exactly 4 experts in each MoE layer. Each expert in the MoE layer is a feed forward network with one hidden layer of size 2048 and ReLU activation. Thus, each expert contains [512 ∗ 2048] + [2048 ∗ 512] = 2M parameters. The output of the MoE layer is passed through a sigmoid function. We use the strictly-balanced gating function described in Appendix F.\\n\\nModel Architecture for Multilingual MoE Model: We used the same model architecture as for the single-language-pair models, with the following exceptions: We used noisy-top-k gating as described in Section 2.1, not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts, and k = 2. Each expert has a larger hidden layer of size 8192. This doubles the amount of computation in the MoE layers, raising the computational budget of the entire model from 85M to 102M ops/timestep. Training: We trained our networks using the Adam optimizer (Kingma & Ba, 2015). The base learning rate was increased linearly for the first 2000 training steps, held constant for an additional 8000 steps, and decreased after that so as to be proportional to the inverse square root of the step number. For the single-language-pair models, similarly to (Wu et al., 2016), we applied dropout (Zaremba et al., 2014) to the output of all embedding, LSTM and MoE layers, using DropP rob = 0.4. Training was done synchronously on a cluster of up to 64 GPUs as described in section 3. Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU.\\n\\nTo ensure balanced expert utilization we set wimportance = 0.01 and wload = 0.01, as described in Section 4 and Appendix A. Metrics: We evaluated our models using the perplexity and the standard BLEU score metric. We reported tokenized BLEU score as computed by the multi-bleu.pl script, downloaded from the public implementation of Moses (on Github), which was also used in (Luong et al., 2015a). Results: Tables 2, 3 and 4 in Section 5.3 show comparisons of our results to other published methods. Figure 4 shows test perplexity as a function of number of words in the (training data\\'s) source sentences processed for models with different numbers of experts. As can be seen from the Figure, as we increased the number of experts to approach 2048, the test perplexity of our model continued to improve.\\n\\nWe found that the experts indeed become highly specialized by syntax and/or semantics, as can be seen in Table 9. For example, one expert is used when the indefinite article \"a\" introduces the direct object in a verb phrase indicating importance or leadership.\\n\\nExpert 381 Expert 752 Expert 2004 ... with researchers , ... ... plays a core ... ... with rapidly growing ... ... to innovation . ... plays a critical ... ... under static conditions ... ... tics researchers . ... provides a legislative ... ... to swift ly ... ... the generation of ... ... play a leading ... ... to dras tically ... ... technology innovations is ... ... assume a leadership ... ... the rapid and ... ... technological innovations , ... ... plays a central ... ... the fast est ... ... support innovation throughout ... ... taken a leading ... ... the Quick Method ... ... role innovation will ... ... established a reconciliation ... ... rec urrent ) ... ... research scienti st ... ... played a vital ... ... provides quick access ... ... promoting innovation where ... ... have a central ... ... of volatile organic ... ... ... ...\\n\\nDue to some peculiarities in our infrastructure which have since been fixed, at the time we ran some of the machine translation experiments, our models ran faster if every expert received exactly the same batch size. To accommodate this, we used a different gating function which we describe below. Recall that we define the softmax gating function to be:\\n\\n$$G_{\\\\sigma}(x)=S o f t m a x(x\\\\cdot W_{g})$$ Gσ(x) = Sof tmax(x · Wg) (15) Sparse Gating (alternate formulation): To obtain a sparse gating vector, we multiply Gσ(x) component-wise with a sparse mask M(Gσ(x)) and normalize the output. The mask itself is a function of Gσ(x) and specifies which experts are assigned to each input example:\\n\\n$$(15)$$ $$G(x){i}=\\\\frac{G{\\\\sigma}(x){i}M(G{\\\\sigma}(x)){i}}{\\\\sum{j=1}^{n}G_{\\\\sigma}(x){j}M(G{\\\\sigma}(x))_{j}}\\\\tag{1}$$ $$(17)$$\\n\\n$$(16)$$\\n\\nTop-K Mask: To implement top-k gating in this formulation, we would let M(v) = T opK(v, k), where:\\n\\n$$T o p K(v,k){i}=\\\\begin{cases}1&{\\\\mathrm{if~}}v{i}{\\\\mathrm{~is~in~the~top~}}k{\\\\mathrm{~elements~of~}}v.\\\\ 0&{\\\\mathrm{otherwise.}}\\\\end{cases}$$ 0 otherwise. (17) Batchwise Mask: To force each expert to receive the exact same number of examples, we introduce an alternative mask function, Mbatchwise(X, m), which operates over batches of input vectors.\\n\\nInstead of keeping the top k values per example, we keep the top m values per expert across the training batch, where m = k|X| n, so that each example is sent to an average of k experts.\\n\\n$$M_{b a t c h w i s e}(X,m){j,i}=\\\\begin{cases}1&\\\\text{if}X{j,i}\\\\text{is in the top}m\\\\text{values for to expert}i\\\\ 0&\\\\text{otherwise}\\\\end{cases}$$\\n\\nAs our experiments suggest and also observed in (Ioffe & Szegedy, 2015), using a batchwise function during training (such as Mbatchwise) requires modifications to the inference when we may not have a large batch of examples. Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask. We use the following mask at inference time:\\n\\n$$(18)$$ $$M_{threshold}(x,T){i}=\\\\begin{cases}1&\\\\text{if}x{i}>T_{i}\\\\ 0&\\\\text{otherwise}\\\\end{cases}\\\\tag{1}$$ $$(19)$$\\n\\nTo learn the threshold values, we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical.\\n\\n$$L_{batchwise}(X,T,m)=\\\\sum_{j=1}^{|X|}\\\\sum_{i=1}^{n}(M_{threshold}(x,T){i}-M{batchwise}(X,m){j,i})(X{j,i}-T_{i})\\\\tag{20}$$\\n\\nG ATTENTION FUNCTION The attention mechanism described in GNMT (Wu et al., 2016) involves a learned \"Attention Function\" A(xi, yj ) which takes a \"source vector\" xi and a \"target vector\" yj , and must be computed for every source time step i and target time step j. In GNMT, the attention function is implemented as a feed forward neural network with a hidden layer of size n. It can be expressed as:\\n\\n$$A_{G N M T}(x_{i},y_{j})=\\\\sum_{d=1}^{n}V_{d}t a n h((x_{i}U){d}+(y{j}W)_{d})$$\\n\\nWhere U and W are trainable weight matrices and V is a trainable weight vector.\\n\\nFor performance reasons, in our models, we used a slightly different attention function:\\n\\n$$A(x_{i},y_{j})=\\\\sum_{d=1}^{n}V_{d}tanh((x_{i}U){d})tanh((y{j}W)_{d})\\\\tag{22}$$\\n\\nWith our attention function, we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications. We found little difference in quality between the two functions.\\n\\n$$(21)$$'),\n",
       " Document(metadata={'source': 'Sample_Docs_Markdown\\\\2202.09368v2.md'}, page_content='Mixture-Of-Experts With Expert Choice Routing\\n\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon Google, Mountain View, CA, USA {yanqiz, taole, hanxiaol, dunan, huangyp, vzhao, adai, zhifengc, qvl, jlaudon}@google.com\\n\\nAbstract\\n\\nSparsely-activated Mixture-of-experts (MoE) models allow the number of parameters to greatly increase while keeping the amount of computation for a given token or a given sample unchanged. However, a poor expert routing strategy can cause certain experts to be under-trained, leading to an expert being under or over-specialized. Prior work allocates a fixed number of experts to each token using a top-k function regardless of the relative importance of different tokens. To address this, we propose a heterogeneous mixture-of-experts employing an expert choice method. Instead of letting tokens select the top-k experts, we have experts selecting the top-k tokens. As a result, each token can be routed to a variable number of experts and each expert can have a fixed bucket size. We systematically study pre-training speedups using the same computational resources of the Switch Transformer top-1 and GShard top-2 gating of prior work and find that our method improves training convergence time by more than 2×. For the same computational cost, our method demonstrates higher performance in fine-tuning 11 selected tasks in the GLUE and SuperGLUE benchmarks. For a smaller activation cost, our method outperforms the T5 dense model in 7 out of the 11 tasks.\\n\\n1 Introduction\\n\\nScaling up model capacity, dataset size, and training time has demonstrated huge success in enhancing the performance of computer vision architectures [4, 11, 13, 14] as well as neural language models [2, 20, 26, 27]. The final model quality has been found to have a power-law relationship with the amount of data, model size, and compute time [16, 20]. However, training efficiency, which is defined as the total amount of computation used to achieve superior model quality than the state of the art system [21], should receive greater attention as we increase our efforts towards green AI [29].\\n\\nSparsely gated mixture-of-experts [31] (MoE) provides an effective way to scale model capacity given a fixed computational cost, and has recently played an important role in increasing the training efficiency of large-scale language models [10, 21]. MoE operate by adopting a number of experts, each as a sub-network, and by activating only one or a few experts for each input token. A gating network must be chosen and optimized in order to route each token to the most suited expert(s). For example, recent work has implemented sparse routing via k-means clustering [12], linear assignment to maximize token-expert affinities [22], or hashing [8, 28]. Many of the prior work use a routing strategy concerning the token choice, where each token selects the best one or two experts.\\n\\nWe argue that the independent token choice of prior work often leads to an imbalanced load of experts, which causes training inefficiency and sub-optimal training of the model. In order to mitigate this\\n\\nissue, previous sparsely gated networks introduce additional auxiliary losses as regularization to prevent too many tokens being routed to a single expert, but the effectiveness is still limited. Recent approaches [8, 22, 28] explore alternative strategies for routing, but they focus on pre-training only and do not demonstrate performance gain on downstream tasks. Moreover, none of the previous methods consider allocating a variable number of experts to each token based on importance, which can be beneficial.\\n\\nWe propose a very simple yet effective routing method we are calling expert choice. Unlike conventional MoE where tokens select one or two top-scoring experts, our method lets each expert pick the top-k tokens. Our method guarantees perfect load balancing, allows a variable number of experts for each token, and achieves substantial gains in training efficiency and downstream performance as demonstrated in our experiments. Our major contributions include:\\n\\nWe identify common pitfalls in conventional MoE such as load imbalance as described in Section 3.1. We then propose a heterogeneous, expert choice method to provide a fluid allocation of model parameters based on a learnt token-to-expert importance. This method intrinsically guarantees load balance without imposing an auxiliary loss.\\n\\nWe show our method provides over 2× faster training convergence in a 8B/64E (8 billion activated parameters, 64 experts) model, compared to the top-1 and top-2 gating counterparts in Switch Transformer [10] and GShard [21].\\n\\nWe show our method demonstrates strong scaling when increasing the number of experts from 16 to 128, evaluated in training perplexity.\\n\\nWe show our method demonstrates strong performance on downstream tasks selected from GLUE and SuperGLUE at all the evaluated scales. More specifically, our 8B/64E model outperforms a T5 11B dense model in 7 out of 11 tasks evaluated.\\n\\n2 Related Work\\n\\nScaling: Various approaches have been proposed to scale up neural network capacity to improve performance. Recent works have successfully scaled models to billions of parameters via various forms of model parallelism [2, 21, 26, 27, 33]. Model parallelism [30] splits weights and tensors across multiple cores while pipeline parallelism [18, 24] splits different layers across devices with micro-batches pipelined to the different layers. To enable continued scaling of neural networks, improving model training and serving efficiency has become a critical research area.\\n\\nConditional Computation: Computation decisions can be made dynamically based on the input [23, 25]. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation, by activating certain parameters and computation on demand, on a per-example or per-token basis [3]. Conditional convolution layers [1] with task-specific gating has been used to combat catastrophic forgetting when a sequence of learning problems are optimized. The gating decisions may be binary or sparse and continuous, stochastic or deterministic.\\n\\nMixture of Experts: Sparsely-gated MoE [31] is the first model to demonstrate massive improvements in model capacity, training time, or model quality with gating. Switch Transformer [10] simplifies the gating by selecting only the top expert per token using a softmax over the hidden state and demonstrates better scaling than previous work. All the prior work requires an auxiliary loss to explicitly encourage balancing. This loss term has to be carefully weighted to not overwhelm the primary loss. However, auxiliary loss does not guarantee balancing and a hard capacity factor has to be imposed. As a result, many tokens can still be unprocessed by the MoE layer. Hard MoE [12] with a single decoding layer can be efficiently trained to good effect on large scale hashtag prediction tasks.\\n\\nBase Layers [22] formulate a linear assignment that maximizes token-expert affinities while ensuring each expert receives an equal number of tokens. Hash layers [8, 28] devise hashing techniques on input tokens. However, the evaluations are limited to pre-training perplexity. THOR [? ] randomly activates experts during training and inference and is trained with a consistency regularization loss.\\n\\nTHOR has demonstrated strong performance on translation tasks. Different from these prior works, our method is a learnt method that enables heterogeneous MoE and effectively improves downstream fine-tuning performance.\\n\\n3 Method\\n\\nWe first identify a few pitfalls in the routing method of conventional mixture-of-experts (MoE) models and then present our method using expert choice to tackle these problems.\\n\\n3.1 Pitfalls Of Token-Choice Routing\\n\\nMoE can be computationally advantageous compared to a dense model, a routing strategy must be used to assign each token to the most-suited experts. Conventional MoE models employ token-choice routing which independently selects the top-k experts for each token [10, 21, 31]. We argue that this strategy has a few pitfalls that lead to sub-optimal training.\\n\\nLoad Imbalance: Token-choice routing often lead to poor load balancing across experts. That is, some experts may be trained with most tokens, leaving the remaining experts under-utilized. Experts can be under specialized because a lot of model capacity in the under-utilized experts are wasted. On the other side, some tokens will not be processed, since over-utilized experts can only take a maximum number of tokens at each step in order to avoid running out of memory. Load imbalance can also hurt step latency, thus inference time, as the step latency can be determined by the most loaded expert. Previous methods add an auxiliary loss on load balancing to mitigate the issue. However, this auxiliary loss does not guarantee a balanced load, especially during the important early stages of training. Indeed, we empirically observe that the over-capacity ratio can reach 20%–40% for some experts in token choice routing, indicating that a significant portion of the tokens routed to these experts will be dropped.\\n\\nUnder Specialization: Each MoE layer uses a gating network to learn token-to-expert affinity.\\n\\nIdeally, the learnt gating network should produce the affinity such that similar or relevant tokens are routed to the same expert. A sub-optimal strategy can produce redundant experts and/or experts that are not sufficiently specialized. Under specialization may result by imposing an large auxiliary loss which favors more load balanced but less effective routing. Finding the right balance on the auxiliary loss to promote both load balancing and specialization is challenging for token-choice routing.\\n\\nSame Compute for Every Token: Finally, in a token-choice strategy each token receives exactly k experts and therefore occupies the same amount of compute. We hypothesize that this is not necessary nor desired. Instead, a MoE model should flexibly allocate its compute resource based on the complexity of the input. Motivated by the aforementioned observations, we next describe a simple yet effective method which produces load balanced assignments based on expert choice.\\n\\n3.2 Heterogeneous Moe Via Expert Choice\\n\\nDifferent from conventional routing, an expert choice method independently selects top-k tokens for each expert, where k is a fixed expert capacity (i.e. the number of tokens each expert can take).\\n\\nDespite its simplicity, expert choice achieves perfect load balancing by design. It also enables more flexible allocation of model compute since tokens can be received by a variable number of experts.\\n\\n$$k={\\\\frac{n\\\\times c}{c}}$$\\n\\ne(1) where n is the total number of tokens in the input batch (such as batch size × sequence length), c is the capacity factor, and e is the number of experts. The capacity factor c denotes on average how many experts are utilized by a token. Given input token representations X ∈ R n×d where d is the model hidden dimension, our method produces a token-to-expert assignment denoted by three output matrices I, G and P. The matrix I is an index matrix where I[i, j] specifies j-th selected token of the i-th expert. The gating matrix G ∈ R e×k denotes the weight of expert for the selected token, and P ∈ R e×k×n refers to an one-hot version of I that will be used to gather tokens for each expert.\\n\\nThese matrices are computed using a gating function,\\n\\n$$\\\\begin{array}{l l}{{S=\\\\mathrm{Softmax}(X\\\\cdot W_{g}),}}&{{S\\\\in\\\\mathbb{R}^{n\\\\times e}}}\\\\ {{G,I=\\\\mathrm{TopK}(S^{\\\\top},k),P=\\\\mathrm{Onehot}(I)}}\\\\end{array}$$\\n\\n$$(2)$$\\n\\nwhere S denotes the token-to-expert affinity scores, Wg ∈ R d×e denotes the expert embeddings, and T opK() selects the k largest entries for each row of S>.\\n\\nSimilar to Switch Transformer [10] and GShard [21], we apply mixture of experts and the gating function in the dense feed-forward (FFN) layer, as it is the most computationally expensive part in a Transformer-based network. The input to the gated FFN, denoted by Xin ∈ R e×k×d, is produced using the permutation matrix P. Here Xin[i] ∈ R k×d denotes the input of the i-th expert. Similarly, let W1 and W2 denote the parameters of gated FFN in which W1[i] and W2[i] ∈ R d×d 0denote the parameter matrices of the i-th expert. We compute the output of each expert Xe[i] as follows, $$X_{i n}=P\\\\cdot X$$ $$\\\\forall i:\\\\ \\\\ X_{e}[i]=\\\\mathrm{{GeLU}}(X_{i n}[i]\\\\cdot W_{1}[i])\\\\cdot W_{2}[i]^{\\\\top}$$\\n\\n(3) We omit the bias terms here for brevity. The finally output of the gated FFN layer Xout ∈ R n×dcan be obtained given Xe, the permutation and gating matrices P and G,\\n\\n$$({\\\\mathfrak{I}})$$ $$X_{\\\\mathrm{out}}[l,d]=\\\\sum_{i,j}P[i,j,l]\\\\;G[i,j]\\\\;X_{e}[i,j,d]$$ $$(4)$$\\n\\nP[i, j, l] G[i, j] Xei, j, d Both Xe and Xout can be efficiently computed using Einstein summation (einsum) operations.\\n\\n3.3 Expert Choice With Additional Constraint\\n\\nWe also consider regularizing our expert choice routing by limiting the maximum number of experts for each token. We are interested in whether adding this constraint improves pre-training and finetuning results. More importantly, it helps analyzing to what degree using a variable number of experts per token affects the model performance.\\n\\nLet A ∈ R e×n be a positive matrix where A[i, j] represents whether the i-th expert selects j-th token.\\n\\nWe solve the following entropy-regularized linear programming problem\\n\\n$$\\\\begin{array}{l}{{\\\\operatorname{max}{A}\\\\;\\\\left\\\\langle S^{\\\\top},A\\\\right\\\\rangle+\\\\lambda H(A)}}\\\\ {{\\\\forall i:\\\\;\\\\sum{j^{\\\\prime}}A[i,j^{\\\\prime}]=k;\\\\;\\\\;\\\\forall j:\\\\;\\\\sum_{i^{\\\\prime}}A[i^{\\\\prime},j]\\\\leq b;\\\\;\\\\;\\\\forall i,j:\\\\;0\\\\leq A[i,j]\\\\leq1}}\\\\end{array}$$ $$\\\\mathbf{s.t.}$$ s.t. ∀i : where < S>, A >* denotes the inner product, H(A) is the sum of element-wise entropy1, and b > 0 is an integer that upper bounds the selection for each token. Adding a small entropy term gives a near-integer solution while enabling a fast iterative solver we can run on TPUs. Specifically, the solution space is the intersection of three convex sets each satisfying one of the linear constraints.\\n\\nWe use Dykstra\\'s algorithm [9] that alternatively projects the intermediate solution onto one of the convex sets.2 After A is computed, the routing indices I is selected using T opK(A, k) instead.\\n\\n1H(A) = Pij −A[i, j] log A[i, j] 2We use λ = 0.001 and a maximum of 100 iterations.\\n\\nModel Type nparams nact-params L M H nheads dhead E 0.1B Dense 130M 130M - 0.1B/16E MoE 548M 145M 16 0.1B/32E MoE 1.0B 145M 12 768 3,072 12 64 32 0.1B/64E MoE 1.9B 145M 64 0.1B/128E MoE 3.7B 145M 128 8B Dense 8.7B 8.7B 32 4,096 16,384 32 128 - 8B/64E MoE 143B 9.8B 64\\n\\n3.4 Model Architecture\\n\\nAt the high level, we adopt the idea of sparsely activated Mixture-of-Experts (MoE) [31]. We use a Transformer architecture and replace the feed-forward component of every other Transformer layer with a MoE layer, following recent practice [10, 21]. Interleaving regular Transformer layers and MoE layers empirically improves model performance and training efficiency, probably because forcing some shared components in between MoE layers can mitigate the negative effects of skipping tokens. Several additional modifications adopted in recent work have been applied in our experiments. For example, we replace the standard positional embedding with per-layer relative positional bias [5].\\n\\nIn the non-MoE feed-forward sub-layers (only every other layers are MoE layers), we replace the first linear projection and the activation function with the Gated Linear Unit [6], which computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit [15] activation function.\\n\\nAs described earlier, each MoE layer consists of a group of independent feed-forward networks as denoted as \"experts\". The gating function in Eq. (2) uses a softmax activation function to model a probability distribution over these experts. This distribution denotes the preference over experts of each incoming token, which is computed similarly in a conventional gating network [10, 21, 31]. During training, each MoE layer\\'s learnable gating network described in Eq. (2) is trained to use the input to activate the best subset of experts using a top-k function along the token dimension. An \"shuffle\" stage and an \"unshuffle\" stage are inserted to the MoE layer, where the first stage gathers the tokens to their designated experts while the second stage permutes the tokens back to their original order in the input batch. This step is formulated in Eq. (3) and Eq. (4).\\n\\nSimilar to conventional MoE method, there are more parameters in the MoE layer. However, the activated model size per token can be comparable to a dense layer because during training or inference, only a limited subset of experts is activated for any given token. For instance, Switch Transformer [10] has only one activated expert while GShard [21] uses two experts per token. In our method, the number of activated experts can vary for each token but the overall computation is kept the same as the baseline architectures by fixing the capacity factor c in Eq. (1). Unless otherwise specified, we set c = 2 such that our method can be directly compared to the top-2 token-choice gating in GShard.\\n\\nWe train several variants of our architecture at the 100M scale (i.e. 100M expert size) by increasing the number of experts to understand the scaling effects of our method. We also train a 8B scale MoE model. The large MoE model is partitioned with a 2D sharding algorithm as presented in GSPMD [36], which fully exploits the 2D topology of the TPU cluster [19]. Across different scales and setups, our method outperforms related work and demonstrates strong downstream task performance on selected tasks in GLUE and SuperGLUE.\\n\\n4 Experiments 4.1 Setup\\n\\nTable 1 summarizes the hyperparameter settings of different MoE models. As a reference point, we also include the respective dense model configurations with comparable numbers of activated parameters per-token during inference. To study of the effect of scaling the number of experts, we\\n\\nstudied varying the number of experts but fixing the per expert size to 100M parameters. For example, 0.1B/64E represents the architecture of an approximately 100M parameter dense model with every other layer replaced by a 64-expert MoE layer. The MoE model degenerates into a dense transformer architecture when each MoE layer only has one expert. While nparams is the total number of trainable parameters, nact−params represents the number of activated parameters per token. L is the total number of Transformer layers, M is the model dimension, H is the hidden dimension after the projection in each transformer layer, nheads is the number of attention heads, and dhead is the hidden dimension of each attention head.\\n\\nDataset: We use the high-quality dataset from GLaM [? ] of 1.6 trillion tokens that are representative of a wide range of natural language use cases. An in-house classifier is trained to classify between a collection of curated text and other webpages and estimate the content quality of a webpage. A high-quality filtered subset of webpages are combined with books, Wikipedia pages, conversations, forums, and news to create the final dataset. The data and mixture weights can be found in Table 3 in the GLaM paper.\\n\\nModel Training: Our model training follows the setups of GLaM [? ] where a maximum sequence length of 1024 tokens is adopted. We use an Adafactor optimizer [32] with first-moment decay β1 = 0 and second-moment decay β2 = 0.99. We keep the learning rate constant for the first 10K training steps, and then decay it with an inverse square root schedule. Unlike most related works, we do not impose any auxiliary loss for load balance, such as described in Switch Transformer [10] and GShard [21]. We use the SentencePiece subword tokenizer with a vocabulary of size of 256K. The largest model (8B/64E) is trained on 512 TPU V4 chips. We use a dropout rate of 0 during training as the number of tokens in the training data corpus is much greater than the total number of tokens during training.\\n\\nModel Evaluation: We mainly focus on evaluating the finetuning performance on the 11 selected tasks from GLUE and SuperGLUE benchmarks [34, 35].\\n\\n4.2 Training Efficiency\\n\\nWe first study training efficiency and convergence. We use expert choice with a capacity factor of 2 (EC-CF2) to match the activated model size and computational cost on a per token basis in GShard top-2 gating and run both for a fixed number of steps. The results are shown in Fig. 2 (a). Comparing to GShard top-2 gating, which showed stronger performance in both perplexity in the evaluation dataset and fine-tuning on downstream tasks compared to Switch Transformer top-1 gating, EC-CF2 converges more than 2x faster during training. More specifically, EC-CF2 reaches the same perplexity as GShard top-2 in less than half the steps, and with each GShard top-2 step being 20% slower than our method. As explained in Section 3.1, the slower step time in top-2 gating is due to load imbalance\\n\\n100M/128E 100M/64E Name Metric Split ST Top-1 GS Top-2 EC-CF2 ST Top-1 GS Top-2 EC-CF2 BoolQ acc dev 77.4 76.5 76.9 73.2 77.5 79.7 CB acc dev 87.5 80.9 89.1 85.9 84.4 89.1 CoLA acc dev 78.9 84.0 86.7 64.1 85.2 88.3 MNLI acc dev 82.3 83.6 84.9 80.8 85.2 86.7 MRPC acc dev 82.6 81.0 83.1 81.3 81.3 84.4 QNLI acc dev 89.5 88.6 89.0 89.4 89.7 91.3 QQP acc dev 90.6 90.3 90.4 88.9 90.5 91.0 RTE acc dev 77.0 78.9 78.5 74.1 79.3 81.6 SST2 acc dev 92.0 94.5 94.6 91.8 95.1 95.1 WiC acc dev 67.8 65.5 68.1 64.4 67.8 65.6 WNLI acc dev 65.6 70.3 67.2 68.8 68.8 71.7 Avg - - 81.0 81.3 82.6 78.4 82.2 84.0 100M/32E 8B/64E Name Metric Split ST Top-1 GS Top-2 EC-CF2 ST Top-1 GS Top-2 EC-CF2 BoolQ acc dev 74.5 79.0 79.3 89.1 89.5 89.2 CB acc dev 80.6 81.3 92.2 93.8 96.7 100 CoLA acc dev 87.5 92.2 93.8 88.3 87.5 89.1 MNLI acc dev 83.1 87.8 88.0 90.7 91.4 91.1 MRPC acc dev 82.3 85.2 84.4 89.3 91.7 90.6 QNLI acc dev 91.6 91.9 92.5 94.5 94.9 95.0 QQP acc dev 90.1 91.5 92.0 92.1 92.5 93.8 RTE acc dev 75.0 79.1 78.1 91.0 92.2 95.2 SST2 acc dev 93.3 94.4 95.4 97.1 98.0 97.7 WiC acc dev 62.5 65.9 69.8 74.5 76.4 83.8 WNLI acc dev 65.6 64.1 68.8 78.1 82.8 92.8 Avg - - 80.6 83.5 85.0 88.9 90.3 92.6\\n\\nTable 2: Expert choice with capacity factor of 2 (EC-CF2) outperforms Top-1 gating in Switch Transformer (ST) and top-2 gating in GShard (GS) on GLUE and SuperGLUE tasks. Note that with an expert size of 100M parameters, 100M/32E works best for our method and Ghard Top-2 while 100M/128E works better for Switch Transformer Top-1. Our method consistently outperforms the others across all the scales.\\n\\nwhere some experts can receive a lot more tokens than the desired capacity. As a result, the step latency will be bottlenecked by the most loaded expert.\\n\\n4.3 Scaling The Number Of Experts 7\\n\\nAs presented in Table 1, increasing the number of experts effectively increases model capacity without increasing activated model size. We scale the number of experts while fixing the expert size to 100M parameters for both expert choice (EC) and GShard (Top-2) methods and find both methods work well in terms of perplexity on the evaluation dataset during pre-training. As demonstrated in Fig. 2 (b), having more experts consistently improves training perplexity.\\n\\n4.4 Fine-Tuning On Glue And Superglue\\n\\nTo validate whether improved perplexity directly translates to better performance in downstream tasks, we perform fine-tuning on 11 selected tasks from GLUE and SuperGLUE. We compare three MoE methods including Switch Transformer top-1 gating (ST Top-1), GShard top-2 gating (GS Top-2) and our method (EC-CF2) that matches the activation memory size and computational cost of GS Top-2. Indicated by the results in Table 2, our EC-CF2 method consistently outperforms the related methods and yields more than 2% average accuracy increase in a large 8B/64E setting. Table 3 further compares our 8B/64E model against its dense counterpart. Again, our method achieves stronger fine-tuning results, increasing the average score by 3.4 point.\\n\\nInterestingly, we observe the 100M/32E model setting works the best for both GS Top-2 and EC-CF2, even though the effective model capacity is smaller than that of 100M/64E and 100M/128E. This result indicates that a good training perplexity does not always translate to better performance of downstream tasks.\\n\\nModel BoolQ CB CoLA MNLI MRPC QNLI QQP RTE SST2 WiC WNLI Avg Dense 8B 88.2 100 86.4 91.3 86.7 94.7 91.2 92.2 97.2 75.6 78.1 89.2 EC-CF2 8B/64E 89.2 100 89.1 91.1 90.6 95.0 93.8 95.2 97.7 83.8 92.8 92.6\\n\\nTable 3: Comparison between Dense 8B and Expert Choice (EC-CF2) 8B/64E models: Our method significantly outperforms the dense model in downstream tasks.\\n\\nFigure 3: Distribution of the number of experts routed to per token in a 100M/64E model.\\n\\nLayer. Method Max # of Experts Avg acc. EC-CAP2 2 83.2 ± 0.4 EC-CAP3 3 84.0 ± 0.4 EC-CF2 - 84.0 ± 0.2 Hash Layer - 81.3 ± 0.1\\n\\n4.5 Heterogeneity Matters\\n\\nCapped Expert Choice: We regularized expert choice by limiting the maximum number of experts for each token, using the method described in Section 3.3. Table 4 reports the average accuracy on the 11 selected datasets. EC-CAP2 is the variant of our expert choice method by limiting the number of experts of each token to 2. This decreases the fine-tuning accuracy by 0.8 points on average. In addition, EC-CAP3 allows a maximum of 3 experts per token and achieves on par results compared to the vanilla expert choice method. This ablation study confirms that allowing variable number of experts per token is indeed helpful.\\n\\nVariable Experts per Token: We compute statistics on token-to-expert routing, particularly on the ratio of tokens that have been routed to a certain number of experts. According to Fig. 3, a majority of tokens have been routed to one or two experts while 23% have been routed to three or four experts and only about 3% tokens have been routed to more than 4 experts. This plot verifies our hypothesis that our method learns to allocate a variable number experts to tokens, which can be beneficial for important tokens.\\n\\n4.6 Comparison With Hash Layer\\n\\nIn this section, we compare our method with Hash Layers [28]. We use mod x to map a token ID to an expert ID. This ensures load balance and generates specialized experts. The fine-tuning results are presented in the last row in Table 4. Hashing based routing performs worse than expert choice in terms of average scores and variance. This indicates that load balancing alone does not generate all the benefits.\\n\\n4.7 Ablation\\n\\nCapacity Factor: We study the capacity factor in our expert choice method and compare the training perplexity with the baseline top-1 gating method used in Switch Transformer. As described in Eq. (1), the capacity factor determines how many experts on average each token can be routed to, thus the bucket size k of each expert. In all our previous experiments, we use a capacity factor of 2, which matches the computational footprint of the top-2 gating used in GShard method. To match the computation cost on a per-token basis fairly with top-1 gating used in Switch Transformer, we reduce the capacity factor to 1 and plot the training perplexity in Fig. 4 (a). Not surprisingly, using a smaller capacity factor yields higher perplexity, but our method still significantly outperforms top-1 gating.\\n\\nWe further push the capacity factor down to 0.5, and observe that it still outperforms the top-1 gating.\\n\\nComparison with Dense Models on Pre-training: We compare our method with dense models on pre-training. As shown in Fig. 4 (b), our method consistently outperforms the dense method in\\n\\nperplexity and convergence time. For a small expert size of 100M parameters, the benefit of sparse gating is even more significant. Orthogonal to results presented in Fig. 2 (b), where scaling the number of experts improves model performance, Fig. 4 (b) shows that increasing expert capacity also significantly increases model performance.\\n\\n5 Conclusion\\n\\nWe propose a new routing method for sparsely activated mixture-of-experts (MoE) models. This method addresses load imbalance and under-utilization of experts in conventional MoE methods, and enables selecting different numbers of experts for each token. Our model demonstrates more than 2x training efficiency improvements when compared to the state-of-the-art GShard and Switch Transformer models, and also achieves strong gains when finetuning on 11 datasets in the GLUE and SuperGLUE benchmark.\\n\\n6 Limitations\\n\\nThe expert choice method might not immediately apply to auto-regressive text generation as our current implementation takes in the past and future tokens to perform the top-k selection. One possible solution is to collect a large batch of input sequences, dispatch tokens of the same sequence into separate groups, and perform expert choice routing for each group. Another scenario where the expert choice method does not immediately apply is when the batch size becomes very small during serving or inference. A global top-k can be selected instead and we can cap the number of times each expert or token gets selected. We leave these possible improvements for future work.\\n\\nAnother long-standing issue with MoE has been the large memory footprint. Even though computational cost can be reduced using sparsely gated networks, the total number of parameters increases linearly or sub-linearly with the number of experts. Increasing the number of experts requires reservation of a large number of hardware devices. Therefore, dynamic (used) power is saved while static (reserved) power is not. Power saving techniques such as the ability to put hardware devices into low power states while not in use [17] can help with reducing the reserved power requirements.\\n\\nReferences\\n\\n[1] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and Babak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continual learning. In CVPR, pages 3930–3939. Computer Vision Foundation / IEEE, 2020.\\n\\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. In Advances in Neural Information Processing Systems.\\n\\n[3] Kyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning, 2014.\\n\\n[4] Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. CoAtNet: Marrying convolution and attention for all data sizes. In Advances in Neural Information Processing Systems, 2021.\\n\\n[5] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\\n\\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy, July 2019. Association for Computational Linguistics.\\n\\n[6] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\\'17, page 933–941. JMLR.org, 2017.\\n\\n[7] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language models with mixtureof-experts, 2021.\\n\\n[8] Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, and Angela Fan.\\n\\nTricks for training sparse translation models, 2021.\\n\\n[9] Richard L Dykstra. An iterative procedure for obtaining i-projections onto the intersection of convex sets. The annals of Probability, pages 975–984, 1985.\\n\\n[10] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021.\\n\\n[11] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. NAS-FPN: learning scalable feature pyramid architecture for object detection. In CVPR, pages 7036–7045. Computer Vision Foundation / IEEE, 2019.\\n\\n[12] Sam Gross, Marc\\'Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale weakly supervised vision, 2017.\\n\\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\\n\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision – ECCV 2016, pages 630–645, Cham, 2016. Springer International Publishing.\\n\\n[15] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs), 2016.\\n\\n[16] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically, 2017.\\n\\n[17] Ping Huang, Zuocheng Xing, Tianran Wang, Qiang Wei, Hongyan Wang, and Guitao Fu. A brief survey on power gating design. In 2010 10th IEEE International Conference on Solid-State and Integrated Circuit Technology, pages 788–790, 2010.\\n\\n[18] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\\'Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 103–112, 2019.\\n\\n[19] Norman P. Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, Cliff Young, and David A. Patterson. A domain-specific supercomputer for training deep neural networks. Commun. ACM, 63(7):67–78, 2020.\\n\\n[20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\\n\\n[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021.\\n\\n[22] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 6265–6274. PMLR, 18–24 Jul 2021.\\n\\n[24] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. New York, NY, USA, 2019. Association for Computing Machinery.\\n\\n[25] Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, Cédric Renggli, André Susano Pinto, Sylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models. In ICLR. OpenReview.net, 2021.\\n\\n[26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.\\n\\n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.\\n\\n[28] Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse models, 2021.\\n\\n[30] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\\'18, page 10435–10444, Red Hook, NY, USA, 2018. Curran Associates Inc.\\n\\n[31] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E.\\n\\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In ICLR (Poster). OpenReview.net, 2017.\\n\\n[32] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596–4604. PMLR, 10–15 Jul 2018. [23] Min Lin, Jie Fu, and Yoshua Bengio. Conditional computation for continual learning, 2019.\\n\\n[29] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai, 2019.\\n\\n[33] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.\\n\\n[34] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems. Curran Associates, Inc.\\n\\n[35] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\n\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\n\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Brussels, Belgium, November 2018. Association for Computational Linguistics.\\n\\n[36] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. GSPMD: general and scalable parallelization for ML computation graphs. CoRR, abs/2105.04663, 2021.\\n\\n7 Checklist\\n\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\\'s contributions and scope? Yes (b) Have you read the ethics review guidelines and ensured that your paper conforms to them? Yes (c) Did you discuss any potential negative societal impacts of your work? N/A. Not any. (d) Did you describe the limitations of your work? Yes (a) Did you include the code, data, and instructions needed to reproduce the main experimental results? Yes. We include details in the experiment setup to help reproduce the main results. (b) Did you specify all the training details? Yes (c) Did you report error bars? Yes (d) Did you include the amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Yes (a) If your work uses existing assets, did you cite the creators? Yes (b) Did you mention the license of the assets? No. The used dataset is not released yet. (c) Did you include any new assets either in the supplemental material or as a URL? No. The dataset is not released yet.\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you\\'re using/curating? No. Not using persons\\' data. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Yes. The dataset does not contain any personally identifiable information or offensive content.\\n\\nA Comparison On Fine-Tuning With A Dense Model\\n\\nOur 8B MoE model achieves stronger pre-training perplexity than its dense counterpart. However, a better perplexity does not always directly translate to downstream performance as demonstrated in Section 4.4. To this end, we compare fine-tuning performance of the 8B dense model and MoE model in Table 1. As shown in the table, our MoE model using expert choice routing consistently outperforms the dense model across the 11 tasks in GLUE and SuperGLUE.\\n\\nModel BoolQ CB CoLA MNLI MRPC QNLI QQP RTE SST2 WiC WNLI Avg Dense 8B 88.2 100 86.4 91.3 86.7 94.7 91.2 92.2 97.2 75.6 78.1 89.2 EC-CF2 8B/64E 89.2 100 89.1 91.1 90.6 95.0 93.8 95.2 97.7 83.8 92.8 92.6\\n\\nTable 1: Comparison between Dense 8B and Expert Choice (EC-CF2) 8B/64E models: Our method significantly outperforms the dense model in downstream tasks.\\n\\nB Capacity Factor\\n\\nWe evaluate the downstream task fine-tuning performance by varying the capacity factors. Note that a capacity factor of n indicates on average how many experts each token can be received. EC-CF2 is our baseline expert choice, which matches GShard top-2 gating computational footprint. EC-CF1, however, matches Switch Transformer top-1 gating computational footprint. EC-CF0.5 further verifies that an aggressively lowered capacity factor can provide strong enough performance, that almost matches the top-2 gating baseline.\\n\\nModel BoolQ CB CoLA MNLI MRPC QNLI QQP RTE SST2 WiC WNLI Avg Top-2 78.1 87.0 88.3 85.0 82.6 90.1 90.7 81.6 94.7 68.2 67.2 83.0±0.3 EC-CAP2 78.2 88.0 88.5 85.7 83.0 90.8 91.1 80.0 95.4 70.4 64.1 83.2±0.4 EC-CAP3 78.5 91.7 89.3 86.3 83.5 90.9 91.1 81.8 94.9 70.0 65.6 84.0±0.4 EC-CF2 79.1 89.6 89.3 86.8 84.3 91.3 91.2 81.1 95.2 68.1 68.0 84.0±0.2 EC-CF1 77.4 90.6 88.0 85.5 83.6 90.3 91.2 79.8 95.3 66.5 64.9 83.0±0.2 EC-CF0.5 77.4 89.6 86.3 85.2 82.7 91.7 91.0 79.6 94.9 67.3 63.5 83.0 ±0.05 Hash Layers 76.1 85.2 86.7 83.4 82.5 90.0 90.3 75.7 94.0 67.4 63.3 81.3±1.0\\n\\nTable 2: Comparison between different routing methods in fine-tuning of 100M/64E models. We perform 3 independent fine-tuning runs for each method and report the average results. This gives more accurate difference between the variants of expert choice method, since they achieve close fine-tuning results. We do not report averaged results in other experiments.\\n\\nC Capped Expert Choice\\n\\nAs described in Section 4.5, the maximum number of experts each token is assigned can be capped by an entropy-regularized linear programming. Figure 1 compares the validation perplexity when training the 100M/64E models using the base expert choice method (EC-BASE), expert choice capped by two experts per token (EC-CAP2), expert choice capped by three experts per token (EC-CAP3), and GShard top-2 gating.\\n\\nAs shown in the figure, restricting the number of experts to 2 degrades the perplexity compared to the base expert choice method. This suggests that a more flexible allocation of experts (e.g. more than 2 experts for a token) can enhance model expressiveness. On the other hand, our EC-CAP2 and EC-CAP3 methods still outperform the top-2 gating method by a clear margin. We believe this confirms the effectiveness of a load balanced training, provided by our method. Finally, EC-CAP3 obtains comparable perplexity to EC-BASE. As indicated by Figure 3, only a little fraction of tokens use more than 3 experts therefore we see little or no difference between EC-BASE and EC-CAP3 variants. We present the fine-tuning results of these methods in Table 2.\\n\\nD Comparison With Hash Layer\\n\\nIn this section, we compare our method with Hash Layers [? ]. We use mod x to map a token ID to an expert ID. This in some way ensures load balance and generates specialized experts. The fine-tuning results are presented in the last row in Table 2. Hashing based routing performs much worse than expert choice in terms of average scores and variance.\\n\\nE Fine-Tuning Details\\n\\nWe did a hyperparameter search for both baseline models and expert choice method. For fine-tuning of the 8B dense model, we use a constant learning rate of 0.0001 and a dropout rate of 0.1. We freeze the attention layer and feed-forward layer while leaving the embedding and layer normalization trainable. This setting has been found optimal for the 8B dense model. For MoE 8B/64E models including GShard top-2 gating and expert choice, we found continuing the learning rate from the pre-trained model while using a square root learning rate decay works better. In addition, we do not apply parameter freezing for fine-tuning MoE models. For models with 100M expert size, we use a constant learning rate of 0.0001 and no dropout is used.'),\n",
       " Document(metadata={'source': 'Sample_Docs_Markdown\\\\DeepSeekMoE_2.md'}, page_content='Deepseekmoe: Towards Ultimate Expert Specialization In\\n\\nMixture-Of-Experts Language Models\\n\\nDamai Dai∗1,2, Chengqi Deng1, Chenggang Zhao∗1,3, R.X. Xu1, Huazuo Gao1, Deli Chen1, Jiashi Li1, Wangding Zeng1, Xingkai Yu∗1,4, Y. Wu1, Zhenda Xie1, Y.K. Li1, Panpan Huang1, Fuli Luo1, Chong Ruan1, Zhifang Sui2, Wenfeng Liang1 1DeepSeek-AI 2National Key Laboratory for Multimedia Information Processing, Peking University 3Institute for Interdisciplinary Information Sciences, Tsinghua University 4National Key Laboratory for Novel Software Technology, Nanjing University {daidamai, szf}@pku.edu.cn, {wenfeng.liang}@deepseek.com https://github.com/deepseek-ai/DeepSeek-MoE\\n\\nAbstract\\n\\nIn the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top- out of experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into ones and activating from them, allowing for a more flexible combination of activated experts; (2) isolating experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5× expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations.\\n\\nFurther, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.\\n\\n1. Introduction\\n\\nRecent research and practices have empirically demonstrated that, with sufficient training data available, scaling language models with increased parameters and computational budgets can yield remarkably stronger models (Brown et al., 2020; Hoffmann et al., 2022; OpenAI, 2023; Touvron et al., 2023a). It is imperative to acknowledge, however, that the endeavor to scale models to an extremely large scale is also associated with exceedingly high computational costs. Considering the substantial costs, the Mixture-of-Experts (MoE) architecture (Jacobs et al., 1991; Jordan and Jacobs, 1994; Shazeer et al., 2017) has emerged as a popular solution. It can\\n\\nenable parameter scaling, while concurrently keeping computational costs at a modest level.\\n\\nRecent applications of MoE architectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at scaling language models to a substantial size (Du et al., 2022; Fedus et al., 2021; Lepikhin et al., 2021; Zoph, 2022), accompanied with remarkable performance. These achievements underscore the considerable potential and promise of MoE language models.\\n\\nDespite the promising potential of MoE architectures, existing MoE architectures potentially suffer from issues of knowledge hybridity and knowledge redundancy, which limit the expert specialization, i.e., each expert acquires non-overlapping and focused knowledge. Conventional MoE architectures substitute the Feed-Forward Networks (FFNs) in a Transformer with MoE layers. Each MoE layer consists of multiple experts, with each structurally identical to a standard FFN, and each token is assigned to one (Fedus et al., 2021) or two (Lepikhin et al., 2021) experts.\\n\\nThis architecture manifests two potential issues: (1) Knowledge Hybridity: existing MoE practices often employ a limited number of experts (e.g., 8 or 16), and thus tokens assigned to a specific expert will be likely to cover diverse knowledge. Consequently, the designated expert will intend to assemble vastly different types of knowledge in its parameters, which are hard to utilize simultaneously. (2) Knowledge Redundancy: tokens assigned to different experts may require common knowledge. As a result, multiple experts may converge in acquiring shared knowledge in their respective parameters, thereby leading to redundancy in expert parameters.\\n\\nThese issues collectively hinder the expert specialization in existing MoE practices, preventing them from reaching the theoretical upper-bound performance of MoE models.\\n\\nIn response to the aforementioned issues, we introduce DeepSeekMoE, an innovative MoE architecture specifically designed towards ultimate expert specialization. Our architecture involves two principal strategies: (1) Fine-Grained Expert Segmentation: while maintaining the number of parameters constant, we segment the experts into a finer grain by splitting the FFN intermediate hidden dimension. Correspondingly, keeping a constant computational cost, we also activate more fine-grained experts to enable a more flexible and adaptable combination of activated experts. Fine-grained expert segmentation allows diverse knowledge to be decomposed more finely and be learned more precisely into different experts, where each expert will retain a higher level of specialization. In addition, the increased flexibility in combining activated experts also contributes to a more accurate and targeted knowledge acquisition. (2) Shared Expert Isolation: we isolate certain experts to serve as shared experts that are always activated, aiming at capturing and consolidating common knowledge across varying contexts.\\n\\nThrough compressing common knowledge into these shared experts, redundancy among other routed experts will be mitigated. This can enhance the parameter efficiency and ensure that each routed expert retains specialized by focusing on distinctive aspects. These architectural innovations in DeepSeekMoE offer opportunities to train a parameter-efficient MoE language model where each expert is highly specialized.\\n\\nStarting from a modest scale with 2B parameters, we validate the advantages of the DeepSeekMoE architecture. We conduct evaluations on 12 zero-shot or few-shot benchmarks spanning diverse tasks. Empirical results indicate that DeepSeekMoE 2B surpasses GShard 2B (Lepikhin et al., 2021) by a substantial margin, and even matches GShard 2.9B, a larger MoE model with 1.5× expert parameters and computation. Remarkably, we find that DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with an equivalent number of parameters, which sets the strict upper bound of MoE language models. In pursuit of deeper insights, we conduct elaborate ablation studies and analysis on the expert specialization for DeepSeekMoE.\\n\\nThese studies validate the effectiveness of fine-grained expert segmentation and shared expert isolation, and provide empirical evidence supporting the assertion that DeepSeekMoE can achieve a high level of expert specialization.\\n\\nLeveraging our architecture, we subsequently scale up the model parameters to 16B and train DeepSeekMoE 16B on a large-scale corpus with 2T tokens. Evaluation results reveal that with only about 40% of computations, DeepSeekMoE 16B achieves comparable performance with DeepSeek 7B (DeepSeek-AI, 2024), a dense model trained on the same 2T corpus. We also compare DeepSeekMoE with open source models and the evaluations demonstrate that DeepSeekMoE 16B consistently outperforms models with a similar number of activated parameters by a large margin, and achieves comparable performance with LLaMA2 7B (Touvron et al., 2023b), which has approximately 2.5 times the activated parameters. Figure 1 demonstrates the evaluation results on the Open LLM Leaderboard1. Additionally, we conduct supervised fine-tuning (SFT) for alignment, transforming the model into a chat model. Evaluation results show that DeepSeekMoE Chat 16B also achieves comparable performance with DeepSeek Chat 7B and LLaMA2 SFT 7B in the chat setting. Encouraged by these results, we further undertake a preliminary endeavor to scale up DeepSeekMoE to 145B. The experimental results still validate its substantial advantages over the GShard architecture consistently. In addition, it shows performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.\\n\\nOur contributions are summarized as follows: - Architectural Innovation. We introduce DeepSeekMoE, an innovative MoE architecture aiming at achieving ultimate expert specialization, which employs two principal strategies of fine-grained expert segmentation and shared expert isolation.\\n\\nEmpirical Validation. We conduct extensive experiments to empirically validate the effectiveness of the DeepSeekMoE architecture. Experimental results validate the high 1https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard level of expert specialization in DeepSeekMoE 2B, and indicate that DeepSeekMoE 2B can nearly approach the upper bound performance for MoE models\\n\\nScalability. We scale up DeepSeekMoE to train a 16B model and show that with only about 40% of computations, DeepSeekMoE 16B achieves comparable performance with DeepSeek 7B and LLaMA2 7B. We also undertake a preliminary endeavor to scale up DeepSeekMoE to 145B, highlighting its consistent advantages over the GShard architecture and showing a comparable performance with DeepSeek 67B.\\n\\nAlignment for MoE. We successfully perform supervised fine-tuning on DeepSeekMoE 16B to create an aligned chat model, showcasing the adaptability and versatility of DeepSeekMoE 16B.\\n\\nPublic Release. In the spirit of open research, we release the model checkpoint of DeepSeekMoE 16B to the public. Notably, this model can be deployed on a single GPU with 40GB of memory without the need for quantization.\\n\\n2. Preliminaries: Mixture-Of-Experts For Transformers\\n\\nWe first introduce a generic MoE architecture commonly used in Transformer language models. A standard Transformer language model is constructed by stacking layers of standard Transformer blocks, where each block can be represented as follows:\\n\\n$$\\\\mathbf{u}{1:T}^{l}=\\\\text{Self-Att}\\\\left(\\\\mathbf{h}{1:T}^{l-1}\\\\right)+\\\\mathbf{h}{1:T}^{l-1},\\\\tag{1}$$ $$\\\\mathbf{h}{t}^{l}=\\\\text{FFN}\\\\left(\\\\mathbf{u}{t}^{l}\\\\right)+\\\\mathbf{u}{t}^{l},\\\\tag{2}$$\\n\\nwhere denotes the sequence length, Self-Att(·) denotes the self-attention module, FFN(·) denotes the Feed-Forward Network (FFN), u 1: ∈ R× are the hidden states of all tokens after the -th attention module, and h\\n\\n∈ Ris the output hidden state of the -th token after the -th Transformer block. For brevity, we omit the layer normalization in the above formulations.\\n\\nA typical practice to construct an MoE language model usually substitutes FFNs in a Transformer with MoE layers at specified intervals (Du et al., 2022; Fedus et al., 2021; Lepikhin et al., 2021; Zoph, 2022). An MoE layer is composed of multiple experts, where each expert is structurally identical to a standard FFN. Then, each token will be assigned to one (Fedus et al., 2021) or two (Lepikhin et al., 2021) experts. If the -th FFN is substituted with an MoE layer, the computation for its output hidden state h\\n\\nis expressed as: where denotes the total number of experts, FFN(·) is the -th expert FFN, , denotes the gate value for the -th expert, , denotes the token-to-expert affinity, Topk(·, ) denotes the set comprising highest affinity scores among those calculated for the -th token and all experts, and e\\n\\nis the centroid of the -th expert in the -th layer. Note that ,is sparse, indicating that only out of gate values are nonzero. This sparsity property ensures computational efficiency within an MoE layer, i.e., each token will be assigned to and computed in only experts. Also, in the above formulations, we omit the layer normalization operation for brevity.\\n\\n$$\\\\mathbf{h}{t}^{l}=\\\\sum{i=1}^{N}\\\\left(g_{i,t}\\\\operatorname{FFN}{i}\\\\left(\\\\mathbf{u}{t}^{l}\\\\right)\\\\right)+\\\\mathbf{u}{t}^{l},$$ $$g{i,t}=\\\\begin{cases}s_{i,t},&s_{i,t}\\\\in\\\\operatorname{Topk}({s_{j,t}|1\\\\leqslant j\\\\leqslant N},K),\\\\ 0,&\\\\text{otherwise},\\\\end{cases}$$ $$s_{i,t}=\\\\operatorname{Softmax}{i}\\\\left(\\\\mathbf{u}{t}^{l}\\\\,\\\\mathbf{e}_{i}^{l}\\\\right),$$\\n\\n$$(3)$$\\n\\n(4) $\\\\binom{4}{5}$ .\\n\\n3. Deepseekmoe Architecture\\n\\nOn top of the generic MoE architecture outlined in Section 2, we introduce DeepSeekMoE, which is specifically designed to exploit the potential of expert specialization. As illustrated in Figure 2, our architecture incorporates two principal strategies: fine-grained expert segmentation and shared expert isolation. Both of these strategies are designed to elevate the level of expert specialization.\\n\\n3.1. Fine-Grained Expert Segmentation\\n\\nIn scenarios where the number of experts is limited, tokens assigned to a particular expert will be more likely to cover diverse types of knowledge. As a consequence, the designated expert will intend to learn vastly different types of knowledge in its parameters, and they are hard to be simultaneously utilized. However, if each token can be routed to more experts, diverse knowledge will gain the potential to be decomposed and learned in different experts respectively. In this context, each expert can still retain a high level of expert specialization, contributing to a more focused knowledge distribution across experts.\\n\\nIn pursuit of the goal, while maintaining a consistent number of expert parameters and computational cost, we segment the experts with a finer grain. The finer expert segmentation enables a more flexible and adaptable combination of activated experts. To be specific, on top of a typical MoE architecture shown in Figure 2(a), we segment each expert FFN into smaller experts by reducing the FFN intermediate hidden dimension to 1 times its original size. Since each expert becomes smaller, in response, we also increase the number of activated experts to times to keep the same computation cost, as illustrated in Figure 2(b). With the fine-grained expert segmentation, the output of an MoE layer can be expressed as:\\n\\n$$\\\\mathbf{h}{t}^{i}=\\\\sum{i=1}^{\\\\text{mN}}\\\\left(g_{i,t}\\\\text{FFN}{i}\\\\left(\\\\mathbf{u}{t}^{i}\\\\right)\\\\right)+\\\\mathbf{u}{t}^{i},\\\\tag{6}$$ $$g{i,t}=\\\\begin{cases}s_{i,t},&s_{i,t}\\\\in\\\\text{Tokr}(s_{i,t}|1\\\\leqslant j\\\\leqslant m\\\\text{N}),m\\\\text{K}),\\\\ 0,&\\\\text{otherwise},\\\\end{cases}$$ (7) $$s_{i,t}=\\\\text{Softmax}{i}\\\\left(\\\\mathbf{u}{t}^{T}\\\\mathbf{e}_{t}^{i}\\\\right),\\\\tag{8}$$ where the total number of expert parameters is equal to $N$ times the number of parameters in a\\n\\n$$(6)$$\\n\\n(7) (8) $\\\\frac{1}{2}$\\n\\nstandard FFN, and denotes the total number of fine-grained experts. With the fine-grained expert segmentation strategy, the number of nonzero gates will also increases to .\\n\\nFrom a combinatorial perspective, the fine-grained expert segmentation strategy substantially enhances the combinatorial flexibility of activated experts. As an illustrative example, we consider the case where = 16. A typical top-2 routing strategy can yield 16 2 = 120 possible combinations. By contrast, if each expert is split into 4 smaller experts, the fine-grained routing strategy can yield 64 8 = 4, 426, 165, 368 potential combinations. The surge in combinatorial flexibility enhances the potential for achieving more accurate and targeted knowledge acquisition.\\n\\n3.2. Shared Expert Isolation\\n\\nWith a conventional routing strategy, tokens assigned to different experts may necessitate some common knowledge or information. As a result, multiple experts may converge in acquiring shared knowledge in their respective parameters, thereby resulting in redundancy in expert parameters. However, if there are shared experts dedicated to capturing and consolidating common knowledge across varying contexts, the parameter redundancy among other routed experts will be alleviated. This alleviation of redundancy will contribute to a more parameterefficient model with more specialized experts.\\n\\nTowards this objective, in addition to the fine-grained expert segmentation strategy, we further isolate experts to serve as shared experts. Regardless of the router module, each token will be deterministically assigned to these shared experts. In order to maintain a constant computational cost, the number of activated experts among the other routed experts will be decreased by , as depicted in Figure 2(c). With the shared expert isolation strategy integrated, an MoE layer in the complete DeepSeekMoE architecture is formulated as follows:\\n\\n$$\\\\mathbf{h}{t}^{l}=\\\\sum{i=1}^{K_{s}}\\\\mathrm{FFN}{i}\\\\left(\\\\mathbf{u}{t}^{l}\\\\right)+\\\\sum_{i=K_{s}+1}^{mN}\\\\left(g_{i,t}\\\\;\\\\mathrm{FFN}{i}\\\\left(\\\\mathbf{u}{t}^{l}\\\\right)\\\\right)+\\\\mathbf{u}{t}^{l},\\\\tag{9}$$ $$g{i,t}=\\\\begin{cases}s_{i,t},&s_{i,t}\\\\in\\\\mathrm{Topk}{s_{j,t}|K_{s}+1\\\\leqslant j\\\\leqslant mN},mK-K_{s}},\\\\ 0,&\\\\text{otherwise},\\\\end{cases}$$ (10) $$s_{i,t}=\\\\mathrm{Softmax}{i}\\\\left(\\\\mathbf{u}{t}^{l,T}e_{i}^{l}\\\\right).\\\\tag{11}$$ $\\\\mathbf{h}\\\\mathbf{M}\\\\mathbf{E}$ the number of closed-compact is $K_{s}$ the total number of routed currents.\\n\\nFinally, in DeepSeekMoE, the number of shared expert is , the total number of routed experts is − , and the number of nonzero gates is − .\\n\\nIt is worth noting that the prototype of shared expert isolation can be credited to Rajbhandari et al. (2022). The key distinction lies in the fact that they derive this strategy from an engineering perspective, while we approach it from an algorithmic standpoint.\\n\\n3.3. Load Balance Consideration\\n\\nAutomatically learned routing strategies may encounter the issue of load imbalance, which manifests two notable defects. Firstly, there is a risk of routing collapse (Shazeer et al., 2017), i.e., the model always selects only a few experts, preventing other experts from sufficient training.\\n\\nSecondly, if experts are distributed across multiple devices, load imbalance can exacerbate computation bottlenecks.\\n\\nExpert-Level Balance Loss. In order to mitigate the risk of routing collapse, we also employ an expert-level balance loss. The computation of the balance loss is as follows:\\n\\n$$\\\\begin{array}{l}{{{\\\\mathcal L}{\\\\mathrm{ExpBal}}=\\\\alpha{1}\\\\sum_{i=1}^{N^{\\\\prime}}f_{i}P_{i},}}\\\\ {{f_{i}=\\\\frac{N^{\\\\prime}}{K^{T}}\\\\sum_{t=1}^{T}\\\\mathds{1}\\\\,(\\\\mathrm{Token}\\\\;t\\\\;\\\\mathrm{s}\\\\mathrm{lects}\\\\;\\\\mathrm{Expert}\\\\;i),}}\\\\ {{P_{i}=\\\\frac{1}{T}\\\\sum_{t=1}^{T}s_{i,t},}}\\\\end{array}$$\\n\\n(12) $$\\\\begin{array}{l}\\\\small\\\\text{(13)}\\\\end{array}$$ = (14) $$\\\\begin{array}{l}\\\\small\\\\text{(14)}\\\\end{array}$$ .\\n\\n7 where 1 is a hyper-parameter called expert-level balance factor, ′is equal to ( − ) and ′ is equal to ( − ) for brevity. 1(·) denotes the indicator function.\\n\\nDevice-Level Balance Loss. In addition to the expert-level balance loss, we introduce a devicelevel balance loss. When aiming to alleviate computation bottlenecks, it becomes unnecessary to enforce strict balance constraints at the expert level, because excessive constraints on load balance will compromise model performance. Instead, our primary objective is to ensure balanced computation across the devices. If we partition all routed experts into groups {E1, E2, ..., E}, and deploy each group on a single device, the device-level balance loss is computed as follows:\\n\\n$$\\\\mathcal{L}{\\\\text{DevBal}}=\\\\alpha{2}\\\\sum_{i=1}^{D}f_{i}^{\\\\prime}P_{i}^{\\\\prime},$$ $$f_{i}^{\\\\prime}=\\\\frac{1}{|\\\\mathcal{E}{i}|}\\\\sum{j\\\\in\\\\mathcal{E}{i}}f{j},$$ $$P_{i}^{\\\\prime}=\\\\sum_{j\\\\in\\\\mathcal{E}{i}}P{j},$$\\n\\n(15) $$\\\\begin{array}{l}\\\\small\\\\text{(16)}\\\\end{array}$$ = (17) .\\n\\nwhere 2 is a hyper-parameter called device-level balance factor. In practice, we set a small expert-level balance factor to mitigate the risk of routing collapse, and meanwhile set a larger device-level balance factor to promote balanced computation across the devices.\\n\\n4. Validation Experiments 4.1. Experimental Setup 4.1.1. Training Data And Tokenization\\n\\nOur training data is sampled from a large-scale multilingual corpus created by DeepSeek-AI. The corpus primarily focuses on English and Chinese but also encompasses other languages. It is derived from diverse sources, including web text, mathematical material, coding scripts, published literature, and various other textual materials. For the purpose of validation experiments, we sample a subset containing 100B tokens from the corpus to train our models. For tokenization, we utilize the HuggingFace Tokenizer2tools to train byte pair encoding (BPE) (Sennrich et al., 2016) tokenizers on a smaller subset of the training corpus. In the validation experiments, we prepare a tokenizer with a vocabulary size of 8K, and the vocabulary size will be scaled up when training larger models.\\n\\n4.1.2. Infrastructures\\n\\nWe conduct experiments based on HAI-LLM (High-Flyer, 2023), an efficient and light-weight training framework which integrates multiple parallelism strategies, including tensor parallelism (Korthikanti et al., 2023; Narayanan et al., 2021; Shoeybi et al., 2019), ZeRO data parallelism (Rajbhandari et al., 2020), PipeDream pipeline parallelism (Harlap et al., 2018), and more specifically, expert parallelism (Lepikhin et al., 2021) by combining data and tensor parallelism.\\n\\nIn order to optimize performance, we develop GPU kernels with CUDA and Triton (Tillet et al., 2019) for gating algorithms and fusing computations across linear layers in different experts.\\n\\nAll experiments are carried out on clusters equipped with NVIDIA A100 or H800 GPUs.\\n\\nEach node in the A100 cluster contains 8 GPUs connected pairwise via the NVLink bridge.\\n\\nThe H800 cluster also features 8 GPUs per node, interconnected using NVLink and NVSwitch within nodes. For both A100 and H800 clusters, InfiniBand interconnects are utilized to facilitate communication across nodes.\\n\\n4.1.3. Hyper-Parameters\\n\\nModel Settings. In the validation experiments, we set the number of Transformer layers to 9 and the hidden dimension to 1280. We employ the multi-head attention mechanism with a total of 10 attention heads, where each head has a dimension of 128. For initialization, all learnable parameters are randomly initialized with a standard deviation of 0.006. We substitute all FFNs with MoE layers, and ensure that the total number of expert parameters equals 16 times that of a standard FFN. Additionally, we keep the activated expert parameters, including shared expert parameters and activated routed expert parameters, as 2 times that of a standard FFN. Under this configuration, each MoE model has approximately 2B total parameters, with the number of activated parameters around 0.3B.\\n\\nTraining Settings. We employ the AdamW optimizer (Loshchilov and Hutter, 2019) with hyper-parameters set to 1 = 0.9, 2 = 0.95, and weight_decay = 0.1. The learning rate is scheduled using a warmup-and-step-decay strategy. Initially, the learning rate linearly increases from 0 to the maximum value during the first 2K steps. Subsequently, the learning rate is multiplied by 0.316 at 80% of the training steps, and again by 0.316 at 90% of the training steps.\\n\\nThe maximum learning rate for validation experiments is set to 1.08 × 10−3, and the gradient clipping norm is set to 1.0. The batch size is set to 2K, and with a maximum sequence length of 2K, each training batch contains 4M tokens. Correspondingly, the total number of training steps is set to 25,000 to achieve 100B training tokens. Due to the abundance of training data, we do not use dropout during training. Given the relatively small model size, all parameters, including expert parameters, are deployed on a single GPU device to avoid unbalanced computation.\\n\\nCorrespondingly, we do not drop any tokens during training and do not employ the device-level balance loss. In order to prevent routing collapse, we set an expert-level balance factor of 0.01.\\n\\nFor readability, we also present an overview table of hyper-parameters for DeepSeekMoE across different sizes in Appendix A.\\n\\n4.1.4. Evaluation Benchmarks\\n\\nWe conduct evaluations on a wide range of benchmarks covering various types of tasks. We list the benchmarks as follows.\\n\\nLanguage Modeling. For language modeling, we evaluate the models on the test set of Pile (Gao et al., 2020), and the evaluation metric is the cross-entropy loss.\\n\\nLanguage Understanding and Reasoning. For language understanding and reasoning, we consider HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-challenge and ARCeasy (Clark et al., 2018). The evaluation metric for these tasks is accuracy.\\n\\nReading Comprehension. For reading comprehension, we use RACE-high and RACE-middle Lai et al. (2017), and the evaluation metric is accuracy.\\n\\nCode Generation. For code generation, we evaluate the models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). The evaluation metric is Pass@1, which represents the pass rate for only one generation attempt.\\n\\nClosed-Book Question Answering. For closed-book question answering, we consider TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019). The evaluation metric is the Exactly Matching (EM) rate.\\n\\n4.2. Evaluations\\n\\nBaselines. Including DeepSeekMoE, we compare five models for validation experiments.\\n\\nDense denotes a standard dense Transformer language model with 0.2B total parameters. Hash Layer (Roller et al., 2021) is an MoE architecture based on top-1 hash routing, with 2.0B total parameters and 0.2B activated parameters, aligned with the dense baseline. Switch Transformer (Fedus et al., 2021) is another well-known MoE architecture based on top-1 learnable routing, with total parameters and activated parameters the same as Hash Layer. GShard (Lepikhin et al., 2021) employs a top-2 learnable routing strategy, with 2.0B total parameters and 0.3B activated parameters since one more expert is activated compared to top-1 routing methods. DeepSeekMoE has 1 shared expert and 63 routed experts, where each expert is 0.25 times the size of a standard FFN. Including DeepSeekMoE, all compared models share the same training corpus and training hyper-parameters. All compared MoE models have the same number of total parameters, and GShard has the same number of activated parameters as DeepSeekMoE.\\n\\nResults. We present the evaluation results in Table 1. For all demonstrated models, we report the final evaluation results after training on 100B tokens. From the table, we make the following observations: (1) With sparse architectures and more total parameters, Hash Layer\\n\\nMetric # Shot Dense Hash Layer Switch GShard DeepSeekMoE # Total Params N/A 0.2B 2.0B 2.0B 2.0B 2.0B # Activated Params N/A 0.2B 0.2B 0.2B 0.3B 0.3B FLOPs per 2K Tokens N/A 2.9T 2.9T 2.9T 4.3T 4.3T # Training Tokens N/A 100B 100B 100B 100B 100B Pile (Loss) N/A 2.060 1.932 1.881 1.867 1.808 HellaSwag (Acc.) 0-shot 38.8 46.2 49.1 50.5 54.8 PIQA (Acc.) 0-shot 66.8 68.4 70.5 70.6 72.3 ARC-easy (Acc.) 0-shot 41.0 45.3 45.9 43.9 49.4 ARC-challenge (Acc.) 0-shot 26.0 28.2 30.2 31.6 34.3 RACE-middle (Acc.) 5-shot 38.8 38.8 43.6 42.1 44.0 RACE-high (Acc.) 5-shot 29.0 30.0 30.9 30.4 31.7 HumanEval (Pass@1) 0-shot 0.0 1.2 2.4 3.7 4.9 MBPP (Pass@1) 3-shot 0.2 0.6 0.4 0.2 2.2 TriviaQA (EM) 5-shot 4.9 6.5 8.9 10.2 16.6 NaturalQuestions (EM) 5-shot 1.4 1.4 2.5 3.2 5.7\\n\\nTable 1 | Evaluation results for validation experiments. Bold font indicates the best. Compared with other MoE architectures, DeepSeekMoE exhibits a substantial performance advantage.\\n\\nand Switch Transformer achieve significantly stronger performance than the dense baseline with the same number of activated parameters. (2) Compared with Hash Layer and Switch Transformer, GShard has more activated parameters and achieves slightly better performance than Switch Transformer. (3) With the same number of total parameters and activated parameters, DeepSeekMoE demonstrates overwhelming advantages over GShard. These results showcase the superiority of our DeepSeekMoE architecture within the existing landscape of MoE architectures.\\n\\n4.3. Deepseekmoe Aligns Closely With The Upper Bound Of Moe Models\\n\\nWe have demonstrated that DeepSeekMoE outperforms the dense baseline and other MoE architectures. In order to provide a more precise understanding of the performance of DeepSeekMoE, we compare it with larger baselines with more total parameters or activated parameters. The comparisons enable us to estimate the required model size of GShard or dense baselines to achieve equivalent performance to DeepSeekMoE.\\n\\nComparison with GShard×1.5. Table 2 shows the comparison between DeepSeekMoE and a larger GShard model with 1.5 times the expert size, which results in 1.5 times both expert parameters and expert computation. Overall, we observe that DeepSeekMoE achieves comparable performance with GShard×1.5, underscoring the significant advantage inherent in the DeepSeekMoE architecture. In addition to the comparison with GShard×1.5, we also show the comparison with GShard×1.2 in Appendix B.\\n\\nFurthermore, we increase the number of total parameters of DeepSeekMoE to 13.3B and compare it with GShard×1.2 and GShard×1.5 with 15.9B and 19.8B total parameters, respectively.\\n\\nWe find that at a larger scale, DeepSeekMoE can even outperform GShard×1.5 distinctly. These\\n\\nMetric # Shot GShard×1.5 Dense×16 DeepSeekMoE Relative Expert Size N/A 1.5 1 0.25 # Experts N/A 0 + 16 16 + 0 1 + 63 # Activated Experts N/A 0 + 2 16 + 0 1 + 7 # Total Expert Params N/A 2.83B 1.89B 1.89B # Activated Expert Params N/A 0.35B 1.89B 0.24B FLOPs per 2K Tokens N/A 5.8T 24.6T 4.3T # Training Tokens N/A 100B 100B 100B Pile (Loss) N/A 1.808 1.806 1.808 HellaSwag (Acc.) 0-shot 54.4 55.1 54.8 PIQA (Acc.) 0-shot 71.1 71.9 72.3 ARC-easy (Acc.) 0-shot 47.3 51.9 49.4 ARC-challenge (Acc.) 0-shot 34.1 33.8 34.3 RACE-middle (Acc.) 5-shot 46.4 46.3 44.0 RACE-high (Acc.) 5-shot 32.4 33.0 31.7 HumanEval (Pass@1) 0-shot 3.0 4.3 4.9 MBPP (Pass@1) 3-shot 2.6 2.2 2.2 TriviaQA (EM) 5-shot 15.7 16.5 16.6 NaturalQuestions (EM) 5-shot 4.7 6.3 5.7\\n\\nTable 2 | Comparisons among DeepSeekMoE, larger GShard models, and larger dense models.\\n\\nIn the line of \"# Experts\", + denotes shared experts and routed experts. In the line of \"# Activated Experts\", + denotes activated shared experts and activated routed experts. DeepSeekMoE achieves comparable performance with a GShard model containing 1.5 times expert parameters and computation. In addition, DeepSeekMoE nearly approaches the performance of a dense model with 16 times FFN parameters, which sets the upper bound for MoE models in terms of the model capacity.\\n\\nResults Are Also Provided In Appendix B.\\n\\nComparison with Dense×16. Table 2 also shows the comparison between DeepSeekMoE and larger dense models. For a fair comparison, we do not use the widely used ratio (1:2) between the attention and FFN parameters. Instead, we configure 16 shared experts where each expert has the same number of parameters as a standard FFN. This architecture mimics a dense model with 16 times standard FFN parameters. From the table, we find that DeepSeekMoE nearly approaches the performance of Dense×16, which sets the strict upper bound of MoE models in terms of the model capacity. These results suggest that, at least at the scale of about 2B parameters and 100B training tokens, the performance of DeepSeekMoE aligns closely with the theoretical upper bound of MoE models. Also, we provide additional comparisons with Dense×4 in Appendix B.\\n\\n4.4. Ablation Studies\\n\\nIn order to substantiate the effectiveness of the fine-grained expert segmentation and shared expert isolation strategies, we conduct ablation studies for DeepSeekMoE and present the results in Figure 3. For a fair comparison, we ensure all models included in the comparison have the\\n\\nsame number of total parameters and activated parameters.\\n\\nShared Expert Isolation. In order to evaluate the influence of the shared expert isolation strategy, we isolate one expert as the shared one based on GShard. From Figure 3, we observe that compared with GShard, the intentional isolation of a shared expert yields improved performance across a majority of benchmarks. These results support the proposition that the shared expert isolation strategy contributes to a stronger model performance.\\n\\nFine-Grained Expert Segmentation. In order to assess the effectiveness of the fine-grained expert segmentation strategy, we conduct a more detailed comparison by further segmenting the experts into a finer grain. To be specific, we segment each expert into 2 or 4 smaller experts, resulting in a total of 32 (1 shared + 31 routed) or 64 (1 shared + 63 routed) experts. Figure 3 reveals a consistent trend that the continuous refinement of expert segmentation granularity corresponds to a continuous enhancement in overall model performance. These findings provide empirical substantiation for the effectiveness of the fine-grained expert segmentation strategy.\\n\\nRatios Between Shared and Routed Experts. In addition, we investigate the best ratio of shared experts and routed experts. Based on the finest granularity with 64 total experts and keeping the number of total experts and activated experts constant, we attempt to isolate 1, 2, and 4 experts as shared ones. We find that different ratios of the shared experts and routed experts do not significantly impact the performance, and 1, 2, and 4 shared experts achieve a Pile loss of 1.808, 1.806, and 1.811, respectively. Considering that the ratio of 1:3 yields a marginally better Pile loss, when scaling up DeepSeekMoE, we keep the ratio between shared experts and activated routed experts as 1:3.\\n\\n4.5. Analysis On Expert Specialization\\n\\nIn this section, we conduct an empirical analysis on the expert specialization of DeepSeekMoE 2B. DeepSeekMoE 2B in this section refers to the model reported in Table 1, i.e., comprising 2.0B total parameters, with 1 shared expert and 7 out of 63 routed experts being activated.\\n\\nDeepSeekMoE Exhibits Lower Redundancy Among Routed Experts. In order to assess the redundancy among routed experts, we disable varying ratios of top routed experts and evaluate the Pile loss. To be specific, for each token, we mask a certain ratio of experts with the highest routing probability, and then select top-K experts from the remaining routed experts. For fairness, we compare DeepSeekMoE with GShard×1.5 since they have the same Pile loss when no experts are disabled. As shown in Figure 4, compared with GShard×1.5, DeepSeekMoE is more sensitive to the disabling of top routed experts. This sensitivity suggests a lower level of parameter redundancy in DeepSeekMoE, since each routed expert is more irreplaceable. In contrast, GShard×1.5 exhibits greater redundancy among its expert parameters, so it can buffer the performance drop when top routed experts are disabled.\\n\\nShared Experts Are Irreplaceable by Routed Experts. In order to investigate the role of the shared expert in DeepSeekMoE, we disable it and activate one more routed expert. The evaluation on Pile shows a significant increase in the Pile loss, rising from 1.808 to 2.414, even though we maintain the same computational cost. This result highlights the crucial function of the shared expert and indicates that the shared expert captures fundamental and essential knowledge not shared with routed experts, making it irreplaceable by routed ones.\\n\\nDeepSeekMoE Acquires Knowledge More Accurately. In order to validate our claim that higher flexibility in combining activated experts contributes to a more accurate and targeted knowledge acquisition, we investigate whether DeepSeekMoE can acquire requisite knowledge with fewer activated experts. To be specific, we vary the number of activated routed experts from 3 to 7 and evaluate the resulting Pile loss. As demonstrated in Figure 5, even with only\\n\\n4 routed experts activated, DeepSeekMoE achieves a Pile loss comparable with GShard. This observation supports the proposition that DeepSeekMoE can acquire requisite knowledge more accurately and efficiently.\\n\\nEncouraged by these findings, in order to validate the expert specialization and accurate knowledge acquisition of DeepSeekMoE more rigorously, we train a new model from scratch.\\n\\nThis model comprises 1 shared expert and 63 routed experts, where only 3 routed experts are activated. The evaluation results shown in Figure 6 demonstrate that, even with the same total expert parameters and only half of the activated expert parameters, DeepSeekMoE still outperforms GShard. This highlights the ability of DeepSeekMoE to leverage expert parameters more efficiently, i.e., the proportion of effective parameters in the activated experts is much higher than that of GShard.\\n\\n5. Scaling Up To Deepseekmoe 16B\\n\\nWith the DeepSeekMoE architecture, we scale up our MoE model to a larger scale with 16B total parameters and train it on 2T tokens. Our results demonstrate that compared with LLaMA2 7B, DeepSeekMoE 16B achieves superior performance with only about 40% of computations.\\n\\n5.1. Experimental Setup 5.1.1. Training Data And Tokenization\\n\\nWe sample the training data from the same corpus as described in Section 4.1.1. Different from the validation experiments, we sample a larger amount of data with 2T tokens, aligning with the number of training tokens of LLaMA2 7B. We also use the HuggingFace Tokenizer tools to train a BPE tokenizer, but the vocabulary size is set to 100K for DeepSeekMoE 16B.\\n\\n5.1.2. Hyper-Parameters\\n\\nModel Settings. For DeepSeekMoE 16B, we set the number of Transformer layers to 28 and the hidden dimension to 2048. We employ the multi-head attention mechanism with a total of 16 attention heads, where each head has a dimension of 128. As for initialization, all learnable parameters are randomly initialized with a standard deviation of 0.006. We substitute all FFNs except for the first layer with MoE layers, since we observe that the load balance status converges especially slower for the first layer. Each MoE layer consists of 2 shared experts and 64 routed experts, where each expert is 0.25 times the size of a standard FFN. Each token will be routed to these 2 shared experts and 6 out of 64 routed experts. An even finer expert segmentation granularity is not employed due to the potential reduction in computational efficiency associated with excessively small expert sizes. At a larger scale over 16B, a finer granularity can still be employed. Under our configuration, DeepSeekMoE 16B has approximately 16.4B total parameters, with the number of activated parameters around 2.8B.\\n\\nTraining Settings. We employ the AdamW optimizer (Loshchilov and Hutter, 2019) with hyper-parameters set to 1 = 0.9, 2 = 0.95, and weight_decay = 0.1. The learning rate is also scheduled using a warmup-and-step-decay strategy. Initially, the learning rate linearly increases from 0 to the maximum value during the first 2K steps. Subsequently, the learning rate is multiplied by 0.316 at 80% of the training steps, and again by 0.316 at 90% of the training steps.\\n\\nThe maximum learning rate for DeepSeekMoE 16B is set to 4.2 × 10−4, and the gradient clipping norm is set to 1.0. The batch size is set to 4.5K, and with a maximum sequence length of 4K, each training batch contains 18M tokens. Correspondingly, the total number of training steps is set to 106,449 to achieve 2T training tokens. Due to the abundance of training data, we do not use dropout during training. We leverage pipeline parallelism to deploy different layers of a model on different devices, and for each layer, all the experts will be deployed on the same device.\\n\\nTherefore, we also do not drop any tokens during training and do not employ the device-level balance loss. In order to prevent routing collapse, we set a quite small expert-level balance factor of 0.001 because we find that under our parallelization strategy, a higher expert-level balance factor cannot increase the computation efficiency, but instead, it will compromise the model performance.\\n\\n5.1.3. Evaluation Benchmarks\\n\\nIn addition to the benchmarks used in the validation experiments, we incorporate additional benchmarks for a more comprehensive evaluation. We introduce the distinctions from the benchmarks used in validation experiments as follows.\\n\\nLanguage Modeling. For language modeling, we also evaluate the models on the test set of Pile (Gao et al., 2020). Since the tokenizer used in DeepSeekMoE 16B is different from that used in LLaMA2 7B. For a fair comparison, we use bits per byte (BPB) as the evaluation metric.\\n\\nReading Comprehension. For reading comprehension, we additionally consider DROP (Dua et al., 2019). The evaluation metric is the Exactly Matching (EM) rate.\\n\\nMath Reasoning. For math reasoning, we additionally incorporate GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), using EM as the evaluation metric.\\n\\nMulti-Subject Multiple-Choice. For multi-subject multiple-choice, we additionally evaluate the models on MMLU (Hendrycks et al., 2020). The evaluation metric is accuracy.\\n\\nDisambiguation. For disambiguation, we additionally consider WinoGrande (Sakaguchi et al., 2019) and the evaluation metric is accuracy.\\n\\nChinese Benchmarks. Since DeepSeekMoE 16B is pretrained on a bilingual corpus, we also evaluate it on four Chinese benchmarks. CLUEWSC (Xu et al., 2020) is a Chinese disambiguation benchmark. CEval (Huang et al., 2023) and CMMLU (Li et al., 2023) are two Chinese multisubject multiple-choice benchmarks with a similar form to MMLU. CHID (Zheng et al., 2019) is a Chinese idiom completion benchmark, aiming to evaluate the understanding of Chinese culture. The evaluation metrics for the aforementioned Chinese benchmarks are accuracy or EM.\\n\\nOpen LLM Leaderboard. We evaluate all of the aforementioned benchmarks based on our internal evaluation framework. In order to compare DeepSeekMoE 16B with open source models fairly and conveniently, we additionally evaluate DeepSeekMoE 16B on the Open LLM Leaderboard. The Open LLM Leaderboard is a public leaderboard supported by HuggingFace, it consists of six tasks: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), TruthfulQA (Lin et al., 2022), Winogrande (Sakaguchi et al., 2019), and GSM8K (Cobbe et al., 2021).\\n\\n5.2. Evaluations 5.2.1. Internal Comparison With Deepseek 7B\\n\\nWe first conduct an internal comparison between DeepSeekMoE 16B and DeepSeek 7B (DeepSeekAI, 2024), a dense language model with 6.9B parameters. Ensuring fairness, both models are trained on the same corpus with 2T tokens. This enables an accurate assessment of the effectiveness of our MoE architecture, independent of the influence of the training data.\\n\\nMetric # Shot DeepSeek 7B (Dense) DeepSeekMoE 16B # Total Params N/A 6.9B 16.4B # Activated Params N/A 6.9B 2.8B FLOPs per 4K Tokens N/A 183.5T 74.4T # Training Tokens N/A 2T 2T Pile (BPB) N/A 0.75 0.74 HellaSwag (Acc.) 0-shot 75.4 77.1 PIQA (Acc.) 0-shot 79.2 80.2 ARC-easy (Acc.) 0-shot 67.9 68.1 ARC-challenge (Acc.) 0-shot 48.1 49.8 RACE-middle (Acc.) 5-shot 63.2 61.9 RACE-high (Acc.) 5-shot 46.5 46.4 DROP (EM) 1-shot 34.9 32.9 GSM8K (EM) 8-shot 17.4 18.8 MATH (EM) 4-shot 3.3 4.3 HumanEval (Pass@1) 0-shot 26.2 26.8 MBPP (Pass@1) 3-shot 39.0 39.2 TriviaQA (EM) 5-shot 59.7 64.8 NaturalQuestions (EM) 5-shot 22.2 25.5 MMLU (Acc.) 5-shot 48.2 45.0 WinoGrande (Acc.) 0-shot 70.5 70.2 CLUEWSC (EM) 5-shot 73.1 72.1 CEval (Acc.) 5-shot 45.0 40.6 CMMLU (Acc.) 5-shot 47.2 42.5 CHID (Acc.) 0-shot 89.3 89.4\\n\\nTable 3 | Comparison between DeepSeek 7B and DeepSeekMoE 16B. Bold font indicates the best or near the best. With only 40.5% of computations, DeepSeekMoE 16B achieves comparable performance with DeepSeek 7B.\\n\\nThe evaluation results are presented in Table 3, yielding the following observations: (1) On the whole, with about only 40% of the computations, DeepSeekMoE 16B achieves comparable performance with DeepSeek 7B. (2) DeepSeekMoE 16B exhibits notable strengths in language modeling and knowledge-intensive tasks such as Pile, HellaSwag, TriviaQA, and NaturalQuestions. Given that in an MoE model, FFN parameters are much heavier than attention parameters, these outcomes align with the proposition that FFNs in Transformers exhibit the capability for knowledge memorization (Dai et al., 2022a). (3) Compared with the excellent performance on other tasks, DeepSeekMoE exhibits limitations in addressing multiple-choice tasks. This inadequacy stems from the limited attention parameters in DeepSeekMoE 16B (DeepSeekMoE 16B has only about 0.5B attention parameters, while DeepSeek 7B has 2.5B attention parameters).\\n\\nOur earlier investigation on DeepSeek 7B reveals a positive correlation between the attention capacity and performance on multiple-choice tasks. For example, DeepSeek 7B MQA, which is equipped with the multi-query attention mechanism (Shazeer, 2019), also struggled in MMLUlike tasks. In addition, for a more comprehensive understanding of the training process of DeepSeekMoE 16B, we also provide the benchmark curves of DeepSeekMoE 16B and DeepSeek 7B (Dense) during training in Appendix C for reference.\\n\\nCritically, due to the modest number of parameters in DeepSeekMoE 16B, it enables singledevice deployment on a GPU with 40GB of memory. With appropriate operator optimizations, it can achieve nearly 2.5 times the inference speed of a 7B dense model.\\n\\nMetric # Shot LLaMA2 7B DeepSeekMoE 16B # Total Params N/A 6.7B 16.4B # Activated Params N/A 6.7B 2.8B FLOPs per 4K Tokens N/A 187.9T 74.4T # Training Tokens N/A 2T 2T Pile (BPB) N/A 0.76 0.74 HellaSwag (Acc.) 0-shot 75.6 77.1 PIQA (Acc.) 0-shot 78.0 80.2 ARC-easy (Acc.) 0-shot 69.1 68.1 ARC-challenge (Acc.) 0-shot 49.0 49.8 RACE-middle (Acc.) 5-shot 60.7 61.9 RACE-high (Acc.) 5-shot 45.8 46.4 DROP (EM) 1-shot 34.0 32.9 GSM8K (EM) 8-shot 15.5 18.8 MATH (EM) 4-shot 2.6 4.3 HumanEval (Pass@1) 0-shot 14.6 26.8 MBPP (Pass@1) 3-shot 21.8 39.2 TriviaQA (EM) 5-shot 63.8 64.8 NaturalQuestions (EM) 5-shot 25.5 25.5 MMLU (Acc.) 5-shot 45.8 45.0 WinoGrande (Acc.) 0-shot 69.6 70.2 CLUEWSC (EM) 5-shot 64.0 72.1 CEval (Acc.) 5-shot 33.9 40.6 CMMLU (Acc.) 5-shot 32.6 42.5 CHID (Acc.) 0-shot 37.9 89.4\\n\\nTable 4 | Comparison between LLaMA2 7B and DeepSeekMoE 16B. With only 39.6% of computations, DeepSeekMoE 16B outperforms LLaMA2 7B on the majority of benchmarks.\\n\\n5.2.2. Comparison With Open Source Models\\n\\nInternal Comparison with LLaMA2 7B. In the realm of open source models, we mainly compare DeepSeekMoE 16B with LLaMA2 7B (Touvron et al., 2023b), a well-known and strong open source language model with 6.7B parameters. Both DeepSeekMoE 16B and LLaMA2 7B are pretrained on 2T tokens. Compared with LLaMA2 7B, DeepSeekMoE has 245% of total parameters but only needs 39.6% of computations. The results on our internal benchmarks are presented in Table 4, leading to the following observations. (1) Among the evaluated benchmarks, with only about 40% of computations, DeepSeekMoE 16B outperforms LLaMA2 7B on the majority of benchmarks. (2) The math reasoning and code generation capabilities of DeepSeekMoE 16B are stronger than LLaMA2 7B, attributed to the enriched presence of mathematical and coderelated text in our pretraining corpus. (3) Given the presence of Chinese texts in our pretraining corpus, DeepSeekMoE 16B exhibits a substantial performance advantage over LLaMA2 7B on Chinese benchmarks. (4) Despite being trained on fewer English texts, DeepSeekMoE 16B achieves comparable or better performance compared with LLaMA2 7B on English understanding or knowledge-intensive benchmarks, which demonstrates the exceptional capabilities of DeepSeekMoE 16B.\\n\\nEvaluation on Open LLM Leaderboard. Beyond our internal evaluations, we also evaluate DeepSeekMoE 16B on the Open LLM Leaderboard and compare it with other open source models. In addition to LLaMA2 7B, we take a broader set of open source models into consideration, including LLaMA 7B (Touvron et al., 2023a), Falcon 7B (Almazrouei et al., 2023), GPT-J 6B (Wang and Komatsuzaki, 2021), RedPajama-INCITE 7B and 3B (Together-AI, 2023), Open LLaMA 7B and 3B (Geng and Liu, 2023), OPT 2.7B (Zhang et al., 2022), Pythia 2.8B (Biderman et al., 2023), GPT-neo 2.7B (Black et al., 2021), and BLOOM 3B (Scao et al., 2022). The evaluation results, as presented in Figure 1, show that DeepSeekMoE 16B consistently outperforms models with similar activated parameters by a large margin. Moreover, it achieves comparable performance with LLaMA2 7B, which has approximately 2.5 times the activated parameters.\\n\\n6. Alignment For Deepseekmoe 16B\\n\\nPrevious research indicates that MoE models typically do not emerge significant gains from fine-tuning (Artetxe et al., 2022; Fedus et al., 2021). However, Shen et al. (2023) present findings suggesting that MoE models can indeed benefit from instruction tuning. In order to assess whether DeepSeekMoE 16B can benefit from fine-tuning, we conduct supervised fine-tuning to construct a chat model based on DeepSeekMoE 16B. The experimental results reveal that DeepSeekMoE Chat 16B also achieves comparable performance with LLaMA2 SFT 7B and DeepSeek Chat 7B.\\n\\n6.1. Experimental Setup\\n\\nTraining Data. For training the chat model, we conduct supervised fine-tuning (SFT) on our in-house curated data, comprising 1.4M training examples. This dataset spans a broad range of categories including math, code, writing, question answering, reasoning, summarization, and more. The majority of our SFT training data is in English and Chinese, rendering the chat model versatile and applicable in bilingual scenarios.\\n\\nHyper-Parameters. During supervised fine-tuning, we set the batch size to 1024 examples and conduct training over 8 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019).\\n\\nWe employ a maximum sequence length of 4K, and pack the training examples as densely as possible until reaching the sequence length limit. We do not use dropout for supervised fine-tuning, and simply set a constant learning rate of 10−5 without incorporating any learning rate scheduling strategy.\\n\\nEvaluation Benchmarks. For the evaluation of the chat models, we employ benchmarks similar to those used in Section 5.1.3, with the following adjustments: (1) We exclude Pile (Gao et al., 2020) since chat models are seldom employed for pure language modeling. (2) We exclude\\n\\nMetric # Shot LLaMA2 DeepSeek DeepSeekMoE SFT 7B Chat 7B Chat 16B # Total Params N/A 6.7B 6.9B 16.4B # Activated Params N/A 6.7B 6.9B 2.8B FLOPs per 4K Tokens N/A 187.9T 183.5T 74.4T HellaSwag (Acc.) 0-shot 67.9 71.0 72.2 PIQA (Acc.) 0-shot 76.9 78.4 79.7 ARC-easy (Acc.) 0-shot 69.7 70.2 69.9 ARC-challenge (Acc.) 0-shot 50.8 50.2 50.0 BBH (EM) 3-shot 39.3 43.1 42.2 RACE-middle (Acc.) 5-shot 63.9 66.1 64.8 RACE-high (Acc.) 5-shot 49.6 50.8 50.6 DROP (EM) 1-shot 40.0 41.7 33.8 GSM8K (EM) 0-shot 63.4 62.6 62.2 MATH (EM) 4-shot 13.5 14.7 15.2 HumanEval (Pass@1) 0-shot 35.4 45.1 45.7 MBPP (Pass@1) 3-shot 27.8 39.0 46.2 TriviaQA (EM) 5-shot 60.1 59.5 63.3 NaturalQuestions (EM) 0-shot 35.2 32.7 35.1 MMLU (Acc.) 0-shot 50.0 49.7 47.2 WinoGrande (Acc.) 0-shot 65.1 68.4 69.0 CLUEWSC (EM) 5-shot 48.4 66.2 68.2 CEval (Acc.) 0-shot 35.1 44.7 40.0 CMMLU (Acc.) 0-shot 36.9 51.2 49.3\\n\\nCHID (Zheng et al., 2019) due to the observed instability of results, hindering the derivation of solid conclusions. (3) We additionally include BBH (Suzgun et al., 2022) to provide a more comprehensive assessment of the reasoning ability of the chat models.\\n\\nTable 5 | Comparison among LLaMA2 SFT 7B, DeepSeek Chat 7B and DeepSeekMoE Chat 16B, with all of these three models fine-tuned on the same SFT data. Compared with both 7B dense models, DeepSeekMoE Chat 16B still achieves comparable or better performance on the majority of benchmarks with only 40% of computations.\\n\\n6.2. Evaluations\\n\\nBaselines. In order to validate the potential of DeepSeekMoE 16B after alignment, we conduct supervised fine-tuning for LLaMA2 7B, DeepSeek 7B, and DeepSeekMoE 16B, where we utilize totally the same fine-tuning data to ensure fairness. Correspondingly, we construct three chat models, including LLaMA2 SFT 7B3, DeepSeek Chat 7B, and DeepSeekMoE Chat 16B.\\n\\nSubsequently, we compare DeepSeekMoE Chat 16B with the other two dense chat models (with about 2.5 times the FLOPs) across a wide range of downstream tasks.\\n\\nResults. The evaluation results are presented in Table 5. Our key observations include: (1) DeepSeekMoE Chat 16B, while consuming nearly 40% of computations, achieves comparable performance with 7B dense models across language understanding and reasoning (PIQA, ARC, BBH), machine reading comprehension (RACE), mathematical (GSM8K, MATH), and knowledge-intensive tasks (TriviaQA, NaturalQuestions). (2) On code generation tasks, DeepSeekMoE Chat 16B significantly outperforms LLaMA2 SFT 7B, demonstrating notable improvements on HumanEval and MBPP. In addition, it also surpasses DeepSeek Chat 7B. (3) On multiple-choice question answering benchmarks including MMLU, CEval, and CMMLU, DeepSeekMoE Chat 16B still falls behind DeepSeek Chat 7B, consistent with the observations for the base model (Section 5.2.1). However, it is worth noting that, after supervised finetuning, the performance gap between DeepSeekMoE 16B and DeepSeek 7B is narrowed. (4) Benefiting from the pretraining on a bilingual corpus, DeepSeekMoE Chat 16B notably outperforms LLaMA2 SFT 7B on all Chinese benchmarks. These results demonstrate the balanced capabilities of DeepSeekMoE 16B in both Chinese and English, enhancing its versatility and applicability in diverse scenarios. In conclusion, the evaluation for the chat models highlights the potential of DeepSeekMoE 16B in benefiting from alignment, and validates its consistent advantages in achieving comparable performance with dense models while using only about 40% of computations.\\n\\n7. Deepseekmoe 145B Ongoing\\n\\nEncouraged by the outstanding performance of DeepSeekMoE 16B, we further undertake a preliminary endeavor to scale up DeepSeekMoE to 145B. In this initial study, DeepSeekMoE 145B is trained on 245B tokens, but it has demonstrated consistent advantages over the GShard architecture and shown promise to match or exceed the performance of DeepSeek 67B (Dense). Furthermore, upon the completion of the final version and full training of DeepSeekMoE 145B, we also plan to make it publicly available.\\n\\n7.1. Experimental Setup\\n\\nTraining Data and Tokenization. For DeepSeekMoE 145B, we employ exactly the same training corpus and tokenizer as DeepSeekMoE 16B, with the only difference being that DeepSeekMoE 145B is trained on 245B tokens for an initial study.\\n\\nModel Settings. For DeepSeekMoE 145B, we set the number of Transformer layers to 62 and the hidden dimension to 4096. We employ the multi-head attention mechanism with a total of 32 attention heads, where each head has a dimension of 128. As for initialization, all learnable parameters are randomly initialized with a standard deviation of 0.006. As in DeepSeekMoE 16B, we also substitute all FFNs except for the first layer with MoE layers. Each MoE layer consists of 4 shared experts and 128 routed experts, where each expert is 0.125 times the size of a standard FFN. Each token will be routed to these 4 shared experts and 12 out of 128 routed experts. Under this configuration, DeepSeekMoE 145 has approximately 144.6B total parameters, with the number of activated parameters around 22.2B.\\n\\nTraining Settings. We employ the AdamW optimizer (Loshchilov and Hutter, 2019) with hyper-parameters set to 1 = 0.9, 2 = 0.95, and weight_decay = 0.1. For the preliminary study of DeepSeekMoE 145B, we employ a warmup-and-constant learning rate scheduler. Initially, the learning rate linearly increases from 0 to the maximum value during the first 2K steps.\\n\\nSubsequently, the learning rate keeps constant during the remaining training process. The maximum learning rate for DeepSeekMoE 145B is set to 3.0 × 10−4, and the gradient clipping norm is set to 1.0. The batch size is set to 4.5K, and with a maximum sequence length of 4K, each training batch contains 18M tokens. We train DeepSeekMoE 145B for 13,000 steps, achieving 245B training tokens. Also, we do not use dropout during training. We leverage pipeline parallelism to deploy different layers of a model on different devices, and for each layer, all the routed experts will be uniformly deployed on 4 devices (i.e., expert parallelism combined with data parallelism). Since we employ expert parallelism for DeepSeekMoE 145B, the device-level load balance should be considered to reduce the computational bottleneck. In response, we set the device-level balance factor to 0.05 to encourage balanced computation across devices. Also, we still set a small expert-level balance factor of 0.003 to prevent routing collapse.\\n\\nEvaluation Benchmarks. We evaluate DeepSeekMoE 145B on exactly the same internal benchmarks as used for DeepSeekMoE 16B (see Section 5.1.3).\\n\\n7.2. Evaluations\\n\\nBaselines. Apart from DeepSeekMoE 145B, we consider three additional models for comparison. DeepSeek 67B (Dense) is a dense model with 67.4B total parameters (refer to DeepSeek-AI (2024) for the model and training details). GShard 137B shares the same hidden dimension and number of layers as DeepSeekMoE 145B, but follows the GShard architecture. Note that DeepSeekMoE 145B aligns the intermediate hidden dimension in each expert to a multiple of 64 for computation efficiency, so its model size is 6% larger than GShard 137B. DeepSeekMoE 142B (Half Activated) has a similar architecture to DeepSeekMoE 145B, but it contains only 2 shared experts, and only 6 out of 128 routed experts are activated. It is noteworthy that all compared models, including DeepSeekMoE 145B, share the same training corpus. In addition, all MoE models in the comparison are trained from scratch and share the same training hyper-parameters.\\n\\nResults. From the evaluation results presented in Table 6, we have the following observations: (1) Despite having comparable total parameters and computations, DeepSeekMoE 145B significantly outperforms GShard 137B, highlighting the advantages of the DeepSeekMoE architecture again. (2) On the whole, with only 28.5% of computations, DeepSeekMoE 145B achieves comparable performance with DeepSeek 67B (Dense). Consistent with the findings from DeepSeekMoE 16B, DeepSeekMoE 145B exhibits remarkable strengths in language modeling and knowledge-intensive tasks, but with limitations in multiple-choice tasks. (3) At a larger scale, the performance of DeepSeekMoE 142B (Half Activated) does not lag behind too much from DeepSeekMoE 145B. In addition, despite having only a half of activated expert parameters, DeepSeekMoE 142B (Half Activated) still match the performance of DeepSeek 67B (Dense), with only 18.2% of computations. It also outperforms GShard 137B, which aligns with the conclusion from Section 4.5.\\n\\n8. Related Work\\n\\nThe Mixture of Experts (MoE) technique is first proposed by Jacobs et al. (1991); Jordan and Jacobs (1994) to deal with different samples with independent expert modules. Shazeer et al. (2017) introduce MoE into language model training and build a large-scale LSTM-based (Hochreiter and Schmidhuber, 1997) MoE models. As Transformer become the most popular architecture\\n\\nMetric # Shot DeepSeek GShard DeepSeekMoE DeepSeekMoE 142B 67B (Dense) 137B 145B (Half Activated) # Total Params N/A 67.4B 136.5B 144.6B 142.3B # Activated Params N/A 67.4B 21.6B 22.2B 12.2B Relative Expert Size N/A N/A 1 0.125 0.125 # Experts N/A N/A 0 + 16 4 + 128 2 + 128 # Activated Experts N/A N/A 0 + 2 4 + 12 2 + 6 FLOPs per 4K Tokens N/A 2057.5T 572.7T 585.6T 374.6T # Training Tokens N/A 245B 245B 245B 245B Pile (Loss.) N/A 1.905 1.961 1.876 1.888 HellaSwag (Acc.) 0-shot 74.8 72.0 75.8 74.9 PIQA (Acc.) 0-shot 79.8 77.6 80.7 80.2 ARC-easy (Acc.) 0-shot 69.0 64.0 69.7 67.9 ARC-challenge (Acc.) 0-shot 50.4 45.8 48.8 49.0 RACE-middle (Acc.) 5-shot 63.2 59.2 62.1 59.5 RACE-high (Acc.) 5-shot 46.9 43.5 45.5 42.6 DROP (EM) 1-shot 27.5 21.6 27.8 28.9 GSM8K (EM) 8-shot 11.8 6.4 12.2 13.8 MATH (EM) 4-shot 2.1 1.6 3.1 2.8 HumanEval (Pass@1) 0-shot 23.8 17.7 19.5 23.2 MBPP (Pass@1) 3-shot 33.6 27.6 33.2 32.0 TriviaQA (EM) 5-shot 57.2 52.5 61.1 59.8 NaturalQuestions (EM) 5-shot 22.6 19.0 25.0 23.5 MMLU (Acc.) 5-shot 45.1 26.3 39.4 37.5 WinoGrande (Acc.) 0-shot 70.7 67.6 71.9 70.8 CLUEWSC (EM) 5-shot 69.1 65.7 71.9 72.6 CEval (Acc.) 5-shot 40.3 26.2 37.1 32.8 CMMLU (Acc.) 5-shot 40.6 25.4 35.9 31.9 CHID (Acc.) 0-shot 88.5 86.9 90.3 88.3\\n\\nTable 6 | Comparison among DeepSeek 67B (Dense) and MoE models at the scale of about 140B total parameters. In the lines of \"# Experts\" and \"# Activated Experts\", + denotes shared experts and routed experts, respectively. Bold font indicates the best or near the best performance excluding the last column. DeepSeekMoE 145B, and even DeepSeekMoE 142B (Half Activated) that has only a half of activated expert parameters, outperform GShard 137B by a large margin. Moreover, with 28.5% of computations, DeepSeekMoE 145B achieves comparable performance with DeepSeek 67B.\\n\\nfor NLP, many attempts extend FFNs in a Transformer as MoE layers to build MoE language models. GShard (Lepikhin et al., 2021) and Switch Transformer (Fedus et al., 2021) are pioneers which employ learnable top-2 or top-1 routing strategies to scale the MoE language models to an extremely large scale. Hash Layer (Roller et al., 2021) and StableMoE (Dai et al., 2022b) use fixed routing strategies for more stable routing and training. Zhou et al. (2022) propose an expert-choice routing strategy, where each token can be assigned to different numbers of experts.\\n\\nZoph (2022) focus on the issues of training instability and fine-tuning difficulty in MoE models, and propose ST-MoE to overcome these challenges. In addition to research on MoE architectures and training strategies, recent years have also witnessed the emergence of numerous large-scale language or multimodal models (Du et al., 2022; Lin et al., 2021; Ren et al., 2023; Xue et al., 2023) based on existing MoE architectures. By and large, most of the previous MoE models are based on conventional top-1 or top-2 routing strategies, leaving large room for improving expert specialization. In response, our DeepSeekMoE architecture aims to improve the expert specialization to the utmost extent.\\n\\n9. Conclusion\\n\\nIn this paper, we introduce the DeepSeekMoE architecture for MoE language models, with the objective of achieving ultimate expert specialization. Through fine-grained expert segmentation and shared expert isolation, DeepSeekMoE achieves significantly higher expert specialization and performance compared with prevailing MoE architectures. Starting with a modest scale of 2B parameters, we validate the advantages of DeepSeekMoE, demonstrating its capability to approach the upper bound performance for MoE models. Furthermore, we provide empirical evidence to show that DeepSeekMoE has a higher level of expert specialization than GShard.\\n\\nScaling up to a larger scale of 16B total parameters, we train DeepSeekMoE 16B on 2T tokens and demonstrate its outstanding performance comparable with DeepSeek 7B and LLaMA2 7B, with only about 40% of computations. Additionally, supervised fine-tuning is conducted for alignment to construct an MoE chat model based on DeepSeekMoE 16B, further showing its adaptability and versatility. Further, we perform a preliminary exploration to scale DeepSeekMoE to 145B parameters. We find that DeepSeekMoE 145B still keeps substantial advantages over the GShard architecture, and demonstrates comparable performance with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.\\n\\nFor research purposes, we release the model checkpoint of DeepSeekMoE 16B to the public, which can be deployed on a single GPU with 40GB of memory. We aspire for this work to provide valuable insights for both academia and industry, and contribute to the accelerated advancement of large-scale language models.\\n\\nReferences\\n\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo. Falcon-40B: an open large language model with state-of-the-art performance, 2023.\\n\\nM. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru, G. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou, P. S. Koura, B. O\\'Horo, J. Wang, L. Zettlemoyer, M. T. Diab, Z. Kozareva, and V. Stoyanov.\\n\\nEfficient large scale language modeling with mixtures of experts. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11699–11732. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022 .EMNLP-MAIN.804. URL https://doi.org/10.18653/v1/2022.emnlp-main.804.\\n\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\\'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A suite for analyzing large language models across training and scaling. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2397–2430. PMLR, 2023. URL https: //proceedings.mlr.press/v202/biderman23a.html.\\n\\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.\\n\\nS. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, Mar. 2021. URL https://doi.org/10.5281/ zenodo.5297715. If you use this misc, please cite it using these metadata.\\n\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8 ac142f64a-Abstract.html.\\n\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\n\\nURL https://arxiv.org/abs/2107.03374.\\n\\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.\\n\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nD. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. Knowledge neurons in pretrained transformers. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8493–8502. Association for Computational Linguistics, 2022a. doi: 10.18653/V1/2022.ACL-LONG.581. URL https://doi.org/10.1 8653/v1/2022.acl-long.581.\\n\\nD. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei. Stablemoe: Stable routing strategy for mixture of experts. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 7085–7095.\\n\\nAssociation for Computational Linguistics, 2022b. doi: 10.18653/V1/2022.ACL-LONG.489.\\n\\nURL https://doi.org/10.18653/v1/2022.acl-long.489.\\n\\nDeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.\\n\\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 5547–5569. PMLR, 2022.\\n\\nURL https://proceedings.mlr.press/v162/du22c.html.\\n\\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368– 2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/ abs/2101.03961.\\n\\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\\n\\nX. Geng and H. Liu. Openllama: An open reproduction of llama, May 2023. URL https: //github.com/openlm-research/open_llama.\\n\\nA. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, and P. B.\\n\\nGibbons. Pipedream: Fast and efficient pipeline parallel DNN training. CoRR, abs/1806.03377, 2018. URL http://arxiv.org/abs/1806.03377. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\n\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\\n\\nMeasuring mathematical problem solving with the math dataset, 2021.\\n\\nHigh-Flyer. Hai-llm: An efficient and lightweight tool for training large models, 2023. URL https://www.high-flyer.cn/en/blog/hai-llm.\\n\\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computing, 9(8):1735–1780, 1997. URL https://doi.org/10.1162/neco.1997.9.8.1735.\\n\\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre.\\n\\nTraining compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550 /arXiv.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556.\\n\\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\\n\\nR. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.\\n\\nNeural Computing, 3(1):79–87, 1991. URL https://doi.org/10.1162/neco.1991.3.1.\\n\\n79.\\n\\nM. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computing, 6(2):181–214, 1994. URL https://doi.org/10.1162/neco.1994.6.2.181.\\n\\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017.\\n\\nV. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro.\\n\\nReducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023.\\n\\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research.\\n\\nTransactions of the Association of Computational Linguistics, 2019.\\n\\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785–794. Association for Computational Linguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1 7-1082.\\n\\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\\n\\nGshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021.\\n\\nURL https://openreview.net/forum?id=qrwe7XHTmYb.\\n\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.\\n\\nJ. Lin, R. Men, A. Yang, C. Zhou, M. Ding, Y. Zhang, P. Wang, A. Wang, L. Jiang, X. Jia, J. Zhang, J. Zhang, X. Zou, Z. Li, X. Deng, J. Liu, J. Xue, H. Zhou, J. Ma, J. Yu, Y. Li, W. Lin, J. Zhou, J. Tang, and H. Yang. M6: A chinese multimodal pretrainer. CoRR, abs/2103.00823, 2021.\\n\\nURL https://arxiv.org/abs/2103.00823.\\n\\nS. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3214–3252. Association for Computational Linguistics, 2022.\\n\\ndoi: 10.18653/V1/2022.ACL-LONG.229. URL https://doi.org/10.18653/v1/2022.a cl-long.229.\\n\\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\n\\nOpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\\n\\nD. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–15, 2021.\\n\\nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774.\\n\\nURL https://doi.org/10.48550/arXiv.2303.08774.\\n\\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training trillion parameter models. In C. Cuicchi, I. Qualters, and W. T. Kramer, editors, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20.\\n\\nIEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https://doi.org/10.1109/SC 41405.2020.00024.\\n\\nS. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and Y. He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 18332–18346. PMLR, 2022. URL https://proceedings.mlr.press/v162/rajbh andari22a.html.\\n\\nX. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov, A. Bout, I. Piontkovskaya, J. Wei, X. Jiang, T. Su, Q. Liu, and J. Yao. Pangu-Σ: Towards trillion parameter language model with sparse heterogeneous computing. CoRR, abs/2303.10845, 2023. URL https://doi.org/10.48550/arXiv.2303.10845.\\n\\nS. Roller, S. Sukhbaatar, A. Szlam, and J. Weston. Hash layers for large sparse models. CoRR, abs/2106.04426, 2021. URL https://arxiv.org/abs/2106.04426.\\n\\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\\n\\nT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. doi: 10.48550/ARXIV.2211.05100. URL https: //doi.org/10.48550/arXiv.2211.05100.\\n\\nR. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/P16-1162. URL https: //doi.org/10.18653/v1/p16-1162.\\n\\nN. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL http://arxiv.org/abs/1911.02150.\\n\\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https: //openreview.net/forum?id=B1ckMDqlg.\\n\\nS. Shen, L. Hou, Y. Zhou, N. Du, S. Longpre, J. Wei, H. W. Chung, B. Zoph, W. Fedus, X. Chen, T. Vu, Y. Wu, W. Chen, A. Webson, Y. Li, V. Zhao, H. Yu, K. Keutzer, T. Darrell, and D. Zhou.\\n\\nFlan-moe: Scaling instruction-finetuned language models with sparse mixture of experts.\\n\\nCoRR, abs/2305.14705, 2023. doi: 10.48550/ARXIV.2305.14705. URL https://doi.org/10 .48550/arXiv.2305.14705.\\n\\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\\n\\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n\\nP. Tillet, H. T. Kung, and D. Cox. Triton: An intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, MAPL 2019, page 10–19, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367196. doi: 10.1145/331550 8.3329973. URL https://doi.org/10.1145/3315508.3329973.\\n\\nTogether-AI. Redpajama-data: An open source recipe to reproduce llama training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data.\\n\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/arXiv.230 2.13971. URL https://doi.org/10.48550/arXiv.2302.13971.\\n\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\n\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307. 09288.\\n\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, pages 5998–6008, 2017.\\n\\nURL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd 053c1c4a845aa-Abstract.html. B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\\n\\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\\n\\nL. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chinese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762–4772. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL https://doi.org/10.18653/v1/2020.coling-main.419.\\n\\nF. Xue, Z. Zheng, Y. Fu, J. Ni, Z. Zheng, W. Zhou, and Y. You. Openmoe: Open mixture-of-experts language models. https://github.com/XueFuzhao/OpenMoE, 2023.\\n\\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1 9-1472.\\n\\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\\n\\nC. Zheng, M. Huang, and A. Sun. Chid: A large-scale chinese idiom dataset for cloze test. In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 778–787. Association for Computational Linguistics, 2019.\\n\\ndoi: 10.18653/V1/P19-1075. URL https://doi.org/10.18653/v1/p19-1075. Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Z. Chen, Q. V. Le, and J. Laudon.\\n\\nMixture-of-experts with expert choice routing. In NeurIPS, 2022. URL http://papers.nip s.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abs tract-Conference.html.\\n\\nB. Zoph. Designing effective sparse expert models. In IEEE International Parallel and Distributed Processing Symposium, IPDPS Workshops 2022, Lyon, France, May 30 - June 3, 2022, page 1044. IEEE, 2022. URL https://doi.org/10.1109/IPDPSW55747.2022.0 0171.\\n\\nAppendices A. Overview Of Hyper-Parameters\\n\\nWe present the overview of hyper-parameters for DeepSeekMoE across various sizes in Table 7.\\n\\n# Params # Layers Hidden # Attn # Shared # Routed Relative Sequence Batch Size Learning Size Heads Experts Experts Expert Size Length (Sequence) Rate 2.0B 9 1280 10 1 63 (7 activated) 0.25 2048 2048 1.08e-3 16.4B 28 2048 16 2 64 (6 activated) 0.25 4096 4608 4.2e-4 144.6B 62 4096 32 4 128 (12 activated) 0.125 4096 4608 3.0e-4\\n\\nTable 7 | Overview of hyper-parameters for DeepSeekMoE across various sizes. The relative expert size is in comparison to a standard FFN.\\n\\nB. Comparing Deepseekmoe With Larger Models\\n\\nComparisons among DeepSeekMoE, GShard×1.2, and GShard×1.5 are shown in Table 8. Comparisons among DeepSeekMoE, Dense×4, and Dense×16 are shown in Table 9.\\n\\nMetric # Shot GShard×1.2 GShard×1.5 DeepSeekMoE Relative Expert Size N/A 1.2 1.5 0.25 # Experts N/A 0 + 16 0 + 16 1 + 63 # Activated Experts N/A 0 + 2 0 + 2 1 + 7 # Total Expert Params N/A 2.3B 2.8B 1.9B # Activated Expert Params N/A 0.28B 0.35B 0.24B # Training Tokens N/A 100B 100B 100B Pile (Loss) N/A 1.824 1.808 1.808 HellaSwag (Acc.) 0-shot 53.7 54.4 54.8 PIQA (Acc.) 0-shot 71.8 71.1 72.3 ARC-easy (Acc.) 0-shot 46.8 47.3 49.4 ARC-challenge (Acc.) 0-shot 31.7 34.1 34.3 RACE-middle (Acc.) 5-shot 43.7 46.4 44.0 RACE-high (Acc.) 5-shot 31.9 32.4 31.7 HumanEval (Pass@1) 0-shot 3.7 3.0 4.9 MBPP (Pass@1) 3-shot 2.4 2.6 2.2 TriviaQA (EM) 5-shot 15.2 15.7 16.6 NaturalQuestions (EM) 5-shot 4.5 4.7 5.7\\n\\nTable 8 | Comparison between DeepSeekMoE and larger GShard models.\\n\\nAt a larger scale of 13B total parameters, we also compare DeepSeekMoE with GShard×1.2 and GShard×1.5, and show results in Table 10. At a larger scale, DeepSeekMoE even outperforms GShard×1.5 distinctly.\\n\\nMetric # Shot Dense×4 Dense×16 DeepSeekMoE Relative Expert Size N/A 1 1 0.25 # Experts N/A 4 + 0 16 + 0 1 + 63 # Activated Experts N/A 4 + 0 16 + 0 1 + 7 # Total Expert Params N/A 0.47B 1.89B 1.89B # Activated Expert Params N/A 0.47B 1.89B 0.24B # Training Tokens N/A 100B 100B 100B Pile (Loss) N/A 1.908 1.806 1.808 HellaSwag (Acc.) 0-shot 47.6 55.1 54.8 PIQA (Acc.) 0-shot 70.0 71.9 72.3 ARC-easy (Acc.) 0-shot 43.9 51.9 49.4 ARC-challenge (Acc.) 0-shot 30.5 33.8 34.3 RACE-middle (Acc.) 5-shot 42.4 46.3 44.0 RACE-high (Acc.) 5-shot 30.7 33.0 31.7 HumanEval (Pass@1) 0-shot 1.8 4.3 4.9 MBPP (Pass@1) 3-shot 0.2 2.2 2.2 TriviaQA (EM) 5-shot 9.9 16.5 16.6 NaturalQuestions (EM) 5-shot 3.0 6.3 5.7\\n\\nTable 9 | Comparison between DeepSeekMoE and larger dense baselines.\\n\\nMetric # Shot GShard×1.2 GShard×1.5 DeepSeekMoE Relative Expert Size N/A 1.2 1.5 0.25 # Experts N/A 0 + 16 0 + 16 1 + 63 # Activated Experts N/A 0 + 2 0 + 2 1 + 7 # Total Expert Params N/A 15.9B 19.8B 13.3B # Activated Expert Params N/A 2.37B 2.82B 2.05B # Training Tokens N/A 100B 100B 100B HellaSwag (Acc.) 0-shot 66.6 67.7 69.1 PIQA (Acc.) 0-shot 75.6 76.0 75.7 ARC-easy (Acc.) 0-shot 56.8 56.8 58.8 ARC-challenge (Acc.) 0-shot 39.9 37.6 38.5 RACE-middle (Acc.) 5-shot 51.6 50.6 52.4 RACE-high (Acc.) 5-shot 37.4 36.3 38.5 HumanEval (Pass@1) 0-shot 6.1 6.1 9.8 MBPP (Pass@1) 3-shot 7.0 11.6 10.6 TriviaQA (EM) 5-shot 36.5 36.7 38.2 NaturalQuestions (EM) 5-shot 12.6 12.1 13.7\\n\\nTable 10 | Comparison between DeepSeekMoE and larger GShard models at a larger scale.\\n\\nC. Training Benchmark Curves Of Deepseekmoe 16B\\n\\nWe present the benchmark curves during training of DeepSeekMoE 16B and DeepSeek 7B (Dense) in Figure 7 for reference.'),\n",
       " Document(metadata={'source': 'Sample_Docs_Markdown\\\\README.md'}, page_content='license: apache-2.0'),\n",
       " Document(metadata={'source': 'Sample_Docs_Markdown\\\\switch_transformers.md'}, page_content='Switch Transformers: Scaling To Trillion Parameter Models With Simple And Efficient Sparsity\\n\\nWilliam Fedus∗ liamfedus@google.com Barret Zoph∗ barretzoph@google.com Noam Shazeer noam@google.com Google, Mountain View, CA 94043, USA Editor: Alexander Clark\\n\\nAbstract\\n\\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model—with an outrageous number of parameters—but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques mitigate the instabilities, and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large (Raffel et al., 2019) to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\", and achieve a 4x speedup over the T5-XXL model.12 Keywords: mixture-of-experts, natural language processing, sparsity, large-scale machine learning, distributed computing arXiv:2101.03961v3 [cs.LG] 16 Jun 2022 Contents\\n\\n1 Introduction 3 2 Switch Transformer 4 2.1 Simplifying Sparse Routing 5 2.2 Efficient Sparse Routing 6 2.3 Putting It All Together: The Switch Transformer 8 2.4 Improved Training and Fine-Tuning Techniques 8 3 Scaling Properties 11 3.1 Scaling Results on a Step-Basis 12 3.2 Scaling Results on a Time-Basis 13 3.3 Scaling Versus a Larger Dense Model 13 4 Downstream Results 14 4.1 Fine-Tuning 14 4.2 Distillation 16 4.3 Multilingual Learning 17 5 Designing Models with Data, Model, and Expert-Parallelism 18 5.1 Data Parallelism 20 5.2 Model Parallelism 20 5.3 Model and Data Parallelism 21 5.4 Expert and Data Parallelism 22 5.5 Expert, Model and Data Parallelism 22 5.6 Towards Trillion Parameter Models 22 6 Related Work 24 7 Discussion 25 8 Future Work 26 9 Conclusion 27 A Switch for Attention 27 B Preventing Token Dropping with No-Token-Left-Behind 29\\n\\nC Encouraging Exploration Across Experts 29 D Switch Transformers in Lower Compute Regimes 29 E Relation of Upstream to Downstream Model Performance 32 F Pseudo Code for Switch Transformers 33\\n\\n1. Introduction\\n\\nLarge scale training has been an effective path towards flexible and powerful neural language models (Radford et al., 2018; Kaplan et al., 2020; Brown et al., 2020). Simple architectures— backed by a generous computational budget, data set size and parameter count—surpass more complicated algorithms (Sutton, 2019). An approach followed in Radford et al. (2018); Raffel et al. (2019); Brown et al. (2020) expands the model size of a densely-activated Transformer (Vaswani et al., 2017). While effective, it is also extremely computationally intensive (Strubell et al., 2019). Inspired by the success of model scale, but seeking greater computational efficiency, we instead propose a sparsely-activated expert model: the Switch Transformer. In our case the sparsity comes from activating a subset of the neural network weights for each incoming example.\\n\\nFigure 1: Scaling and sample efficiency of Switch Transformers. Left Plot: Scaling properties for increasingly sparse (more experts) Switch Transformers. Right Plot: Negative log perplexity comparing Switch Transformers to T5 (Raffel et al., 2019) models using the same compute budget. Sparse training is an active area of research and engineering (Gray et al., 2017; Gale et al., 2020), but as of today, machine learning libraries and hardware accelerators still cater to dense matrix multiplications. To have an efficient sparse algorithm, we start with the Mixture-of-Expert (MoE) paradigm (Jacobs et al., 1991; Jordan and Jacobs, 1994; Shazeer et al., 2017), and simplify it to yield training stability and computational benefits. MoE models have had notable successes in machine translation (Shazeer et al., 2017, 2018; Lepikhin et al., 2020), however, widespread adoption is hindered by complexity, communication costs, and training instabilities.\\n\\nWe address these issues, and then go beyond translation, to find that these class of algorithms are broadly valuable in natural language. We measure superior scaling on a diverse set of natural language tasks and across three regimes in NLP: pre-training, finetuning and multi-task training. While this work focuses on scale, we also show that the Switch Transformer architecture not only excels in the domain of supercomputers, but is beneficial even with only a few computational cores. Further, our large sparse models can be distilled (Hinton et al., 2015) into small dense versions while preserving 30% of the sparse model quality gain. Our contributions are the following: - The Switch Transformer architecture, which simplifies and improves over Mixture of Experts.\\n\\nScaling properties and a benchmark against the strongly tuned T5 model (Raffel et al., 2019) where we measure 7x+ pre-training speedups while still using the same FLOPS per token. We further show the improvements hold even with limited computational resources, using as few as two experts.\\n\\nSuccessful distillation of sparse pre-trained and specialized fine-tuned models into small dense models. We reduce the model size by up to 99% while preserving 30% of the quality gains of the large sparse teacher.\\n\\nImproved pre-training and fine-tuning techniques: (1) selective precision training that enables training with lower bfloat16 precision (2) an initialization scheme that allows for scaling to a larger number of experts and (3) increased expert regularization that improves sparse model fine-tuning and multi-task training.\\n\\nA measurement of the pre-training benefits on multilingual data where we find a universal improvement across all 101 languages and with 91% of languages benefiting from 4x+ speedups over the mT5 baseline (Xue et al., 2020).\\n\\nAn increase in the scale of neural language models achieved by efficiently combining data, model, and expert-parallelism to create models with up to a trillion parameters. These models improve the pre-training speed of a strongly tuned T5-XXL baseline by 4x.\\n\\n2. Switch Transformer\\n\\nThe guiding design principle for Switch Transformers is to maximize the parameter count of a Transformer model (Vaswani et al., 2017) in a simple and computationally efficient way.\\n\\nThe benefit of scale was exhaustively studied in Kaplan et al. (2020) which uncovered powerlaw scaling with model size, data set size and computational budget. Importantly, this work advocates training large models on relatively small amounts of data as the computationally optimal approach.\\n\\nHeeding these results, we investigate a fourth axis: increase the parameter count while keeping the floating point operations (FLOPs) per example constant. Our hypothesis is that the parameter count, independent of total computation performed, is a separately important axis on which to scale. We achieve this by designing a sparsely activated model that efficiently uses hardware designed for dense matrix multiplications such as GPUs and TPUs. Our work here focuses on TPU architectures, but these class of models may be similarly trained on GPU clusters. In our distributed training setup, our sparsely activated layers split unique weights on different devices. Therefore, the weights of the model increase with the number of devices, all while maintaining a manageable memory and computational footprint on each device.\\n\\nFigure 2: Illustration of a Switch Transformer encoder block. We replace the dense feed forward network (FFN) layer present in the Transformer with a sparse Switch FFN layer (light blue). The layer operates independently on the tokens in the sequence. We diagram two tokens (x1 = \"More\" and x2 = \"Parameters\" below) being routed (solid lines) across four FFN experts, where the router independently routes each token. The switch FFN layer returns the output of the selected FFN multiplied by the router gate value (dotted-line).\\n\\n2.1 Simplifying Sparse Routing\\n\\nMixture of Expert Routing. Shazeer et al. (2017) proposed a natural language Mixtureof-Experts (MoE) layer which takes as an input a token representation x and then routes this to the best determined top-k experts, selected from a set {Ei(x)} N i=1 of N experts.\\n\\nThe router variable Wr produces logits h(x) = Wr · x which are normalized via a softmax distribution over the available N experts at that layer. The gate-value for expert i is given by,\\n\\n$$p_{i}(x)=\\\\frac{e^{h(x){i}}}{\\\\sum{j}^{N}e^{h(x)_{j}}}.\\\\tag{1}$$ $$(1)$$\\n\\n$$\\\\left(2\\\\right)$$\\n\\nThe top-k gate values are selected for routing the token x. If T is the set of selected top-k indices then the output computation of the layer is the linearly weighted combination of each expert\\'s computation on the token by the gate value,\\n\\n$$y=\\\\sum_{i\\\\in{\\\\mathcal{T}}}p_{i}(x)E_{i}(x).$$ pi(x)Ei(x). (2) Switch Routing: Rethinking Mixture-of-Experts. Shazeer et al. (2017) conjectured that routing to k > 1 experts was necessary in order to have non-trivial gradients to the routing functions. The authors intuited that learning to route would not work without the ability to compare at least two experts. Ramachandran and Le (2018) went further to study the top-k decision and found that higher k-values in lower layers in the model were important for models with many routing layers. Contrary to these ideas, we instead use a simplified strategy where we route to only a single expert. We show this simplification preserves model quality, reduces routing computation and performs better. This k = 1 routing strategy is later referred to as a Switch layer. Note that for both MoE and Switch Routing, the gate value pi(x) in Equation 2 permits differentiability of the router.\\n\\nThe benefits for the Switch layer are three-fold: (1) The router computation is reduced as we are only routing a token to a single expert. (2) The batch size (expert capacity) of each expert can be at least halved since each token is only being routed to a single expert.3 (3) The routing implementation is simplified and communication costs are reduced. Figure 3 shows an example of routing with different expert capacity factors.\\n\\nFigure 3: Illustration of token routing dynamics. Each expert processes a fixed batch-size of tokens modulated by the capacity factor. Each token is routed to the expert with the highest router probability, but each expert has a fixed batch size of (total tokens / num experts) × capacity factor. If the tokens are unevenly dispatched then certain experts will overflow (denoted by dotted red lines), resulting in these tokens not being processed by this layer. A larger capacity factor alleviates this overflow issue, but also increases computation and communication costs (depicted by padded white/empty slots).\\n\\n2.2 Efficient Sparse Routing\\n\\nWe use Mesh-Tensorflow (MTF) (Shazeer et al., 2018) which is a library, with similar semantics and API to Tensorflow (Abadi et al., 2016) that facilitates efficient distributed data and model parallel architectures. It does so by abstracting the physical set of cores to a logical mesh of processors. Tensors and computations may then be sharded per named dimensions, facilitating easy partitioning of models across dimensions. We design our model with TPUs in mind, which require statically declared sizes. Below we describe our distributed Switch Transformer implementation.\\n\\nSee Section 2.2 for a technical description.\\n\\n$$\\\\left({\\\\boldsymbol{3}}\\\\right)$$\\n\\nDistributed Switch Implementation. All of our tensor shapes are statically determined at compilation time, but our computation is dynamic due to the routing decisions at training and inference. Because of this, one important technical consideration is how to set the expert capacity. The expert capacity—the number of tokens each expert computes—is set by evenly dividing the number of tokens in the batch across the number of experts, and then further expanding by a capacity factor,\\n\\n$${\\\\mathrm{~expert~capacity}}=!!\\\\left({\\\\frac{{\\\\mathrm{tokens~per~batch}}}{{\\\\mathrm{number~of~experts}}}}\\\\right)\\\\times{\\\\mathrm{capacity~factor}}.$$\\n\\nA capacity factor greater than 1.0 creates additional buffer to accommodate for when tokens are not perfectly balanced across experts. If too many tokens are routed to an expert (referred to later as dropped tokens), computation is skipped and the token representation is passed directly to the next layer through the residual connection. Increasing the expert capacity is not without drawbacks, however, since high values will result in wasted computation and memory. This trade-off is explained in Figure 3. Empirically we find ensuring lower rates of dropped tokens are important for the scaling of sparse expert-models.\\n\\nThroughout our experiments we didn\\'t notice any dependency on the number of experts for the number of tokens dropped (typically < 1%). Using the auxiliary load balancing loss (next section) with a high enough coefficient ensured good load balancing. We study the impact that these design decisions have on model quality and speed in Table 1.\\n\\nA Differentiable Load Balancing Loss. To encourage a balanced load across experts we add an auxiliary loss (Shazeer et al., 2017, 2018; Lepikhin et al., 2020). As in Shazeer et al. (2018); Lepikhin et al. (2020), Switch Transformers simplifies the original design in Shazeer et al. (2017) which had separate load-balancing and importance-weighting losses.\\n\\nFor each Switch layer, this auxiliary loss is added to the total model loss during training.\\n\\nGiven N experts indexed by i = 1 to N and a batch B with T tokens, the auxiliary loss is computed as the scaled dot-product between vectors f and P,\\n\\n$$\\\\operatorname{loss}=\\\\alpha\\\\cdot N\\\\cdot\\\\sum_{i=1}^{N}f_{i}\\\\cdot P_{i}$$ $$\\\\left(4\\\\right)$$\\n\\n$$\\\\left({\\\\mathfrak{h}}\\\\right)$$ $$\\\\left({\\\\mathfrak{h}}\\\\right)$$ fi· Pi (4) where $ f_{i}$ is the frac where fiis the fraction of tokens dispatched to expert i,\\n\\ndispatched to expert $i$, . $$f_{i}={\\\\frac{1}{T}}\\\\sum_{x\\\\in{\\\\mathcal{B}}}\\\\mathbbm{1}{{\\\\mathrm{argmax}}\\\\,p(x)=i}$$ 1{argmax p(x) = i} (5) and Piis the fraction of the router probability allocated for expert i,\\n\\nrt $\\\\,i_1$ ? $$P_{i}={\\\\frac{1}{T}}\\\\sum_{x\\\\in{\\\\mathcal{B}}}p_{i}(x).$$ pi(x). (6) Since we seek uniform routing of the batch of tokens across the N experts, we desire both vectors to have values of 1/N. The auxiliary loss of Equation 4 encourages uniform routing since it is minimized under a uniform distribution. The objective can also be differentiated as\\n\\nthe P-vector is differentiable, but the f-vector is not. The final loss is multiplied by expert count N to keep the loss constant as the number of experts varies since under uniform routing PN i=1(fi· Pi) = PN i=1( 1 N · 1 N ) = 1N . Finally, a hyper-parameter α is a multiplicative coefficient for these auxiliary losses; throughout this work we use an α = 10−2 which was sufficiently large to ensure load balancing while small enough to not to overwhelm the primary cross-entropy objective. We swept hyper-parameter ranges of α from 10−1to 10−5 in powers of 10 and found 10−2 balanced load quickly without interfering with training loss.\\n\\n2.3 Putting It All Together: The Switch Transformer\\n\\nOur first test of the Switch Transformer starts with pre-training on the \"Colossal Clean Crawled Corpus\" (C4), introduced in (Raffel et al., 2019). For our pre-training objective, we use a masked language modeling task (Taylor, 1953; Fedus et al., 2018; Devlin et al., 2018) where the model is trained to predict missing tokens. In our pre-training setting, as determined in Raffel et al. (2019) to be optimal, we drop out 15% of tokens and then replace the masked sequence with a single sentinel token. To compare our models, we record the negative log perplexity.4 Throughout all tables in the paper, ↑ indicates that a higher value for that metric is better and vice-versa for ↓. A comparison of all the models studied in this work are in Table 9.\\n\\nA head-to-head comparison of the Switch Transformer and the MoE Transformer is presented in Table 1. Our Switch Transformer model is FLOP-matched to \\'T5-Base\\' (Raffel et al., 2019) (same amount of computation per token is applied). The MoE Transformer, using top-2 routing, has two experts which each apply a separate FFN to each token and thus its FLOPS are larger. All models were trained for the same number of steps on identical hardware. Note that the MoE model going from capacity factor 2.0 to 1.25 actually slows down (840 to 790) in the above experiment setup, which is unexpected.5 We highlight three key findings from Table 1: (1) Switch Transformers outperform both carefully tuned dense models and MoE Transformers on a speed-quality basis. For a fixed amount of computation and wall-clock time, Switch Transformers achieve the best result. (2) The Switch Transformer has a smaller computational footprint than the MoE counterpart. If we increase its size to match the training speed of the MoE Transformer, we find this outperforms all MoE and Dense models on a per step basis as well. (3) Switch Transformers perform better at lower capacity factors (1.0, 1.25). Smaller expert capacities are indicative of the scenario in the large model regime where model memory is very scarce and the capacity factor will want to be made as small as possible.\\n\\n2.4 Improved Training And Fine-Tuning Techniques\\n\\nSparse expert models may introduce training difficulties over a vanilla Transformer. Instability can result because of the hard-switching (routing) decisions at each of these layers.\\n\\nFurther, low precision formats like bfloat16 (Wang and Kanwar, 2019) can exacerbate issues\\n\\nModel Capacity Quality after Time to Quality Speed (↑) Factor 100k steps (↑) Threshold (↓) (examples/sec) (Neg. Log Perp.) (hours) T5-Base - -1.731 Not achieved† 1600 T5-Large - -1.550 131.1 470 MoE-Base 2.0 -1.547 68.7 840 Switch-Base 2.0 -1.554 72.8 860 MoE-Base 1.25 -1.559 80.7 790 Switch-Base 1.25 -1.553 65.0 910 MoE-Base 1.0 -1.572 80.1 860 Switch-Base 1.0 -1.561 62.8 1000 Switch-Base+ 1.0 -1.534 67.6 780\\n\\nTable 1: Benchmarking Switch versus MoE. Head-to-head comparison measuring per step and per time benefits of the Switch Transformer over the MoE Transformer and T5 dense baselines. We measure quality by the negative log perplexity and the time to reach an arbitrary chosen quality threshold of Neg. Log Perp.=-1.50. All MoE and Switch Transformer models use 128 experts, with experts at every other feed-forward layer. For Switch-Base+, we increase the model size until it matches the speed of the MoE model by increasing the model hidden-size from 768 to 896 and the number of heads from 14 to 16. All models are trained with the same amount of computation (32 cores) and on the same hardware (TPUv3). Further note that all our models required pre-training beyond 100k steps to achieve our level threshold of -1.50. † T5-Base did not achieve this negative log perplexity in the 100k steps the models were trained.\\n\\nin the softmax computation for our router. We describe training difficulties here and the methods we use to overcome them to achieve stable and scalable training.\\n\\nSelective precision with large sparse models. Model instability hinders the ability to train using efficient bfloat16 precision, and as a result, Lepikhin et al. (2020) trains with float32 precision throughout their MoE Transformer. However, we show that by instead selectively casting to float32 precision within a localized part of the model, stability may be achieved, without incurring expensive communication cost of float32 tensors. This technique is inline with modern mixed precision training strategies where certain parts of the model and gradient updates are done in higher precision Micikevicius et al. (2017). Table 2 shows that our approach permits nearly equal speed to bfloat16 training while conferring the training stability of float32.\\n\\nTo achieve this, we cast the router input to float32 precision. The router function takes the tokens as input and produces the dispatch and combine tensors used for the selection and recombination of expert computation (refer to Code Block 15 in the Appendix for details). Importantly, the float32 precision is only used within the body of the router function—on computations local to that device. Because the resulting dispatch and combine tensors are recast to bfloat16 precision at the end of the function, no expensive float32 tensors\\n\\nModel Quality Speed (precision) (Neg. Log Perp.) (↑) (Examples/sec) (↑) Switch-Base (float32) -1.718 1160 Switch-Base (bfloat16) -3.780 [diverged] 1390 Switch-Base (Selective precision) -1.716 1390\\n\\nTable 2: Selective precision. We cast the local routing operations to float32 while preserving bfloat16 precision elsewhere to stabilize our model while achieving nearly equal speed to (unstable) bfloat16-precision training. We measure the quality of a 32 expert model after a fixed step count early in training its speed performance. For both Switch-Base in float32 and with Selective prevision we notice similar learning dynamics. are broadcast through all-to-all communication operations, but we still benefit from the increased stability of float32.\\n\\nSmaller parameter initialization for stability. Appropriate initialization is critical to successful training in deep learning and we especially observe this to be true for Switch Transformer. We initialize our weight matrices by drawing elements from a truncated normal distribution with mean µ = 0 and standard deviation σ =ps/n where s is a scale hyper-parameter and n is the number of input units in the weight tensor (e.g. fan-in).6 As an additional remedy to the instability, we recommend reducing the default Transformer initialization scale s = 1.0 by a factor of 10. This both improves quality and reduces the likelihood of destabilized training in our experiments. Table 3 measures the improvement of the model quality and reduction of the variance early in training. We find that\\n\\nModel (Initialization scale) Average Quality Std. Dev. of Quality (Neg. Log Perp.) (Neg. Log Perp.) Switch-Base (0.1x-init) -2.72 0.01 Switch-Base (1.0x-init) -3.60 0.68\\n\\nTable 3: Reduced initialization scale improves stability. Reducing the initialization scale results in better model quality and more stable training of Switch Transformer. Here we record the average and standard deviation of model quality, measured by the negative log perplexity, of a 32 expert model after 3.5k steps (3 random seeds each). the average model quality, as measured by the Neg. Log Perp., is dramatically improved and there is a far reduced variance across runs. Further, this same initialization scheme is broadly effective for models spanning several orders of magnitude. We use the same approach to stably train models as small as our 223M parameter baseline to enormous models in excess of one trillion parameters.\\n\\nRegularizing large sparse models. Our paper considers the common NLP approach of pre-training on a large corpus followed by fine-tuning on smaller downstream tasks such as summarization or question answering. One issue that naturally arises is overfitting since many fine-tuning tasks have very few examples. During fine-tuning of standard Transformers, Raffel et al. (2019) use dropout (Srivastava et al., 2014) at each layer to prevent overfitting. Our Switch Transformers have significantly more parameters than the FLOP matched dense baseline, which can lead to more severe overfitting on these smaller downstream tasks.\\n\\nModel (dropout) GLUE CNNDM SQuAD SuperGLUE T5-Base (d=0.1) 82.9 19.6 83.5 72.4 Switch-Base (d=0.1) 84.7 19.1 83.7 73.0 Switch-Base (d=0.2) 84.4 19.2 83.9 73.2 Switch-Base (d=0.3) 83.9 19.6 83.4 70.7 Switch-Base (d=0.1, ed=0.4) 85.2 19.6 83.7 73.0\\n\\nTable 4: Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set (higher numbers are better). We observe that using a lower standard dropout rate at all non-expert layer, with a much larger dropout rate on the expert feed-forward layers, to perform the best.\\n\\nWe thus propose a simple way to alleviate this issue during fine-tuning: increase the dropout inside the experts, which we name as expert dropout. During fine-tuning we simply increase the dropout rate by a significant amount only at the interim feed-forward computation at each expert layer. Table 4 has the results for our expert dropout protocol.\\n\\nWe observe that simply increasing the dropout across all layers leads to worse performance.\\n\\nHowever, setting a smaller dropout rate (0.1) at non-expert layers and a much larger dropout rate (0.4) at expert layers leads to performance improvements on four smaller downstream tasks.\\n\\n3. Scaling Properties\\n\\nWe present a study of the scaling properties of the Switch Transformer architecture during pre-training. Per Kaplan et al. (2020), we consider a regime where the model is not bottlenecked by either the computational budget or amount of data. To avoid the data bottleneck, we use the large C4 corpus with over 180B target tokens (Raffel et al., 2019) and we train until diminishing returns are observed.\\n\\nThe number of experts is the most efficient dimension for scaling our model. Increasing the experts keeps the computational cost approximately fixed since the model only selects one expert per token, regardless of the number of experts to choose from. The router must compute a probability distribution over more experts, however, this is a lightweight computation of cost O(dmodel × num experts) where dmodel is the embedding dimension of tokens passed between the layers. In this section, we consider the scaling properties on a step-basis and a time-basis with a fixed computational budget.\\n\\n3.1 Scaling Results On A Step-Basis\\n\\nFigure 4 demonstrates consistent scaling benefits with the number of experts when training all models for a fixed number of steps. We observe a clear trend: when keeping the FLOPS per token fixed, having more parameters (experts) speeds up training. The left Figure demonstrates consistent scaling properties (with fixed FLOPS per token) between sparse model parameters and test loss. This reveals the advantage of scaling along this additional axis of sparse model parameters. Our right Figure measures sample efficiency of a dense model variant and four FLOP-matched sparse variants. We find that increasing the number of experts leads to more sample efficient models. Our Switch-Base 64 expert model achieves the same performance of the T5-Base model at step 60k at step 450k, which is a 7.5x speedup in terms of step time. In addition, consistent with the findings of Kaplan et al. (2020), we find that larger models are also more sample efficient—learning more quickly for a fixed number of observed tokens.\\n\\nFigure 4: Scaling properties of the Switch Transformer. Left Plot: We measure the quality improvement, as measured by perplexity, as the parameters increase by scaling the number of experts. The top-left point corresponds to the T5-Base model with 223M parameters. Moving from top-left to bottom-right, we double the number of experts from 2, 4, 8 and so on until the bottom-right point of a 256 expert model with 14.7B parameters. Despite all models using an equal computational budget, we observe consistent improvements scaling the number of experts. Right Plot: Negative log perplexity per step sweeping over the number of experts. The dense baseline is shown with the purple line and we note improved sample efficiency of our Switch-Base models.\\n\\n3.2 Scaling Results On A Time-Basis\\n\\nFigure 4 demonstrates that on a step basis, as we increase the number of experts, the performance consistently improves. While our models have roughly the same amount of FLOPS per token as the baseline, our Switch Transformers incurs additional communication costs across devices as well as the extra computation of the routing mechanism. Therefore, the increased sample efficiency observed on a step-basis doesn\\'t necessarily translate to a better model quality as measured by wall-clock. This raises the question: For a fixed training duration and computational budget, should one train a dense or a sparse model?\\n\\nFigure 5: Speed advantage of Switch Transformer. All models trained on 32 TPUv3 cores with equal FLOPs per example. For a fixed amount of computation and training time, Switch Transformers significantly outperform the dense Transformer baseline. Our 64 expert Switch-Base model achieves the same quality in one-seventh the time of the T5-Base and continues to improve.\\n\\nFigures 5 and 6 address this question. Figure 5 measures the pre-training model quality as a function of time. For a fixed training duration and computational budget, Switch Transformers yield a substantial speed-up. In this setting, our Switch-Base 64 expert model trains in one-seventh the time that it would take the T5-Base to get similar perplexity.\\n\\n3.3 Scaling Versus A Larger Dense Model\\n\\nThe above analysis shows that a computationally-matched dense model is outpaced by its Switch counterpart. Figure 6 considers a different scenario: what if we instead had allocated our resources to a larger dense model? We do so now, measuring Switch-Base against the next strong baseline, T5-Large. But despite T5-Large applying 3.5x more FLOPs per token, Switch-Base is still more sample efficient and yields a 2.5x speedup. Furthermore, more gains can be had simply by designing a new, larger sparse version, Switch-Large, which is FLOP-matched to T5-Large. We do this and demonstrate superior scaling and fine-tuning in the following section.\\n\\n4. Downstream Results\\n\\nSection 3 demonstrated the superior scaling properties while pre-training, but we now validate that these gains translate to improved language learning abilities on downstream tasks. We begin by fine-tuning on a diverse set of NLP tasks. Next we study reducing the memory footprint of our sparse models by over 90% by distilling into small—and easily deployed—dense baselines. Finally, we conclude this section measuring the improvements in a multi-task, multilingual setting, where we show that Switch Transformers are strong multi-task learners, improving over the multilingual T5-base model across all 101 languages.\\n\\n4.1 Fine-Tuning\\n\\nBaseline and Switch models used for fine-tuning. Our baselines are the highly-tuned 223M parameter T5-Base model and the 739M parameter T5-Large model (Raffel et al., 2019). For both versions, we design a FLOP-matched Switch Transformer, with many more parameters, which is summarized in Table 9.\\n\\n7 Our baselines differ slightly from those in Raffel et al. (2019) because we pre-train on an improved C4 corpus which removes intraexample text duplication and thus increases the efficacy as a pre-training task Lee et al.\\n\\nFLOPS are calculated for the forward pass as done in Kaplan et al. (2020).\\n\\n(2021). In our protocol we pre-train with 220 (1,048,576) tokens per batch for 550k steps amounting to 576B total tokens. We then fine-tune across a diverse set of tasks using a dropout rate of 0.1 for all layers except the Switch layers, which use a dropout rate of 0.4 (see Table 4). We fine-tune using a batch-size of 1M for 16k steps and for each task, we evaluate model quality every 200-steps and report the peak performance as computed on the validation set.\\n\\nFine-tuning tasks and data sets. We select tasks probing language capabilities including question answering, summarization and knowledge about the world. The language benchmarks GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) are handled as composite mixtures with all the tasks blended in proportion to the amount of tokens present in each. These benchmarks consist of tasks requiring sentiment analysis (SST2), word sense disambiguation (WIC), sentence similarty (MRPC, STS-B, QQP), natural language inference (MNLI, QNLI, RTE, CB), question answering (MultiRC, RECORD, BoolQ), coreference resolution (WNLI, WSC) and sentence completion (COPA) and sentence acceptability (CoLA). The CNNDM (Hermann et al., 2015) and BBC XSum (Narayan et al., 2018) data sets are used to measure the ability to summarize articles. Question answering is probed with the SQuAD data set (Rajpurkar et al., 2016) and the ARC Reasoning Challenge (Clark et al., 2018). And as in Roberts et al. (2020), we evaluate the knowledge of our models by fine-tuning on three closed-book question answering data sets: Natural Questions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) and Trivia QA (Joshi et al., 2017). Closed-book refers to questions posed with no supplemental reference or context material. To gauge the model\\'s common sense reasoning we evaluate it on the Winogrande Schema Challenge (Sakaguchi et al., 2020). And finally, we test our model\\'s natural language inference capabilities on the Adversarial NLI Benchmark (Nie et al., 2019).\\n\\nFine-tuning metrics. The following evaluation metrics are used throughout the paper: We report the average scores across all subtasks for GLUE and SuperGLUE. The Rouge-2 metric is used both the CNNDM and XSum. In SQuAD and the closed book tasks (Web, Natural, and Trivia Questions) we report the percentage of answers exactly matching the target (refer to Roberts et al. (2020) for further details and deficiency of this measure). Finally, in ARC Easy, ARC Challenge, ANLI, and Winogrande we report the accuracy of the generated responses.\\n\\nFine-tuning results. We observe significant downstream improvements across many natural language tasks. Notable improvements come from SuperGLUE, where we find FLOP-matched Switch variants improve by 4.4 and 2 percentage points over the T5-Base and T5-Large baselines, respectively as well as large improvements in Winogrande, closed book Trivia QA, and XSum.8In our fine-tuning study, the only tasks where we do not observe gains are on the AI2 Reasoning Challenge (ARC) data sets where the T5-Base outperforms Switch-Base on the challenge data set and T5-Large outperforms Switch-Large on the easy data set. Taken as a whole, we observe significant improvements spanning both reasoning and knowledge-heavy tasks. This validates our architecture, not just as one that pre-trains well, but can translate quality improvements to downstream tasks via fine-tuning.\\n\\nModel GLUE SQuAD SuperGLUE Winogrande (XL) T5-Base 84.3 85.5 75.1 66.6 Switch-Base 86.7 87.2 79.5 73.3 T5-Large 87.8 88.1 82.7 79.1 Switch-Large 88.5 88.6 84.7 83.0 Model XSum ANLI (R3) ARC Easy ARC Chal. T5-Base 18.7 51.8 56.7 35.5 Switch-Base 20.3 54.0 61.3 32.8 T5-Large 20.9 56.6 68.8 35.5 Switch-Large 22.3 58.6 66.0 35.5 Model CB Web QA CB Natural QA CB Trivia QA T5-Base 26.6 25.8 24.5 Switch-Base 27.4 26.8 30.7 T5-Large 27.7 27.6 29.5 Switch-Large 31.3 29.5 36.9\\n\\nTable 5: Fine-tuning results. Fine-tuning results of T5 baselines and Switch models across a diverse set of natural language tests (validation sets; higher numbers are better).\\n\\nWe compare FLOP-matched Switch models to the T5-Base and T5-Large baselines. For most tasks considered, we find significant improvements of the Switchvariants. We observe gains across both model sizes and across both reasoning and knowledge-heavy language tasks.\\n\\n4.2 Distillation\\n\\nDeploying massive neural networks with billions, or trillions, of parameters is inconvenient.\\n\\nTo alleviate this, we study distilling (Hinton et al., 2015) large sparse models into small dense models. Future work could additionally study distilling large models into smaller sparse models.\\n\\nDistillation techniques. In Table 6 we study a variety of distillation techniques.\\n\\nThese techniques are built off of Sanh et al. (2019), who study distillation methods for BERT models. We find that initializing the dense model with the non-expert weights yields a modest improvement. This is possible since all models are FLOP matched, so non-expert layers will have the same dimensions. Since expert layers are usually only added at every or every other FFN layer in a Transformer, this allows for many of the weights to be initialized with trained parameters. Furthermore, we observe a distillation improvement using a mixture of 0.25 for the teacher probabilities and 0.75 for the ground truth label. By combining both techniques we preserve ≈ 30% of the quality gains from the larger sparse models with only ≈ 1/20th of the parameters. The quality gain refers to the percent of\\n\\nTechnique Parameters Quality (↑) T5-Base 223M -1.636 Switch-Base 3,800M -1.444 Distillation 223M (3%) -1.631 + Init. non-expert weights from teacher 223M (20%) -1.598 + 0.75 mix of hard and soft loss 223M (29%) -1.580 Initialization Baseline (no distillation) Init. non-expert weights from teacher 223M -1.639\\n\\nthe quality difference between Switch-Base (Teacher) and T5-Base (Student). Therefore, a quality gain of 100% implies the Student equals the performance of the Teacher.\\n\\nTable 6: Distilling Switch Transformers for Language Modeling. Initializing T5-Base with the non-expert weights from Switch-Base and using a loss from a mixture of teacher and ground-truth labels obtains the best performance. We can distill 30% of the performance improvement of a large sparse model with 100x more parameters back into a small dense model. For a final baseline, we find no improvement of T5-Base initialized with the expert weights, but trained normally without distillation.\\n\\nAchievable compression rates. Using our best distillation technique described in Table 6, we distill a wide variety of sparse models into dense models. We distill SwitchBase versions, sweeping over an increasing number of experts, which corresponds to varying between 1.1B to 14.7B parameters. Through distillation, we can preserve 37% of the quality gain of the 1.1B parameter model while compressing 82%. At the extreme, where we compress the model 99%, we are still able to maintain 28% of the teacher\\'s model quality improvement.\\n\\nDistilling a fine-tuned model. We conclude this with a study of distilling a finetuned sparse model into a dense model. Table 8 shows results of distilling a 7.4B parameter Switch-Base model, fine-tuned on the SuperGLUE task, into the 223M T5-Base. Similar to our pre-training results, we find we are able to preserve 30% of the gains of the sparse model when distilling into a FLOP matched dense variant. One potential future avenue, not considered here, may examine the specific experts being used for fine-tuning tasks and extracting them to achieve better model compression.\\n\\n4.3 Multilingual Learning\\n\\nIn our final set of downstream experiments, we measure the model quality and speed tradeoffs while pre-training on a mixture of 101 different languages. We build and benchmark off the recent work of mT5 (Xue et al., 2020), a multilingual extension to T5. We pre-train on the multilingual variant of the Common Crawl data set (mC4) spanning 101 languages introduced in mT5, but due to script variants within certain languages, the mixture contains 107 tasks.\\n\\nIn Figure 7 we plot the quality improvement in negative log perplexity for all languages of a FLOP-matched Switch model, mSwitch-Base to the T5 base variant, mT5-Base. After\\n\\nDense Sparse Parameters 223M 1.1B 2.0B 3.8B 7.4B 14.7B Pre-trained Neg. Log Perp. (↑) -1.636 -1.505 -1.474 -1.444 -1.432 -1.427 Distilled Neg. Log Perp. (↑) - -1.587 -1.585 -1.579 -1.582 -1.578 Percent of Teacher Performance - 37% 32% 30 % 27 % 28 % Compression Percent - 82 % 90 % 95 % 97 % 99 %\\n\\nTable 7: Distillation compression rates. We measure the quality when distilling large sparse models into a dense baseline. Our baseline, T5-Base, has a -1.636 Neg. Log Perp. quality. In the right columns, we then distill increasingly large sparse models into this same architecture. Through a combination of weight-initialization and a mixture of hard and soft losses, we can shrink our sparse teachers by 95%+ while preserving 30% of the quality gain. However, for significantly better and larger pre-trained teachers, we expect larger student models would be necessary to achieve these compression rates.\\n\\nModel Parameters FLOPS SuperGLUE (↑) T5-Base 223M 124B 74.6 Switch-Base 7410M 124B 81.3 Distilled T5-Base 223M 124B (30%) 76.6\\n\\nTable 8: Distilling a fine-tuned SuperGLUE model. We distill a Switch-Base model finetuned on the SuperGLUE tasks into a T5-Base model. We observe that on smaller data sets our large sparse model can be an effective teacher for distillation. We find that we again achieve 30% of the teacher\\'s performance on a 97% compressed model. pre-training both versions for 1M steps, we find that on all 101 languages considered, Switch Transformer increases the final negative log perplexity over the baseline. In Figure 8, we present a different view and now histogram the per step speed-up of using Switch Transformer over the mT5-Base.9 We find a mean speed-up over mT5-Base of 5x and that 91% of languages achieve at least a 4x speedup. This presents evidence that Switch Transformers are effective multi-task and multi-lingual learners.\\n\\n5. Designing Models With Data, Model, And Expert-Parallelism\\n\\nArbitrarily increasing the number of experts is subject to diminishing returns (Figure 4). Here we describe complementary scaling strategies. The common way to scale a Transformer is to increase dimensions in tandem, like dmodel or df f . This increases both the parameters\\n\\nFigure 7: Multilingual pre-training on 101 languages. Improvements of Switch T5 Base model over dense baseline when multi-task training on 101 languages. We observe Switch Transformers to do quite well in the multi-task training setup and yield improvements on all 101 languages.\\n\\nFigure 8: Multilingual pre-training on 101 languages. We histogram for each language, the step speedup of Switch Transformers over the FLOP matched T5 dense baseline to reach the same quality. Over all 101 languages, we achieve a mean step speedup over mT5-Base of 5x and, for 91% of languages, we record a 4x, or greater, speedup to reach the final perplexity of mT5-Base.\\n\\nand computation performed and is ultimately limited by the memory per accelerator. Once it exceeds the size of the accelerator\\'s memory, single program multiple data (SPMD) modelparallelism can be employed. This section studies the trade-offs of combining data, model, and expert-parallelism.\\n\\nReviewing the Feed-Forward Network (FFN) Layer. We use the FFN layer as an example of how data, model and expert-parallelism works in Mesh TensorFlow (Shazeer et al., 2018) and review it briefly here. We assume B tokens in the batch, each of dimension dmodel. Both the input (x) and output (y) of the FFN are of size [B, dmodel] and the intermediate (h) is of size [B, df f ] where df f is typically several times larger than dmodel. In the FFN, the intermediate is h = xWin and then the output of the layer is y = ReLU(h)Wout.\\n\\nThus Win and Wout are applied independently to each token and have sizes [dmodel, df f ] and [df f , dmodel].\\n\\nWe describe two aspects of partitioning: how the weights and batches of data divide over cores, depicted in Figure 9. We denote all cores available as N which Mesh Tensorflow may then remap into a logical multidimensional mesh of processors. Here we create a two-dimensional logical mesh, with one dimension representing the number of ways for data-parallel sharding (n) and the other, the model-parallel sharding (m). The total cores must equal the ways to shard across both data and model-parallelism, e.g. N = n × m.\\n\\nTo shard the layer across cores, the tensors containing that batch of B tokens are sharded across n data-parallel cores, so each core contains B/n tokens. Tensors and variables with df f are then sharded across m model-parallel cores. For the variants with experts-layers, we consider E experts, each of which can process up to C tokens.\\n\\nTerm Description B Number of tokens in the batch. N Number of total cores. n Number of ways for data-parallelism sharding. m Number of ways for model-parallelism sharding. E Number of experts in Switch layers. C Expert capacity, the batch size of each expert.\\n\\n5.1 Data Parallelism\\n\\nWhen training data parallel models, which is the standard for distributed training, then all cores are allocated to the data-parallel dimension or n = N, m = 1. This has the advantage that no communication is needed until the entire forward and backward pass is finished and the gradients need to be then aggregated across all cores. This corresponds to the left-most column of Figure 9.\\n\\n5.2 Model Parallelism\\n\\nWe now consider a scenario where all cores are allocated exclusively to the model-parallel dimension and so n = 1, m = N. Now all cores must keep the full B tokens and each core will contain a unique slice of the weights. For each forward and backward pass, a communication cost is now incurred. Each core sends a tensor of [B, dmodel] to compute the second matrix multiplication ReLU(h)Wout because the df f dimension is partitioned and must be summed over. As a general rule, whenever a dimension that is partitioned across cores must be summed, then an all-reduce operation is added for both the forward and backward pass. This contrasts with pure data parallelism where an all-reduce only occurs at the end of the entire forward and backward pass.\\n\\nFigure 9: Data and weight partitioning strategies. Each 4×4 dotted-line grid represents 16 cores and the shaded squares are the data contained on that core (either model weights or batch of tokens). We illustrate both how the model weights and the data tensors are split for each strategy. First Row: illustration of how model weights are split across the cores. Shapes of different sizes in this row represent larger weight matrices in the Feed Forward Network (FFN) layers (e.g larger df f sizes). Each color of the shaded squares identifies a unique weight matrix. The number of parameters per core is fixed, but larger weight matrices will apply more computation to each token. Second Row: illustration of how the data batch is split across cores. Each core holds the same number of tokens which maintains a fixed memory usage across all strategies. The partitioning strategies have different properties of allowing each core to either have the same tokens or different tokens across cores, which is what the different colors symbolize.\\n\\n5.3 Model And Data Parallelism\\n\\nIt is common to mix both model and data parallelism for large scale models, which was done in the largest T5 models (Raffel et al., 2019; Xue et al., 2020) and in GPT-3 (Brown et al., 2020). With a total of N = n × m cores, now each core will be responsible for B/n tokens and df f /m of both the weights and intermediate activation. In the forward and backward pass each core communicates a tensor of size [B/n, dmodel] in an all-reduce operation.\\n\\n5.4 Expert And Data Parallelism\\n\\nNext we describe the partitioning strategy for expert and data parallelism. Switch Transformers will allocate all of their cores to the data partitioning dimension n, which will also correspond to the number of experts in the model. For each token per core a router locally computes assignments to the experts. The output is a binary matrix of size [n, B/n, E, C] which is partitioned across the first dimension and determines expert assignment. This binary matrix is then used to do a gather via matrix multiplication with the input tensor of [n, B/n, dmodel].\\n\\neinsum([n, B/n, dmodel], [n, B/n, E, C], dimension = [B/n]) (7) resulting in the final tensor of shape [n, E, C, dmodel], which is sharded across the first dimension. Because each core has its own expert, we do an all-to-all communication of size [E, C, dmodel] to now shard the E dimension instead of the n-dimension. There are additional communication costs of bfloat16 tensors of size E×C ×dmodel in the forward pass to analogusly receive the tokens from each expert located on different cores. See Appendix F for a detailed analysis of the expert partitioning code.\\n\\n5.5 Expert, Model And Data Parallelism\\n\\nIn the design of our best model, we seek to balance the FLOPS per token and the parameter count. When we scale the number of experts, we increase the number of parameters, but do not change the FLOPs per token. In order to increase FLOPs, we must also increase the df f dimension (which also increases parameters, but at a slower rate). This presents a trade-off: as we increase df f we will run out of memory per core, which then necessitates increasing m. But since we have a fixed number of cores N, and N = n × m, we must decrease n, which forces use of a smaller batch-size (in order to hold tokens per core constant).\\n\\nWhen combining both model and expert-parallelism, we will have all-to-all communication costs from routing the tokens to the correct experts along with the internal all-reduce communications from the model parallelism. Balancing the FLOPS, communication costs and memory per core becomes quite complex when combining all three methods where the best mapping is empirically determined. See our further analysis in section 5.6 for how the number of experts effects the downstream performance as well.\\n\\n5.6 Towards Trillion Parameter Models\\n\\nCombining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters, respectively. We study how these models perform on both up-stream pre-training as language models and their downstream fine-tuning performance. The parameters, FLOPs per sequence and hyper-parameters of the two different models are listed below in Table 9. Standard hyper-parameters of the Transformer, including dmodel, df f , dkv, number of heads and number of layers are described, as well as a less common feature, F F NGEGLU , which refers to a variation of the FFN layer where the expansion matrix is substituted with two sets of weights which are non-linearly combined (Shazeer, 2020).\\n\\nThe Switch-C model is designed using only expert-parallelism, and no model-parallelism, as described earlier in Section 5.4. As a result, the hyper-parameters controlling the width,\\n\\nModel Parameters FLOPs/seq dmodel F F NGEGLU df f dkv Num. Heads T5-Base 0.2B 124B 768 X 2048 64 12 T5-Large 0.7B 425B 1024 X 2816 64 16 T5-XXL 11B 6.3T 4096 X 10240 64 64 Switch-Base 7B 124B 768 X 2048 64 12 Switch-Large 26B 425B 1024 X 2816 64 16 Switch-XXL 395B 6.3T 4096 X 10240 64 64 Switch-C 1571B 890B 2080 6144 64 32 Model Expert Freq. Num. Layers Num Experts Neg. Log Perp. @250k Neg. Log Perp. @ 500k T5-Base - 12 - -1.599 -1.556 T5-Large - 24 - -1.402 -1.350 T5-XXL - 24 - -1.147 -1.095 Switch-Base 1/2 12 128 -1.370 -1.306 Switch-Large 1/2 24 128 -1.248 -1.177 Switch-XXL 1/2 24 64 -1.086 -1.008 Switch-C 1 15 2048 -1.096 -1.043\\n\\nTable 9: Switch model design and pre-training performance. We compare the hyperparameters and pre-training performance of the T5 models to our Switch Transformer variants. The last two columns record the pre-training model quality on the C4 data set after 250k and 500k steps, respectively. We observe that the SwitchC Transformer variant is 4x faster to a fixed perplexity (with the same compute budget) than the T5-XXL model, with the gap increasing as training progresses.\\n\\ndepth, number of heads, and so on, are all much smaller than the T5-XXL model. In contrast, the Switch-XXL is FLOP-matched to the T5-XXL model, which allows for larger dimensions of the hyper-parameters, but at the expense of additional communication costs induced by model-parallelism (see Section 5.5 for more details).\\n\\nSample efficiency versus T5-XXL. In the final two columns of Table 9 we record the negative log perplexity on the C4 corpus after 250k and 500k steps, respectively. After 250k steps, we find both Switch Transformer variants to improve over the T5-XXL version\\'s negative log perplexity by over 0.061.10 To contextualize the significance of a gap of 0.061, we note that the T5-XXL model had to train for an additional 250k steps to increase 0.052. The gap continues to increase with additional training, with the Switch-XXL model out-performing the T5-XXL by 0.087 by 500k steps.\\n\\nTraining instability. However, as described in the introduction, large sparse models can be unstable, and as we increase the scale, we encounter some sporadic issues. We find that the larger Switch-C model, with 1.6T parameters and 2048 experts, exhibits no training instability at all. Instead, the Switch XXL version, with nearly 10x larger FLOPs per sequence, is sometimes unstable. As a result, though this is our better model on a step-basis, we do not pre-train for a full 1M steps, in-line with the final reported results of T5 (Raffel et al., 2019).\\n\\nReasoning fine-tuning performance. As a preliminary assessment of the model quality, we use a Switch-XXL model partially pre-trained on 503B tokens, or approximately half the text used by the T5-XXL model. Using this checkpoint, we conduct multi-task training for efficiency, where all tasks are learned jointly, rather than individually fine-tuned.\\n\\nWe find that SQuAD accuracy on the validation set increases to 89.7 versus state-of-the-art of 91.3. Next, the average SuperGLUE test score is recorded at 87.5 versus the T5 version obtaining a score of 89.3 compared to the state-of-the-art of 90.0 (Wang et al., 2019). On ANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7 accuracy versus the prior best of 49.4 (Yang et al., 2020). We note that while the SwitchXXL has state-of-the-art Neg. Log Perp. on the upstream pre-training task, its gains have not yet fully translated to SOTA downstream performance. We study this issue more in Appendix E.\\n\\nKnowledge-based fine-tuning performance. Finally, we also conduct an early examination of the model\\'s knowledge with three closed-book knowledge-based tasks: Natural Questions, WebQuestions and TriviaQA, without additional pre-training using Salient Span Masking (Guu et al., 2020). In all three cases, we observe improvements over the prior stateof-the-art T5-XXL model (without SSM). Natural Questions exact match increases to 34.4 versus the prior best of 32.8, Web Questions increases to 41.0 over 37.2, and TriviaQA increases to 47.5 versus 42.9.\\n\\nSumming up, despite training on less than half the data of other models, we already find comparable, and sometimes state-of-the-art, model quality. Currently, the Switch Transformer translates substantial upstream gains better to knowledge-based tasks, than reasoning-tasks (see Appendix E). Extracting stronger fine-tuning performance from large expert models is an active research question, and the pre-training perplexity indicates future improvements should be possible.\\n\\n6. Related Work\\n\\nThe importance of scale in neural networks is widely recognized and several approaches have been proposed. Recent works have scaled models to billions of parameters through using model parallelism (e.g. splitting weights and tensors across multiple cores) (Shazeer et al., 2018; Rajbhandari et al., 2019; Raffel et al., 2019; Brown et al., 2020; Shoeybi et al., 2019). Alternatively, Harlap et al. (2018); Huang et al. (2019) propose using pipeline based model parallelism, where different layers are split across devices and micro-batches are pipelined to the different layers. Finally, Product Key networks (Lample et al., 2019) were proposed to scale up the capacity of neural networks by doing a lookup for learnable embeddings based on the incoming token representations to a given layer.\\n\\nOur work studies a specific model in a class of methods that do conditional computation, where computation decisions are made dynamically based on the input. Cho and Bengio (2014) proposed adaptively selecting weights based on certain bit patterns occuring in the model hidden-states. Eigen et al. (2013) built stacked expert layers with dense matrix multiplications and ReLU activations and showed promising results on jittered MNIST and monotone speech. In computer vision Puigcerver et al. (2020) manually route tokens based on semantic classes during upstream pre-training and then select the relevant experts to be used according to the downstream task.\\n\\nMixture of Experts (MoE), in the context of modern deep learning architectures, was proven effective in Shazeer et al. (2017). That work added an MoE layer which was stacked between LSTM (Hochreiter and Schmidhuber, 1997) layers, and tokens were separately routed to combinations of experts. This resulted in state-of-the-art results in language modeling and machine translation benchmarks. The MoE layer was reintroduced into the Transformer architecture by the Mesh Tensorflow library (Shazeer et al., 2018) where MoE layers were introduced as a substitute of the FFN layers, however, there were no accompanying NLP results. More recently, through advances in machine learning infrastructure, GShard (Lepikhin et al., 2020), which extended the XLA compiler, used the MoE Transformer to dramatically improve machine translation across 100 languages. Finally Fan et al.\\n\\n(2021) chooses a different deterministic MoE strategy to split the model parameters into non-overlapping groups of languages.\\n\\nSparsity along the sequence length dimension (L) in the Transformer attention patterns has been a successful technique to reduce the attention complexity from O(L 2) (Child et al., 2019; Correia et al., 2019; Sukhbaatar et al., 2019; Kitaev et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020). This has enabled learning longer sequences than previously possible. This version of the Switch Transformer does not employ attention sparsity, but these techniques are complimentary, and, as future work, these could be combined to potentially improve learning on tasks requiring long contexts.\\n\\n7. Discussion\\n\\nWe pose and discuss questions about the Switch Transformer, and sparse expert models generally, where sparsity refers to weights, not on attention patterns.\\n\\nIsn\\'t Switch Transformer better due to sheer parameter count? Yes, and by design! Parameters, independent of the total FLOPs used, are a useful axis to scale neural language models. Large models have been exhaustively shown to perform better (Kaplan et al., 2020). But in this case, our model is more sample efficient and faster while using the same computational resources.\\n\\nI don\\'t have access to a supercomputer—is this still useful for me? Though this work has focused on extremely large models, we also find that models with as few as two experts improves performance while easily fitting within memory constraints of commonly available GPUs or TPUs (details in Appendix D). We therefore believe our techniques are useful in small-scale settings.\\n\\nDo sparse models outperform dense models on the speed-accuracy Pareto curve? Yes. Across a wide variety of different models sizes, sparse models outperform dense models per step and on wall clock time. Our controlled experiments show for a fixed amount of computation and time, sparse models outperform dense models.\\n\\nI can\\'t deploy a trillion parameter model—can we shrink these models? We cannot fully preserve the model quality, but compression rates of 10 to 100x are achievable by distilling our sparse models into dense models while achieving ≈30% of the quality gain of the expert model.\\n\\nWhy use Switch Transformer instead of a model-parallel dense model? On a time basis, Switch Transformers can be far more efficient than dense-models with sharded parameters (Figure 6). Also, we point out that this decision is not mutually exclusive—we can, and do, use model-parallelism in Switch Transformers, increasing the FLOPs per token, but incurring the slowdown of conventional model-parallelism.\\n\\nWhy aren\\'t sparse models widely used already? The motivation to try sparse models has been stymied by the massive success of scaling dense models (the success of which is partially driven by co-adaptation with deep learning hardware as argued in Hooker (2020)). Further, sparse models have been subject to multiple issues including (1) model complexity, (2) training difficulties, and (3) communication costs. Switch Transformer makes strides to alleviate these issues.\\n\\n8. Future Work\\n\\nThis paper lays out a simplified architecture, improved training procedures, and a study of how sparse models scale. However, there remain many open future directions which we briefly describe here:\\n\\nA significant challenge is further improving training stability for the largest models. While our stability techniques were effective for our Switch-Base, Switch-Large and Switch-C models (no observed instability), they were not sufficient for Switch-XXL. We have taken early steps towards stabilizing these models, which we think may be generally useful for large models, including using regularizers for improving stability and adapted forms of gradient clipping, but this remains unsolved.\\n\\nGenerally we find that improved pre-training quality leads to better downstream results (Appendix E), though we sometimes encounter striking anomalies. For instance, despite similar perplexities modeling the C4 data set, the 1.6T parameter Switch-C achieves only an 87.7 exact match score in SQuAD, which compares unfavorably to 89.6 for the smaller Switch-XXL model. One notable difference is that the SwitchXXL model applies ≈10x the FLOPS per token than the Switch-C model, even though it has ≈4x less unique parameters (395B vs 1.6T). This suggests a poorly understood dependence between fine-tuning quality, FLOPS per token and number of parameters.\\n\\nPerform a comprehensive study of scaling relationships to guide the design of architectures blending data, model and expert-parallelism. Ideally, given the specs of a hardware configuration (computation, memory, communication) one could more rapidly design an optimal model. And, vice versa, this may also help in the design of future hardware.\\n\\nOur work falls within the family of adaptive computation algorithms. Our approach always used identical, homogeneous experts, but future designs (facilitated by more flexible infrastructure) could support heterogeneous experts. This would enable more flexible adaptation by routing to larger experts when more computation is desired— perhaps for harder examples.\\n\\nInvestigating expert layers outside the FFN layer of the Transformer. We find preliminary evidence that this similarly can improve model quality. In Appendix A, we report quality improvement adding these inside Self-Attention layers, where our layer replaces the weight matrices which produce Q, K, V. However, due to training instabilities with the bfloat16 format, we instead leave this as an area for future work.\\n\\nExamining Switch Transformer in new and across different modalities. We have thus far only considered language, but we believe that model sparsity can similarly provide advantages in new modalities, as well as multi-modal networks. This list could easily be extended, but we hope this gives a flavor for the types of challenges that we are thinking about and what we suspect are promising future directions.\\n\\n9. Conclusion\\n\\nSwitch Transformers are scalable and effective natural language learners. We simplify Mixture of Experts to produce an architecture that is easy to understand, stable to train and vastly more sample efficient than equivalently-sized dense models. We find that these models excel across a diverse set of natural language tasks and in different training regimes, including pre-training, fine-tuning and multi-task training. These advances make it possible to train models with hundreds of billion to trillion parameters and which achieve substantial speedups relative to dense T5 baselines. We hope our work motivates sparse models as an effective architecture and that this encourages researchers and practitioners to consider these flexible models in natural language tasks, and beyond.\\n\\nAcknowledgments\\n\\nThe authors would like to thank Margaret Li who provided months of key insights into algorithmic improvements and suggestions for empirical studies. Hugo Larochelle for sage advising and clarifying comments on the draft, Irwan Bello for detailed comments and careful revisions, Colin Raffel and Adam Roberts for timely advice on neural language models and the T5 code-base, Yoshua Bengio for advising and encouragement on research in adaptive computation, Jascha Sohl-dickstein for interesting new directions for stabilizing new large scale models and paper revisions, and the Google Brain Team for useful discussions on the paper. Blake Hechtman who provided invaluable help in profiling and improving the training performance of our models.\\n\\nA. Switch For Attention\\n\\nShazeer et al. (2018); Lepikhin et al. (2020) designed MoE Transformers (Shazeer et al., 2017) by adding MoE layers into the dense feedfoward network (FFN) computations of the Transformer. Similarly, our work also replaced the FFN layer in the Transformer, but we briefly explore here an alternate design. We add Switch layers into the Transformer Self-Attention layers. To do so, we replace the trainable weight matrices that produce the queries, keys and values with Switch layers as seen in Figure 10.\\n\\nTable 10 records the quality after a fixed number of steps as well as training time for several variants. Though we find improvements, we also found these layers to be more unstable when using bfloat16 precision and thus we did not include them in the final variant.\\n\\nModel Precision Quality Quality Speed @100k Steps (↑) @16H (↑) (ex/sec) (↑) Experts FF float32 -1.548 -1.614 1480 Expert Attention float32 -1.524 -1.606 1330 Expert Attention bfloat16 [diverges] [diverges] - Experts FF + Attention float32 -1.513 -1.607 1240 Expert FF + Attention bfloat16 [diverges] [diverges] -\\n\\nHowever, when these layers do train stably, we believe the preliminary positive results suggests a future promising direction.\\n\\nTable 10: Switch attention layer results. All models have 32 experts and train with 524k tokens per batch. Experts FF is when experts replace the FFN in the Transformer, which is our standard setup throughout the paper. Experts FF + Attention is when experts are used to replace both the FFN and the Self-Attention layers. When training with bfloat16 precision the models that have experts attention diverge.\\n\\nB. Preventing Token Dropping With No-Token-Left-Behind\\n\\nDue to software constraints on TPU accelerators, the shapes of our Tensors must be statically sized. As a result, each expert has a finite and fixed capacity to process token representations. This, however, presents an issue for our model which dynamically routes tokens at run-time that may result in an uneven distribution over experts. If the number of tokens sent to an expert is less than the expert capacity, then the computation may simply be padded - an inefficient use of the hardware, but mathematically correct. However, when the number of tokens sent to an expert is larger than its capacity (expert overflow), a protocol is needed to handle this. Lepikhin et al. (2020) adapts a Mixture-of-Expert model and addresses expert overflow by passing its representation to the next layer without processing through a residual connection which we also follow.\\n\\nWe suspected that having no computation applied to tokens could be very wasteful, especially since if there is overflow on one expert, that means another expert will have extra capacity. With this intuition we create No-Token-Left-Behind, which iteratively reroutes any tokens that are at first routed to an expert that is overflowing. Figure 11 shows a graphical description of this method, which will allow us to guarantee almost no tokens will be dropped during training and inference. We hypothesised that this could improve performance and further stabilize training, but we found no empirical benefits. We suspect that once the network learns associations between different tokens and experts, if this association is changed (e.g. sending a token to its second highest expert) then performance could be degraded.\\n\\nC. Encouraging Exploration Across Experts\\n\\nAt each expert-layer, the router determines to which expert to send the token. This is a discrete decision over the available experts, conditioned on information about the token\\'s representation. Based on the incoming token representation, the router determines the best expert, however, it receives no counterfactual information about how well it would have done selecting an alternate expert. As in reinforcement learning, a classic explorationexploitation dilemma arises (Sutton and Barto, 2018). These issues have been similarly noted and addressed differently by Rosenbaum et al. (2017) which demonstrated success in multi-task learning. This particular setting most closely matches that of a contextual bandit (Robbins, 1952). Deterministically selecting the top expert always amounts to an exploitative strategy - we consider balancing exploration to seek better expert assignment.\\n\\nTo introduce exploration, we consider several approaches: 1) deterministic or argmax 2) sampling from the softmax distribution 3) input dropout on the incoming representation 4) multiplicative jitter noise on the incoming representation. The resulting impact on model quality is reported in Table 11. Throughout this work, we use input jitter to inject noise as we have found it to empirically perform the best.\\n\\nD. Switch Transformers In Lower Compute Regimes\\n\\nSwitch Transformer is also an effective architecture at small scales as well as in regimes with thousands of cores and trillions of parameters. Many of our prior experiments were\\n\\nFigure 11: Diagram of the No-Token-Left-Behind Routing. Stage 1 is equivalent to Switch routing where tokens are routed to the expert with the highest probability from the router. In Stage 2 we look at all tokens that have overflowed and route them to the expert with which has the second highest probability. Tokens can still be overflowed if their second highest expert has too many tokens, but this allows most of the tokens to be routed. This process can be iterated to guarantee virtually no tokens are dropped at all.\\n\\nModel Quality (Neg. Log Perp.) (↑) Argmax -1.471 Sample softmax -1.570 Input dropout -1.480 Input jitter -1.468\\n\\nat the scale of 10B+ parameter models, but we show in Figure 12 as few as 2 experts produce compelling gains over a FLOP-matched counterpart. Even if a super computer is not readily available, training Switch Transformers with 2, 4, or 8 experts (as we typically recommend one expert per core) results in solid improvements over T5 dense baselines.\\n\\nE. Relation Of Upstream To Downstream Model Performance\\n\\nThere is no guarantee that a model\\'s quality on a pre-training objective will translate to downstream task results. Figure 13 presents the correlation of the upstream model quality, for both dense and Switch models, on the C4 pre-training task with two downstream task measures: average SuperGLUE performance and TriviaQA score. We choose these two tasks as one probes the model\\'s reasoning and the other factual knowledge.\\n\\nFigure 13: Upstream pre-trained quality to downstream model quality. We correlate the upstream performance with downstream quality on both SuperGLUE and TriviaQA (SOTA recorded without SSM), reasoning and knowledge-heavy benchmarks, respectively (validation sets). We find that, as with the baseline, the Switch model scales with improvements in the upstream pre-training task. For SuperGLUE, we find a loosely linear relation between negative log perplexity and the average SuperGLUE score. However, the dense model often performs better for a fixed perplexity, particularly in the large-scale regime. Conversely, on the knowledge-heavy task, TriviaQA, we find that the Switch Transformer may follow an improved scaling relationship - for a given upstream perplexity, it does better than a dense counterpart. Further statistics (expensive to collect and left to future work) would be necessary to confirm these observations.\\n\\nWe find a consistent correlation, indicating that for both baseline and Switch models, improved pre-training leads to better downstream results. Additionally, for a fixed upstream perplexity we find that both Switch and dense models perform similarly in the small to medium model size regime. However, in the largest model regime (T5-11B/T5-XXL) our largest Switch models, as mentioned in Section 5.6, do not always translate their upstream perplexity well to downstream fine-tuning on the SuperGLUE task. This warrants future investigation and study to fully realize the potential of sparse models. Understanding the fine-tuning dynamics with expert-models is very complicated and is dependent on regularization, load-balancing, and fine-tuning hyper-parameters.\\n\\nF. Pseudo Code For Switch Transformers\\n\\nPseudocode for Switch Transformers in Mesh Tensorflow (Shazeer et al., 2018). No model parallelism is being used for the below code (see 5.4 for more details).\\n\\nimport mesh tensorflow as mtf\\n\\n``` def load balance loss(router probs, expert mask): \"\"\"Calculate load−balancing loss to ensure diverse expert routing.\"\"\" # router probs is the probability assigned for each expert per token. # router probs shape: [num cores, tokens per core, num experts] # expert index contains the expert with the highest router probability in one−hot format. # expert mask shape: [num cores, tokens per core, num experts] # For each core, get the fraction of tokens routed to each expert. # density 1 shape: [num cores, num experts] density 1 = mtf.reduce mean(expert mask, reduced dim=tokens per core) # For each core, get fraction of probability mass assigned to each expert # from the router across all tokens. # density 1 proxy shape: [num cores, num experts] density 1 proxy = mtf.reduce mean(router probs, reduced dim=tokens per core) # density l for a single core: vector of length num experts that sums to 1. # density l proxy for a single core: vector of length num experts that sums to 1. # Want both vectors to have uniform allocation (1/num experts) across all num expert elements. # The two vectors will be pushed towards uniform allocation when the dot product is minimized. loss = mtf.reduce mean(density 1 proxy ∗ density 1) ∗ (num experts ˆ 2) return loss\\n\\n```\\n\\nFigure 14: Pseudo code for the load balance loss for Switch Transformers in Mesh Tensorflow.\\n\\nimport mesh tensorflow as mtf\\n\\n``` def router(inputs, capacity factor): \"\"\"Produce the combine and dispatch tensors used for sending and receiving tokens from their highest probability expert. \"\"\" # Core layout is split across num cores for all tensors and operations. # inputs shape: [num cores, tokens per core, d model] router weights = mtf.Variable(shape=[d model, num experts]) # router logits shape: [num cores, tokens per core, num experts] router logits = mtf.einsum([inputs, router weights], reduced dim=d model) if is training: # Add noise for exploration across experts. router logits += mtf.random uniform(shape=router logits.shape, minval=1−eps, maxval=1+eps) # Convert input to softmax operation from bfloat16 to float32 for stability. router logits = mtf.to float32(router logits) # Probabilities for each token of what expert it should be sent to. router probs = mtf.softmax(router logits, axis=−1) # Get the top−1 expert for each token. expert gate is the top−1 probability # from the router for each token. expert index is what expert each token # is going to be routed to. # expert gate shape: [num cores, tokens per core] # expert index shape: [num cores, tokens per core] expert gate, expert index = mtf.top 1(router probs, reduced dim=num experts) # expert mask shape: [num cores, tokens per core, num experts] expert mask = mtf.one hot(expert index, dimension=num experts) # Compute load balancing loss. aux loss = load balance loss(router probs, expert mask) # Experts have a fixed capacity, ensure we do not exceed it. Construct # the batch indices, to each expert, with position in expert # make sure that not more that expert capacity examples can be routed to # each expert. position in expert = mtf.cumsum(expert mask, dimension=tokens per core) ∗ expert mask # Keep only tokens that fit within expert capacity. expert mask ∗= mtf.less(position in expert, expert capacity) expert mask flat = mtf.reduce sum(expert mask, reduced dim=experts dim) # Mask out the experts that have overflowed the expert capacity. expert gate ∗= expert mask flat # combine tensor used for combining expert outputs and scaling with router probability. # combine tensor shape: [num cores, tokens per core, num experts, expert capacity] combine tensor = ( expert gate ∗ expert mask flat ∗ mtf.one hot(expert index, dimension=num experts) ∗ mtf.one hot(position in expert, dimension=expert capacity)) # Cast back outputs to bfloat16 for the rest of the layer. combine tensor = mtf.to bfloat16(combine tensor) # Create binary dispatch tensor that is 1 if the token gets routed to the corresponding expert. # dispatch tensor shape: [num cores, tokens per core, num experts, expert capacity] dispatch tensor = mtf.cast(combine tensor, tf.bool) return dispatch tensor, combine tensor, aux loss\\n\\n```\\n\\nFigure 15: Pseudo code for the router for Switch Transformers in Mesh Tensorflow.\\n\\nimport mesh tensorflow as mtf\\n\\n``` def switch layer(inputs, n, capacity factor, num experts): \"\"\"Distributed switch transformer feed−forward layer.\"\"\" # num cores (n) = total cores for training the model (scalar). # d model = model hidden size (scalar). # num experts = total number of experts. # capacity factor = extra buffer for each expert. # inputs shape: [batch, seq len, d model] batch, seq len, d model = inputs.get shape() # Each core will route tokens per core tokens to the correct experts. tokens per core = batch ∗ seq len / num cores # Each expert will have shape [num cores, expert capacity, d model]. # Each core is responsible for sending expert capacity tokens # to each expert. expert capacity = tokens per core ∗ capacity factor / num experts # Reshape to setup per core expert dispatching. # shape: [batch, seq len, d model] −> [num cores, tokens per core, d model] # Core layout: [n, 1, 1] −> [n, 1, 1] inputs = mtf.reshape(inputs, [num cores, tokens per core, d model]) # Core Layout: [n, 1, 1] −> [n, 1, 1, 1], [n, 1, 1, 1] # dispatch tensor (boolean) shape: [num cores, tokens per core, num experts, expert capacity] # dispatch tensor is used for routing tokens to the correct expert. # combine tensor (float) shape: [num cores, tokens per core, num experts, expert capacity] # combine tensor used for combining expert outputs and scaling with router # probability. dispatch tensor, combine tensor, aux loss = router(inputs, expert capacity) # Matmul with large boolean tensor to assign tokens to the correct expert. # Core Layout: [n, 1, 1], −> [1, n, 1, 1] # expert inputs shape: [num experts, num cores, expert capacity, d model] expert inputs = mtf.einsum([inputs, dispatch tensor], reduce dims=[tokens per core]) # All−to−All communication. Cores split across num cores and now we want to split # across num experts. This sends tokens, routed locally, to the correct expert now # split across different cores. # Core layout: [1, n, 1, 1] −> [n, 1, 1, 1] expert inputs = mtf.reshape(expert inputs, [num experts, num cores, expert capacity, d model]) # Standard feed forward computation, where each expert will have its own # unique set of parameters. # Total unique parameters created: num experts ∗ (d model ∗ d ff ∗ 2). # expert outputs shape: [num experts, num cores, expert capacity, d model] expert outputs = feed forward(expert inputs) # All−to−All communication. Cores are currently split across the experts # dimension, which needs to be switched back to being split across num cores. # Core Layout: [n, 1, 1, 1] −> [1, n, 1, 1] expert outputs = mtf.reshape(expert outputs, [num experts, num cores, expert capacity, d model]) # Convert back to input shape and multiply outputs of experts by the routing probability. # expert outputs shape: [num experts, num cores, tokens per core, d model] # expert outputs combined shape: [num cores, tokens per core, d model] # Core Layout: [1, n, 1, 1] −> [n, 1, 1] expert outputs combined = mtf.einsum([expert outputs, combine tensor], reduce dims=[tokens per core]) # Remove tokens per core shapes used for local routing dispatching to match input shape. # Core Layout: [n, 1, 1] −> [n, 1, 1] outputs = mtf.reshape(expert outputs combined, [batch, seq len, d model]) return outputs, aux loss\\n\\n```\\n\\nFigure 16: Pseudo code of the Switch Transformer layer in Mesh Tensorflow.\\n\\nReferences\\n\\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16), pages 265–283, 2016.\\n\\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\\n\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013.\\n\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\n\\nKyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning. arXiv preprint arXiv:1406.7362, 2014.\\n\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Gon¸calo M Correia, Vlad Niculae, and Andr´e FT Martins. Adaptively sparse transformers.\\n\\narXiv preprint arXiv:1909.00015, 2019.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nDavid Eigen, Marc\\'Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.\\n\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1–48, 2021. William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via filling in the . arXiv preprint arXiv:1801.07736, 2018.\\n\\nTrevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse gpu kernels for deep learning. arXiv preprint arXiv:2006.10901, 2020.\\n\\nScott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights.\\n\\nhttps://openai.com/blog/block-sparse-gpu-kernels/, 2017.\\n\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377, 2018.\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28, pages 1693–1701. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\\n\\narXiv preprint arXiv:1503.02531, 2015.\\n\\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n\\nSara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.\\n\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in neural information processing systems, pages 103–112, 2019. Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79–87, 1991.\\n\\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181–214, 1994.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\\n\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\\n\\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\\n\\narXiv preprint arXiv:2001.04451, 2020.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.\\n\\nGuillaume Lample, Alexandre Sablayrolles, Marc\\'Aurelio Ranzato, Ludovic Denoyer, and Herv´e J´egou. Large memory layers with product keys. In Advances in Neural Information Processing Systems, pages 8548–8559, 2019.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.\\n\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\n\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al.\\n\\nMixed precision training. arXiv preprint arXiv:1710.03740, 2017.\\n\\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don\\'t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018.\\n\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\n\\nAdversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019.\\n\\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr´e Susano Pinto, Sylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models. arXiv preprint arXiv:2009.13239, 2020.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\\n\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimization towards training a trillion parameter models. arXiv preprint arXiv:1910.02054, 2019.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\\n\\nPrajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models.\\n\\nIn International Conference on Learning Representations, 2018.\\n\\nHerbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527–535, 1952.\\n\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\\n\\nClemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239, 2017. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8732–8740, 2020.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2019.\\n\\nNoam Shazeer. Glu variants improve transformer, 2020.\\n\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. arXiv preprint arXiv:1701.06538, 2017.\\n\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, pages 10414–10423, 2018. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019. Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014. URL http://www.cs. toronto.edu/~rsalakhu/papers/srivastava14a.pdf. Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.\\n\\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019.\\n\\nRich Sutton. The Bitter Lesson. http://www.incompleteideas.net/IncIdeas/BitterLesson.html, 2019.\\n\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Stanford University, 2018.\\n\\nWilson L Taylor. \"cloze procedure\": A new tool for measuring readability. Journalism quarterly, 30(4):415–433, 1953.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\\n\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for generalpurpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3266–3280, 2019.\\n\\nShibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus.\\n\\nGoogle Cloud Blog, 2019.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2020.\\n\\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:28:19.946258Z",
     "start_time": "2024-11-06T23:28:19.938798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator_llm = TestsetGenerator(llm=llm, embedding_model=embeddings_llm)"
   ],
   "id": "c87c7b13786bc7f3",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:25:58.286112Z",
     "start_time": "2024-11-06T23:25:51.989326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = generator_llm.generate_with_langchain_docs(docs, testset_size=1)\n",
    "dataset.to_pandas()\n"
   ],
   "id": "78e77db5be4ef79a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Applying [SummaryExtractor, HeadlinesExtractor]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48aff4b68e1548b787602a0e12e61759"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying EmbeddingExtractor:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a319139393c493ead290756c84458ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f429deadade0443d84c5085d61d8c754"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying [EmbeddingExtractor, KeyphrasesExtractor, TitleExtractor]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "069c0c55d3d347b096ec4711149a704b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n",
      "unable to apply transformation: object of type 'StringPromptValue' has no len()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying CosineSimilarityBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "575836fcb9bc4e17a4e998baa28d7ad9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Node 4981bad8-ae7d-4f9b-8096-59ec931cf839 has no summary_embedding",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_with_langchain_docs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtestset_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m dataset\u001B[38;5;241m.\u001B[39mto_pandas()\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\testset\\synthesizers\\generate.py:126\u001B[0m, in \u001B[0;36mTestsetGenerator.generate_with_langchain_docs\u001B[1;34m(self, documents, testset_size, transforms, transforms_llm, transforms_embedding_model, query_distribution, run_config, callbacks, with_debugging_logs, raise_exceptions)\u001B[0m\n\u001B[0;32m    123\u001B[0m kg \u001B[38;5;241m=\u001B[39m KnowledgeGraph(nodes\u001B[38;5;241m=\u001B[39mnodes)\n\u001B[0;32m    125\u001B[0m \u001B[38;5;66;03m# apply transforms and update the knowledge graph\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m \u001B[43mapply_transforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransforms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknowledge_graph \u001B[38;5;241m=\u001B[39m kg\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m    130\u001B[0m     testset_size\u001B[38;5;241m=\u001B[39mtestset_size,\n\u001B[0;32m    131\u001B[0m     query_distribution\u001B[38;5;241m=\u001B[39mquery_distribution,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m     raise_exceptions\u001B[38;5;241m=\u001B[39mraise_exceptions,\n\u001B[0;32m    136\u001B[0m )\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\testset\\transforms\\engine.py:106\u001B[0m, in \u001B[0;36mapply_transforms\u001B[1;34m(kg, transforms, run_config, callbacks)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(transforms, t\u001B[38;5;241m.\u001B[39mList):\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m transforms:\n\u001B[0;32m    104\u001B[0m         asyncio\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m    105\u001B[0m             run_coroutines(\n\u001B[1;32m--> 106\u001B[0m                 \u001B[43mtransform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_execution_plan\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkg\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    107\u001B[0m                 get_desc(transform),\n\u001B[0;32m    108\u001B[0m                 run_config\u001B[38;5;241m.\u001B[39mmax_workers,\n\u001B[0;32m    109\u001B[0m             )\n\u001B[0;32m    110\u001B[0m         )\n\u001B[0;32m    111\u001B[0m \u001B[38;5;66;03m# if Parallel, collect inside it and run it all\u001B[39;00m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(transforms, Parallel):\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\testset\\transforms\\base.py:325\u001B[0m, in \u001B[0;36mRelationshipBuilder.generate_execution_plan\u001B[1;34m(self, kg)\u001B[0m\n\u001B[0;32m    322\u001B[0m     relationships \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(filtered_kg)\n\u001B[0;32m    323\u001B[0m     original_kg\u001B[38;5;241m.\u001B[39mrelationships\u001B[38;5;241m.\u001B[39mextend(relationships)\n\u001B[1;32m--> 325\u001B[0m filtered_kg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [apply_build_relationships(filtered_kg\u001B[38;5;241m=\u001B[39mfiltered_kg, original_kg\u001B[38;5;241m=\u001B[39mkg)]\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\testset\\transforms\\relationship_builders\\cosine.py:122\u001B[0m, in \u001B[0;36mSummaryCosineSimilarityBuilder.filter\u001B[1;34m(self, kg)\u001B[0m\n\u001B[0;32m    120\u001B[0m         emb \u001B[38;5;241m=\u001B[39m node\u001B[38;5;241m.\u001B[39mget_property(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproperty_name)\n\u001B[0;32m    121\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m emb \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 122\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnode\u001B[38;5;241m.\u001B[39mid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has no \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproperty_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    123\u001B[0m         nodes\u001B[38;5;241m.\u001B[39mappend(node)\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m KnowledgeGraph(nodes\u001B[38;5;241m=\u001B[39mnodes)\n",
      "\u001B[1;31mValueError\u001B[0m: Node 4981bad8-ae7d-4f9b-8096-59ec931cf839 has no summary_embedding"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Knowledge Graph\n",
   "id": "1face7f4aed27b78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:26:32.908948Z",
     "start_time": "2024-11-06T23:26:32.903638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph()"
   ],
   "id": "539b650f83138a32",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:26:59.901070Z",
     "start_time": "2024-11-06T23:26:59.893945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "for doc in docs:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )"
   ],
   "id": "ab2f3683a07384c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T23:28:47.338038Z",
     "start_time": "2024-11-06T23:28:40.912758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "# define your LLM and Embedding Model\n",
    "# here we are using the same LLM and Embedding Model that we used to generate the testset\n",
    "transformer_llm = generator_llm\n",
    "embedding_model = embeddings_llm\n",
    "\n",
    "trans = default_transforms(llm=transformer_llm, embedding_model=embedding_model)\n",
    "apply_transforms(kg, trans)"
   ],
   "id": "631ebd689d3220f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Applying [SummaryExtractor, HeadlinesExtractor]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adca8c60c8234ac598c358d58a675416"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying EmbeddingExtractor:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c750101d72a401ba7242f89c79b1463"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fb5bd7399bb45f0bf350c96620fce25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying [EmbeddingExtractor, KeyphrasesExtractor, TitleExtractor]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe113524c95c40c18d829795f632047c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n",
      "unable to apply transformation: TestsetGenerator.generate() got an unexpected keyword argument 'n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Applying CosineSimilarityBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29f2ae0d0e33465097c8aba5c110440f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Node a1d5895a-f4b3-4b94-bfa5-03a2a3600042 has no summary_embedding",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m embedding_model \u001B[38;5;241m=\u001B[39m embeddings_llm\n\u001B[0;32m      8\u001B[0m trans \u001B[38;5;241m=\u001B[39m default_transforms(llm\u001B[38;5;241m=\u001B[39mtransformer_llm, embedding_model\u001B[38;5;241m=\u001B[39membedding_model)\n\u001B[1;32m----> 9\u001B[0m \u001B[43mapply_transforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\testset\\transforms\\engine.py:106\u001B[0m, in \u001B[0;36mapply_transforms\u001B[1;34m(kg, transforms, run_config, callbacks)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(transforms, t\u001B[38;5;241m.\u001B[39mList):\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m transforms:\n\u001B[0;32m    104\u001B[0m         asyncio\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m    105\u001B[0m             run_coroutines(\n\u001B[1;32m--> 106\u001B[0m                 \u001B[43mtransform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_execution_plan\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkg\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    107\u001B[0m                 get_desc(transform),\n\u001B[0;32m    108\u001B[0m                 run_config\u001B[38;5;241m.\u001B[39mmax_workers,\n\u001B[0;32m    109\u001B[0m             )\n\u001B[0;32m    110\u001B[0m         )\n\u001B[0;32m    111\u001B[0m \u001B[38;5;66;03m# if Parallel, collect inside it and run it all\u001B[39;00m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(transforms, Parallel):\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\testset\\transforms\\base.py:325\u001B[0m, in \u001B[0;36mRelationshipBuilder.generate_execution_plan\u001B[1;34m(self, kg)\u001B[0m\n\u001B[0;32m    322\u001B[0m     relationships \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(filtered_kg)\n\u001B[0;32m    323\u001B[0m     original_kg\u001B[38;5;241m.\u001B[39mrelationships\u001B[38;5;241m.\u001B[39mextend(relationships)\n\u001B[1;32m--> 325\u001B[0m filtered_kg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [apply_build_relationships(filtered_kg\u001B[38;5;241m=\u001B[39mfiltered_kg, original_kg\u001B[38;5;241m=\u001B[39mkg)]\n",
      "File \u001B[1;32mC:\\sw\\anaconda3\\envs\\torch\\Lib\\site-packages\\ragas\\testset\\transforms\\relationship_builders\\cosine.py:122\u001B[0m, in \u001B[0;36mSummaryCosineSimilarityBuilder.filter\u001B[1;34m(self, kg)\u001B[0m\n\u001B[0;32m    120\u001B[0m         emb \u001B[38;5;241m=\u001B[39m node\u001B[38;5;241m.\u001B[39mget_property(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproperty_name)\n\u001B[0;32m    121\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m emb \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 122\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnode\u001B[38;5;241m.\u001B[39mid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has no \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproperty_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    123\u001B[0m         nodes\u001B[38;5;241m.\u001B[39mappend(node)\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m KnowledgeGraph(nodes\u001B[38;5;241m=\u001B[39mnodes)\n",
      "\u001B[1;31mValueError\u001B[0m: Node a1d5895a-f4b3-4b94-bfa5-03a2a3600042 has no summary_embedding"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
